<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>`shapr`: Explaining individual machine learning predictions with Shapley values • shapr</title>
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="`shapr`: Explaining individual machine learning predictions with Shapley values">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">shapr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-vignettes" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Vignettes</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-vignettes">
<li><a class="dropdown-item" href="../articles/general_usage.html">General usage of shapr</a></li>
    <li><a class="dropdown-item" href="../articles/vaeac.html">Advanced usage of the `vaeac` approach</a></li>
    <li><a class="dropdown-item" href="../articles/regression.html">The separate and surrogate regression approches</a></li>
    <li><a class="dropdown-item" href="../articles/asymmetric_causal.html">Asymmetric and Causal Shapley values</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Manual</a></li>
<li class="nav-item"><a class="nav-link" href="../shaprpy.html">Python</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/NorskRegnesentral/shapr/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>`shapr`: Explaining individual machine learning predictions with Shapley values</h1>
                        <h4 data-toc-skip class="author">Martin Jullum,
Camilla Lingjærde, Lars Henry Berge Olsen &amp; Nikolai Sellereite</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/NorskRegnesentral/shapr/blob/master/vignettes/general_usage.Rmd" class="external-link"><code>vignettes/general_usage.Rmd</code></a></small>
      <div class="d-none name"><code>general_usage.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="intro">Introduction<a class="anchor" aria-label="anchor" href="#intro"></a>
</h2>
<p>The <code>shapr</code> package implements an extended version of the
Kernel SHAP method for approximating Shapley values (<span class="citation">Lundberg and Lee (2017)</span>), in which dependence
between the features is taken into account (<span class="citation">Aas,
Jullum, and Løland (2021)</span>).</p>
<p>Estimation of Shapley values is of interest when attempting to
explain complex machine learning models. Of existing work on
interpreting individual predictions, Shapley values is regarded to be
the only model-agnostic explanation method with a solid theoretical
foundation (<span class="citation">Lundberg and Lee (2017)</span>).
Kernel SHAP is a computationally efficient approximation to Shapley
values in higher dimensions, but it assumes independent features. <span class="citation">Aas, Jullum, and Løland (2021)</span> extends the
Kernel SHAP method to handle dependent features, resulting in more
accurate approximations to the true Shapley values. See the <a href="https://martinjullum.com/publication/aas-2021-explaining/aas-2021-explaining.pdf" class="external-link">paper</a>
(<span class="citation">Aas, Jullum, and Løland (2021)</span>) for
further details.</p>
</div>
<div class="section level2">
<h2 id="overview">Overview of Package<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<div class="section level3">
<h3 id="functionality">Functionality<a class="anchor" aria-label="anchor" href="#functionality"></a>
</h3>
<p>Here is an overview of the main functions. You can read their
documentation and see examples with <code>?function_name</code>.</p>
<table class="table">
<caption>Main functions in the <code>shapr</code> package.</caption>
<colgroup>
<col width="30%">
<col width="69%">
</colgroup>
<thead><tr class="header">
<th align="left">Function Name</th>
<th align="left">Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><code>explain</code></td>
<td align="left">Computes kernel SHAP values for test data.</td>
</tr>
<tr class="even">
<td align="left"><code>explain_forecast</code></td>
<td align="left">Analogous to <code>explain</code>, but for explaining
forecasts from time series models.</td>
</tr>
<tr class="odd">
<td align="left"><code>plot.shapr</code></td>
<td align="left">Plots the individual prediction explanations. Uses the
<code>ggplot</code> and <code>ggbeeswarm</code> package.</td>
</tr>
</tbody>
</table>
<p>The <code>shapr</code> package implements Kernel SHAP estimation of
dependence-aware Shapley values with eight different Monte Carlo-based
approaches for estimating the conditional distributions of the data,
namely <code>"empirical"</code>, <code>"gaussian"</code>,
<code>"copula"</code>, <code>"ctree"</code>, <code>"vaeac"</code>,
<code>"categorical"</code>, <code>"timeseries"</code>, and
<code>"independence"</code>. <code>shapr</code> has also implemented two
regression-based approaches <code>"regression_separate"</code> and
<code>"regression_surrogate"</code>. See <a href="#ex">Estimation
approaches and plotting functionality</a> below for examples. It is also
possible to combine the different approaches, see the <a href="#combined">combined approach</a>.</p>
<p>The package allows for parallelized computation through the
<code>future</code>package, see <a href="#para">Parallelization</a> for
details.</p>
<p>The level of detail in the output can be controlled through the
<code>verbose</code> argument. In addition, progress updates on the
process of estimating the <code>v(S)</code>’s (and training the
<code>"vaeac"</code> model) is available through the
<code>progressr</code> package, supporting progress updates also for
parallelized computation. See <a href="#verbose">Verbosity and progress
updates</a> for details.</p>
<p>Moreover, the default behavior is to estimate the Shapley values
iteratively, with increasing number of feature coalitions being added,
and to stop estimation as the estimated Shapley values has achieved a
certain level of stability. More information about this is provided in
<a href="#iterative">Iterative estimation</a> The above, combined with
batch computation of the <code>v(S)</code> values, enables fast and
accurate estimation of the Shapley values in a memory friendly
manner.</p>
<p>The package also provides functionality for computing Shapley values
for groups of features, and custom function explanation, see <a href="#advanced">Advanced usage</a>. Finally, explanation of multiple
output time series forecasting models are discussed in <a href="#forecasting">Explaining forecasting models</a>.</p>
</div>
<div class="section level3">
<h3 id="default-behavior-of-explain">Default behavior of <code>explain</code><a class="anchor" aria-label="anchor" href="#default-behavior-of-explain"></a>
</h3>
<p>Below we provide brief descriptions of the most important parts of
the default behavior of the <code>explain</code> function.</p>
<p>By default <code>explain</code> always compute feature-wise Shapley
values. Groups of features can be explained by providing the feature
groups through the <code>group</code> argument.</p>
<p>When there are five or less features (or feature groups), iterative
estimation is by default disabled. The reason for this is that it is
usually faster to estimate the Shapley values for all possible
coalitions (<code>v(S)</code>), than to estimate the uncertainty of the
Shapley values, and potentially stop estimation earlier. While iterative
estimation is the default starting from six features, it is mainly when
there are more than ten features, that it is most beneficial, and can
save a lot of computation time. The reason for this is that the number
of possible coalitions grows exponentially. These defaults can be
overridden by setting the <code>iterative</code> argument to
<code>TRUE</code> or <code>FALSE</code>. When using the
<code>iterative</code> argument, the estimation for an observation is
stopped when all Shapley value standard deviations are below
<code>t</code> times the range of the Shapley values. The <code>t</code>
value controls the convergence tolerance, defaults to 0.02, and can be
set through the <code>iterative_args$convergence_tol</code> argument,
see <a href="#iterative">iterative estimation</a> for more details.</p>
<p>Since the iterativeness default changes based on the number of
features (or feature groups), the default is also to have no upper bound
on the number of coalitions considered. This can be controlled through
the <code>max_n_coalitions</code> argument.</p>
</div>
</div>
<div class="section level2">
<h2 id="KSHAP">Kernel SHAP and dependence-aware estimators<a class="anchor" aria-label="anchor" href="#KSHAP"></a>
</h2>
<div class="section level3">
<h3 id="the-kernel-shap-method">The Kernel SHAP Method<a class="anchor" aria-label="anchor" href="#the-kernel-shap-method"></a>
</h3>
<p>Assume a predictive model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\boldsymbol{x})</annotation></semantics></math>
for a response value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
with features
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐱</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{x}\in \mathbb{R}^M</annotation></semantics></math>,
trained on a training set, and that we want to explain the predictions
for new sets of data. This may be done using ideas from cooperative game
theory, letting a single prediction take the place of the game being
played and the features the place of the players. Letting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
denote the set of all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
players, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>⊆</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">S \subseteq N</annotation></semantics></math>
be a subset of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>S</mi><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|S|</annotation></semantics></math>
players, the “contribution” function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(S)</annotation></semantics></math>
describes the total expected sum of payoffs the members of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
can obtain by cooperation. The Shapley value (<span class="citation">Shapley (1953)</span>) is one way to distribute the
total gains to the players, assuming that they all collaborate. The
amount that player
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
gets is then</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>ϕ</mi><mi>i</mi></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>S</mi><mo>⊆</mo><mi>N</mi><mo>\</mo><mo stretchy="false" form="prefix">{</mo><mi>i</mi><mo stretchy="false" form="postfix">}</mo></mrow></munder><mfrac><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>S</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>!</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mo>−</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>S</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow><mrow><mi>M</mi><mi>!</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mo>∪</mo><mo stretchy="false" form="prefix">{</mo><mi>i</mi><mo stretchy="false" form="postfix">}</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\phi_i(v) = \phi_i = \sum_{S \subseteq N \setminus\{i\}} \frac{|S| ! (M-| S| - 1)!}{M!}(v(S\cup \{i\})-v(S)),</annotation></semantics></math></p>
<p>that is, a weighted mean over all subsets
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
of players not containing player
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.
<span class="citation">Lundberg and Lee (2017)</span> define the
contribution function for a certain subset
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
of these features
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>S</mi></msub><annotation encoding="application/x-tex">\boldsymbol{x}_S</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">E</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">v(S) = \mbox{E}[f(\boldsymbol{x})|\boldsymbol{x}_S]</annotation></semantics></math>,
the expected output of the predictive model conditional on the feature
values of the subset. <span class="citation">Lundberg and Lee
(2017)</span> names this type of Shapley values SHAP (SHapley Additive
exPlanation) values. Since the conditional expectations can be written
as</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>s</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo>,</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mo>∫</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo>,</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}
\label{eq:CondExp}
E[f(\boldsymbol{x})|\boldsymbol{x}_s=\boldsymbol{x}_S^*] = E[f(\boldsymbol{x}_{\bar{S}},\boldsymbol{x}_S)|\boldsymbol{x}_S=\boldsymbol{x}_S^*] =
\int f(\boldsymbol{x}_{\bar{S}},\boldsymbol{x}_S^*)\,p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)d\boldsymbol{x}_{\bar{S}},
\end{equation}</annotation></semantics></math><p>the conditional distributions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)</annotation></semantics></math>
are needed to compute the contributions. The Kernel SHAP method of <span class="citation">Lundberg and Lee (2017)</span> assumes feature
independence, so that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)=p(\boldsymbol{x}_{\bar{S}})</annotation></semantics></math>.
If samples
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover><mi>k</mi></msubsup><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{S}}^{k}, k=1,\ldots,K</annotation></semantics></math>,
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)</annotation></semantics></math>
are available, the conditional expectation above can be approximated
by</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mtext mathvariant="normal">KerSHAP</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>K</mi></mfrac><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover><mi>k</mi></msubsup><mo>,</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}
  v_{\text{KerSHAP}}(S) = \frac{1}{K}\sum_{k=1}^K f(\boldsymbol{x}_{\bar{S}}^{k},\boldsymbol{x}_S^*).
\end{equation}</annotation></semantics></math><p>In Kernel SHAP,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover><mi>k</mi></msubsup><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{S}}^{k}, k=1,\ldots,K</annotation></semantics></math>
are sampled from the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>S</mi><mo accent="true">‾</mo></mover><annotation encoding="application/x-tex">\bar{S}</annotation></semantics></math>-part
of the training data, <em>independently</em> of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>S</mi></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{S}</annotation></semantics></math>.
This is motivated by using the training set as the empirical
distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{S}}</annotation></semantics></math>,
and assuming that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{S}}</annotation></semantics></math>
is independent of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup></mrow><annotation encoding="application/x-tex">\boldsymbol{x}_S=\boldsymbol{x}_S^*</annotation></semantics></math>.
Due to the independence assumption, if the features in a given model are
highly dependent, the Kernel SHAP method may give a completely wrong
answer. This can be avoided by estimating the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>S</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)</annotation></semantics></math>
directly and generating samples from this distribution. With this small
change, the contributions and Shapley values may then be approximated as
in the ordinary Kernel SHAP framework. <span class="citation">Aas,
Jullum, and Løland (2021)</span> propose three different approaches for
estimating the conditional probabilities which are implemented:
<code>empirical</code>, <code>gaussian</code> and <code>copula</code>.
The package also implements the <code>ctree</code> method from <span class="citation">Redelmeier, Jullum, and Aas (2020)</span>, the
<code>vaeac</code> method from <span class="citation">Olsen et al.
(2022)</span> and a <code>categorical</code> for categorical data. The
original <code>independence</code> approach of <span class="citation">Lundberg and Lee (2017)</span> is also available. The
methods may also be combined, such that e.g. one method is used when
conditioning on a small number of features, while another method is used
otherwise. The <code>shapr</code> package also supports directly
estimating the contribution function using regression. We briefly
introduce the regression-based methods below, but we refer to the
separate regression vignette (Shapley value explanations using the
regression paradigm) and <span class="citation">Olsen et al.
(2024)</span> for an in-depth explanation of the regression
paradigm.</p>
</div>
<div class="section level3">
<h3 id="gaussian">Multivariate Gaussian Distribution Approach<a class="anchor" aria-label="anchor" href="#gaussian"></a>
</h3>
<p>The first approach arises from the assumption that the feature vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\boldsymbol{x}</annotation></semantics></math>
stems from a multivariate Gaussian distribution with some mean vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝛍</mi><annotation encoding="application/x-tex">\boldsymbol{\mu}</annotation></semantics></math>
and covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\boldsymbol{\Sigma}</annotation></semantics></math>.
Under this assumption, the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>𝒮</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>𝒮</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{\mathcal{S}}} |\boldsymbol{x}_{\mathcal{S}}=\boldsymbol{x}_{\mathcal{S}}^*)</annotation></semantics></math>
is also multivariate Gaussian<br><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">N</mtext><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝛍</mi><mrow><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝒮</mi></mrow></msub><mo>,</mo><msub><mi>𝚺</mi><mrow><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝒮</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{N}_{|\bar{\mathcal{S}}|}(\boldsymbol{\mu}_{\bar{\mathcal{S}}|\mathcal{S}},\boldsymbol{\Sigma}_{\bar{\mathcal{S}}|\mathcal{S}})</annotation></semantics></math>,
with analytical expressions for the conditional mean vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛍</mi><mrow><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝒮</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\mu}_{\bar{\mathcal{S}}|\mathcal{S}}</annotation></semantics></math>
and covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝒮</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{\bar{\mathcal{S}}|\mathcal{S}}</annotation></semantics></math>,
see <span class="citation">Aas, Jullum, and Løland (2021)</span> for
details. Hence, instead of sampling from the marginal empirical
distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{\mathcal{S}}}</annotation></semantics></math>
approximated by the training data, we can sample from the Gaussian
conditional distribution, which is fitted using the training data. Using
the resulting samples
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mi>k</mi></msubsup><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{\mathcal{S}}}^k, k=1,\ldots,K</annotation></semantics></math>,
the conditional expectations be approximated as in the Kernel SHAP.</p>
</div>
<div class="section level3">
<h3 id="copula">Gaussian Copula Approach<a class="anchor" aria-label="anchor" href="#copula"></a>
</h3>
<p>If the features are far from multivariate Gaussian, an alternative
approach is to instead represent the marginals by their empirical
distributions, and model the dependence structure by a Gaussian copula.
Assuming a Gaussian copula, we may convert the marginals of the training
data to Gaussian features using their empirical distributions, and then
fit a multivariate Gaussian distribution to these.</p>
<p>To produce samples from the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>𝒮</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>𝒮</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{\mathcal{S}}} |\boldsymbol{x}_{\mathcal{S}}=\boldsymbol{x}_{\mathcal{S}}^*)</annotation></semantics></math>,
we convert the marginals of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>𝒮</mi></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{\mathcal{S}}</annotation></semantics></math>
to Gaussians, sample from the conditional Gaussian distribution as
above, and convert the marginals of the samples back to the original
distribution. Those samples are then used to approximate the sample from
the resulting multivariate Gaussian conditional distribution. While
other copulas may be used, the Gaussian copula has the benefit that we
may use the analytical expressions for the conditionals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝛍</mi><mrow><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝒮</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\mu}_{\bar{\mathcal{S}}|\mathcal{S}}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝚺</mi><mrow><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>𝒮</mi></mrow></msub><annotation encoding="application/x-tex">\boldsymbol{\Sigma}_{\bar{\mathcal{S}}|\mathcal{S}}</annotation></semantics></math>.
Finally, we may convert the marginals back to their original
distribution, and use the resulting samples to approximate the
conditional expectations as in Kernel SHAP.</p>
</div>
<div class="section level3">
<h3 id="empirical">Empirical Conditional Distribution Approach<a class="anchor" aria-label="anchor" href="#empirical"></a>
</h3>
<p>If both the dependence structure and the marginal distributions of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\boldsymbol{x}</annotation></semantics></math>
are very far from the Gaussian, neither of the two aforementioned
methods will work very well. Few methods exists for the non-parametric
estimation of conditional densities, and the classic kernel estimator
(<span class="citation">Rosenblatt (1956)</span>) for non-parametric
density estimation suffers greatly from the curse of dimensionality and
does not provide a way to generate samples from the estimated
distribution. For such situations, <span class="citation">Aas, Jullum,
and Løland (2021)</span> propose an empirical conditional approach to
sample approximately from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msubsup><mi>𝐱</mi><mi>𝒮</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{\mathcal{S}}}|\boldsymbol{x}_{\mathcal{S}}^*)</annotation></semantics></math>.
The idea is to compute weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>𝒮</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐱</mi><mo>*</mo></msup><mo>,</mo><msup><mi>𝐱</mi><mi>i</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="0.222em"></mspace><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>n</mi><mtext mathvariant="normal">train</mtext></msub></mrow><annotation encoding="application/x-tex">w_{\mathcal{S}}(\boldsymbol{x}^*,\boldsymbol{x}^i),\ i=1,...,n_{\text{train}}</annotation></semantics></math>
for all training instances based on their Mahalanobis distances (in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
subset only) to the instance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>𝐱</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\boldsymbol{x}^*</annotation></semantics></math>
to be explained. Instead of sampling from this weighted (conditional)
empirical distribution, <span class="citation">Aas, Jullum, and Løland
(2021)</span> suggests a more efficient variant, using only the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
instances with the largest weights:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mtext mathvariant="normal">condKerSHAP</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>w</mi><mi>𝒮</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐱</mi><mo>*</mo></msup><mo>,</mo><msup><mi>𝐱</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mo stretchy="true" form="postfix">]</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mo stretchy="true" form="postfix">]</mo></mrow></msubsup><mo>,</mo><msubsup><mi>𝐱</mi><mi>𝒮</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>w</mi><mi>𝒮</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐱</mi><mo>*</mo></msup><mo>,</mo><msup><mi>𝐱</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mo stretchy="true" form="postfix">]</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">v_{\text{condKerSHAP}}(\mathcal{S}) = \frac{\sum_{k=1}^K w_{\mathcal{S}}(\boldsymbol{x}^*,
\boldsymbol{x}^{[k]}) f(\boldsymbol{x}_{\bar{\mathcal{S}}}^{[k]},
\boldsymbol{x}_{\mathcal{S}}^*)}{\sum_{k=1}^K w_{\mathcal{S}}(\boldsymbol{x}^*,\boldsymbol{x}^{[k]})},</annotation></semantics></math></p>
<p>The number of samples
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
to be used in the approximate prediction can for instance be chosen such
that the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
largest weights accounts for a fraction
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>,
for example
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.9</mn><annotation encoding="application/x-tex">0.9</annotation></semantics></math>,
of the total weight. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
exceeds a certain limit, for instance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">5,000</annotation></semantics></math>,
it might be set to that limit. A bandwidth parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
used to scale the weights, must also be specified. This choice may be
viewed as a bias-variance trade-off. A small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
puts most of the weight to a few of the closest training observations
and thereby gives low bias, but high variance. When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo>→</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">\sigma \rightarrow \infty</annotation></semantics></math>,
this method converges to the original Kernel SHAP assuming feature
independence. Typically, when the features are highly dependent, a small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
is typically needed such that the bias does not dominate. <span class="citation">Aas, Jullum, and Løland (2021)</span> show that a
proper criterion for selecting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
is a small-sample-size corrected version of the AIC known as AICc. As
calculation of it is computationally intensive, an approximate version
of the selection criterion is also suggested. Details on this is found
in <span class="citation">Aas, Jullum, and Løland (2021)</span>.</p>
</div>
<div class="section level3">
<h3 id="ctree">Conditional Inference Tree Approach<a class="anchor" aria-label="anchor" href="#ctree"></a>
</h3>
<p>The previous three methods can only handle numerical data. This means
that if the data contains categorical/discrete/ordinal features, the
features first have to be one-hot encoded. When the number of
levels/features is large, this is not feasible. An approach that handles
mixed (i.e numerical, categorical, discrete, ordinal) features and both
univariate and multivariate responses is conditional inference trees
(<span class="citation">Hothorn, Hornik, and Zeileis (2006)</span>).</p>
<p>Conditional inference trees is a special tree fitting procedure that
relies on hypothesis tests to choose both the splitting feature and the
splitting point. The tree fitting procedure is sequential: first a
splitting feature is chosen (the feature that is least independent of
the response), and then a splitting point is chosen for this feature.
This decreases the chance of being biased towards features with many
splits (<span class="citation">Hothorn, Hornik, and Zeileis
(2006)</span>).</p>
<p>We use conditional inference trees (<em>ctree</em>) to model the
conditional distribution,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover></msub><mo stretchy="false" form="prefix">|</mo><msubsup><mi>𝐱</mi><mi>𝒮</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{\mathcal{S}}}|\boldsymbol{x}_{\mathcal{S}}^*)</annotation></semantics></math>,
found in the Shapley methodology. First, we fit a different conditional
inference tree to each conditional distribution. Once a tree is fit for
given dependent features, the end node of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>𝐱</mi><mi>𝒮</mi><mo>*</mo></msubsup><annotation encoding="application/x-tex">\boldsymbol{x}_{\mathcal{S}}^*</annotation></semantics></math>
is found. Then, we sample from this end node and use the resulting
samples,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover><mi>k</mi></msubsup><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{\mathcal{S}}}^k, k=1,\ldots,K</annotation></semantics></math>,
when approximating the conditional expectations as in Kernel SHAP. See
<span class="citation">Redelmeier, Jullum, and Aas (2020)</span> for
more details.</p>
<p>The conditional inference trees are fit using the <em>party</em> or
<em>partykit</em> packages (<span class="citation">Hothorn and Zeileis
(2015)</span>).</p>
</div>
<div class="section level3">
<h3 id="vaeac">Variational AutoEncoder with Arbitrary Conditioning (vaeac)
Approach<a class="anchor" aria-label="anchor" href="#vaeac"></a>
</h3>
<p>Another approach that supports mixed features is the Variational
AutoEncoder with Arbitrary Conditioning (<span class="citation">Olsen et
al. (2022)</span>), abbreviated to <code>vaeac</code>. The
<code>vaeac</code> is an extension of the regular variational
autoencoder (<span class="citation">Kingma and Welling (2014)</span>),
but instead of giving a probabilistic representation of the distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x})</annotation></semantics></math>
it gives a probabilistic representation of the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover></msub><mo>∣</mo><msub><mi>𝐱</mi><mi>𝒮</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\boldsymbol{x}_{\bar{\mathcal{S}}} \mid \boldsymbol{x}_{\mathcal{S}})</annotation></semantics></math>,
for all possible feature subsets
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒮</mi><mo>⊆</mo><mi>ℳ</mi></mrow><annotation encoding="application/x-tex">\mathcal{S}\subseteq\mathcal{M}</annotation></semantics></math>
simultaneously, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics></math>
is the set of all features. That is, only a single <code>vaeac</code>
model is needed to model all conditional distributions.</p>
<p>The <code>vaeac</code> consists of three neural networks: a <em>full
encoder</em>, a <em>masked encoder</em>, and a <em>decoder</em>. The
encoders map the full and masked/conditional input representations,
i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\boldsymbol{x}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>𝒮</mi></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{\mathcal{S}}</annotation></semantics></math>,
respectively, to latent probabilistic representations. Sampled instances
from this latent probabilistic representations are sent to the decoder,
which maps them back to the feature space and provides a samplable
probabilistic representation for the unconditioned features
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mover><mi>𝒮</mi><mo accent="true">‾</mo></mover></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{\bar{\mathcal{S}}}</annotation></semantics></math>.
The full encoder is only used during the training phase of the
<code>vaeac</code> model to guide the training process of the masked
encoder, as the former relies on the full input sample
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\boldsymbol{x}</annotation></semantics></math>,
which is not accessible in the deployment phase (when we generate the
Monte Carlo samples), as we only have access to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐱</mi><mi>𝒮</mi></msub><annotation encoding="application/x-tex">\boldsymbol{x}_{\mathcal{S}}</annotation></semantics></math>.
The networks are trained by minimizing a variational lower bound. See
Section 3 in <span class="citation">Olsen et al. (2022)</span> for an
in-depth introduction to the <code>vaeac</code> methodology. We use the
<code>vaeac</code> model at the epoch which obtains the lowest
validation IWAE score to generate the Monte Carlo samples used in the
Shapley value computations.</p>
<p>We fit the <code>vaeac</code> model using the R-package
<em>torch</em>(<span class="citation">Falbel and Luraschi
(2023)</span>). The main parameters are the the number of layers in the
networks (<code>vaeac.depth</code>), the width of the layers
(<code>vaeac.width</code>), the number of dimensions in the latent space
(<code>vaeac.latent_dim</code>), the activation function between the
layers in the networks (<code>vaeac.activation_function</code>), the
learning rate in the ADAM optimizer (<code>vaeac.lr</code>), the number
of <code>vaeac</code> models to initiate to remedy poorly initiated
model parameter values (<code>vaeac.n_vaeacs_initialize</code>), and the
number of learning epochs (<code>vaeac.epochs</code>). See
<code><a href="../reference/setup_approach.html">?shapr::setup_approach.vaeac</a></code> for a more detailed
description of the parameters.</p>
<p>There are additional extra parameters which can be set by including a
named list in the call to the <code><a href="../reference/explain.html">explain()</a></code> function. For
example, we can the change the batch size to 32 by including
<code>vaeac.extra_parameters = list(vaeac.batch_size = 32)</code> as an
argument in the <code><a href="../reference/explain.html">explain()</a></code> function. See
<code><a href="../reference/vaeac_get_extra_para_default.html">?shapr::vaeac_get_extra_para_default</a></code> for a description of
the possible extra parameters to the <code>vaeac</code> approach. Note
that the main parameters are entered as arguments directly in
<code><a href="../reference/explain.html">explain()</a></code> function, while the extra parameters are
specified through a named list called
<code>vaeac.extra_parameters</code>.</p>
</div>
<div class="section level3">
<h3 id="categorical-approach">Categorical Approach<a class="anchor" aria-label="anchor" href="#categorical-approach"></a>
</h3>
<p>When the features are all categorical, we can estimate the
conditional expectations using basic statistical formulas. For example,
if we have three features,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, x_3</annotation></semantics></math>
with three levels each (indicated as 1, 2, 3), and we are provided with
a table of counts indicating how many times each combination of feature
values occurs, we can estimate the marginal and conditional
probabilities as follows. Marginal probabilities are estimated by
dividing the number of times a given feature (or features) takes on a
certain value in the data set with the total number of observations in
the data set. Conditional probabilities (for example,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo stretchy="false" form="prefix">|</mo><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(X_1 = 1 | X_2 = 1)</annotation></semantics></math>)
are estimated by first subsetting the data set to reflect the
conditioning (i.e., extracting all rows where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">X_2 = 1</annotation></semantics></math>),
and then dividing the number of times the feature on the left hand side
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo stretchy="false" form="prefix">|</mo><annotation encoding="application/x-tex">|</annotation></semantics></math>
takes the given value in this subset by the total number of observations
in this subset. Once the marginal and conditional probabilities are
estimated for all combinations of feature values, each conditional
expectation can be calculated. For example, the expected value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mn>1</mn></msub><annotation encoding="application/x-tex">X_1</annotation></semantics></math>
given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">X_2 = 1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>3</mn></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">X_3 = 2</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><msub><mi>X</mi><mn>3</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>x</mi></munder><mi>x</mi><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>=</mo><mi>x</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>X</mi><mn>3</mn></msub><mo>=</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>x</mi></munder><mi>x</mi><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>=</mo><mi>x</mi><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>X</mi><mn>3</mn></msub><mo>=</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>X</mi><mn>3</mn></msub><mo>=</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">E(X_1|X_2, X_3) = \sum_{x}x P(X_1 = x | X_2=1, X_3=2) = \sum_{x} x \frac{P(X_1 = x, X_2 = 1, X_3 = 2)}{P(X_2=1, X_3=2)}.</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="Regression_approaches">Separate and Surrogate Regression Approaches<a class="anchor" aria-label="anchor" href="#Regression_approaches"></a>
</h3>
<p>Another paradigm for estimating the contribution function is the
regression paradigm. In contrast to the methods above, which belong to
the Monte Carlo paradigm, the regression based methods use regression
models to estimate the contribution function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>𝐱</mi><mi>S</mi></msub><mo>=</mo><msubsup><mi>𝐱</mi><mi>S</mi><mo>*</mo></msubsup><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">v(S) = E[f(\boldsymbol{x})|\boldsymbol{x}_S = \boldsymbol{x}_S^*]</annotation></semantics></math>
directly. The separate regression method class fits a separate
regression model for each coalition
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>,
while the surrogate regression method class fits a single regression
model to simultaneously predict the contribution function for all
coalitions. We refer to <span class="citation">Olsen et al.
(2024)</span> for when one should use the different paradigms, method
classes, and methods.</p>
<p>In a separate vignette (Shapley value explanations using the
regression paradigm), we elaborate and demonstrate the regression
paradigm. We describe how to specify the regression model, enable
automatic cross-validation of the model’s hyperparameters, and apply
pre-processing steps to the data before fitting the regression models.
<span class="citation">Olsen et al. (2024)</span> divides the regression
paradigm into the separate and surrogate regression method classes. In
the separate vignette, we briefly introduce the two method classes. For
an in-depth explanation, we refer the reader to Sections 3.5 and 3.6 in
<span class="citation">Olsen et al. (2024)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 id="ex">Estimation approaches and plotting functionality<a class="anchor" aria-label="anchor" href="#ex"></a>
</h2>
<p>The following example shows how a simple <code>xgboost</code> model
is trained using the <code>airquality</code> dataset, and how
<code>shapr</code> can be used to explain the individual predictions.
Since which approach should be used to estimate the contribution
functions will depend on the type of data one is working with, the user
must specify the approach to be used through the <code>approach</code>
argument. Allowed values are <code>"gaussian"</code>,
<code>"copula"</code>, <code>"empirical"</code>, <code>"ctree"</code>,
<code>"vaeac"</code>, <code>"categorical"</code>,
<code>"timeseries"</code>, <code>"independence"</code>,
<code>"regression_separate"</code>, and
<code>"regression_surrogate"</code>.</p>
<p>First we load the shapr package</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://norskregnesentral.github.io/shapr/">shapr</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/dmlc/xgboost" class="external-link">xgboost</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com" class="external-link">data.table</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"airquality"</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html" class="external-link">complete.cases</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">x_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Solar.R"</span>, <span class="st">"Wind"</span>, <span class="st">"Temp"</span>, <span class="st">"Month"</span><span class="op">)</span></span>
<span><span class="va">y_var</span> <span class="op">&lt;-</span> <span class="st">"Ozone"</span></span>
<span></span>
<span><span class="va">ind_x_explain</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span><span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="fu"><a href="https://rdrr.io/r/base/get.html" class="external-link">get</a></span><span class="op">(</span><span class="va">y_var</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">x_explain</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fitting a basic xgboost model to the training data</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">xgboost</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html" class="external-link">xgboost</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">x_train</span><span class="op">)</span>,</span>
<span>  label <span class="op">=</span> <span class="va">y_train</span>,</span>
<span>  nround <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Specifying the phi_0, i.e. the expected prediction without any features</span></span>
<span><span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Computing the actual Shapley values with Kernel SHAP accounting for feature dependence using</span></span>
<span><span class="co"># the empirical (conditional) distribution approach with bandwidth parameter sigma = 0.1 (default)</span></span>
<span><span class="va">explanation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:12 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93756774fb.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Printing the Shapley values for the test data.</span></span>
<span><span class="co"># For more information about the interpretation of the values in the table, see ?shapr::explain.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">explanation</span><span class="op">$</span><span class="va">shapley_values_est</span><span class="op">)</span></span>
<span><span class="co">#&gt;    explain_id   none  Solar.R    Wind    Temp    Month</span></span>
<span><span class="co">#&gt;         &lt;int&gt;  &lt;num&gt;    &lt;num&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;</span></span>
<span><span class="co">#&gt; 1:          1 43.086 13.21173  4.7856 -25.572  -5.5992</span></span>
<span><span class="co">#&gt; 2:          2 43.086 -9.97277  5.8307 -11.039  -7.8300</span></span>
<span><span class="co">#&gt; 3:          3 43.086 -2.29162 -7.0534 -10.150  -4.4525</span></span>
<span><span class="co">#&gt; 4:          4 43.086  3.32546 -3.2409 -10.225  -6.6635</span></span>
<span><span class="co">#&gt; 5:          5 43.086  4.30396 -2.6278 -14.152 -12.2669</span></span>
<span><span class="co">#&gt; 6:          6 43.086  0.47864 -5.2487 -12.553  -6.6457</span></span>
<span></span>
<span><span class="co"># Plot the resulting explanations for observations 1 and 6</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation</span>, bar_plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fsetup-2-1.png"></p>
<p>There are multiple plot options specified by the
<code>plot_type</code> argument in <code>plot</code>. The
<code>waterfall</code> option shows the changes in the prediction score
due to each features contribution (their Shapley values):</p>
<p>There are multiple plot options specified by the
<code>plot_type</code> argument in <code>plot</code>. The
<code>waterfall</code> option shows the changes in the prediction score
due to each features contribution (their Shapley values):</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation</span>, plot_type <span class="op">=</span> <span class="st">"waterfall"</span>, index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fplot-waterfall-1.png"></p>
<p>The other two plot options, <code>"beeswarm"</code> and
<code>"scatter"</code>, can be useful when you have many observations
that you want to explain. For the purpose of illustration, we explain
the whole <code>airquality</code> dataset (including the training data)
for these plot types. The <code>plot_type = "beeswarm"</code> summarizes
the distribution of the Shapley values along the x-axis across all
features. Each point gives the Shapley value of a given instance, where
the points are colored by the feature value of that instance:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x_explain_many</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span><span class="va">explanation_plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain_many</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:19 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 111</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c931d8811ab.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation_plot</span>, plot_type <span class="op">=</span> <span class="st">"beeswarm"</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fplot-beeswarm-1.png"></p>
<p>The <code>plot_type = "scatter"</code> plots the feature values on
the x-axis and Shapley values on the y-axis, as well as (optionally) a
background scatter_hist showing the distribution of the feature
data:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation_plot</span>, plot_type <span class="op">=</span> <span class="st">"scatter"</span>, scatter_hist <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fplot-scatter-1.png"></p>
<p>We can use mixed (i.e continuous, categorical, ordinal) data with
<code>ctree</code> or <code>vaeac</code>. Use <code>ctree</code> with
mixed data in the following manner:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># convert the month variable to a factor</span></span>
<span><span class="va">data</span><span class="op">[</span>, <span class="va">Month_factor</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">Month</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="va">data_train_cat</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="op">]</span></span>
<span><span class="va">data_explain_cat</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">ind_x_explain</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">x_var_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Solar.R"</span>, <span class="st">"Wind"</span>, <span class="st">"Temp"</span>, <span class="st">"Month_factor"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x_train_cat</span> <span class="op">&lt;-</span> <span class="va">data_train_cat</span><span class="op">[</span>, <span class="va">..x_var_cat</span><span class="op">]</span></span>
<span><span class="va">x_explain_cat</span> <span class="op">&lt;-</span> <span class="va">data_explain_cat</span><span class="op">[</span>, <span class="va">..x_var_cat</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Fitting an lm model here as xgboost does not handle categorical features directly</span></span>
<span><span class="co"># (work around in example below)</span></span>
<span><span class="va">lm_formula</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">as.formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">y_var</span>, <span class="st">" ~ "</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">x_var_cat</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model_lm_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">lm_formula</span>, <span class="va">data_train_cat</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span></span>
<span><span class="va">explanation_lm_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_lm_cat</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain_cat</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train_cat</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"ctree"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:31 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;lm&gt;</span></span>
<span><span class="co">#&gt; • Approach: ctree</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c934d11d260.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Plot the resulting explanations for observations 1 and 6, excluding</span></span>
<span><span class="co"># the no-covariate effect</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation_lm_cat</span>, bar_plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Ffactor-1.png"></p>
<p>We can specify parameters used to build the conditional inference
trees in the following manner. The default values are based on <span class="citation">Hothorn, Hornik, and Zeileis (2006)</span>.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Use the conditional inference tree approach</span></span>
<span><span class="co"># We can specify parameters used to building trees by specifying mincriterion,</span></span>
<span><span class="co"># minsplit, minbucket</span></span>
<span><span class="va">explanation_ctree</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_lm_cat</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain_cat</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train_cat</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"ctree"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  ctree.mincriterion <span class="op">=</span> <span class="fl">0.80</span>,</span>
<span>  ctree.minsplit <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  ctree.minbucket <span class="op">=</span> <span class="fl">20</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:32 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;lm&gt;</span></span>
<span><span class="co">#&gt; • Approach: ctree</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93a8d4f63.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span><span class="co"># Default parameters (based on (Hothorn, 2006)) are:</span></span>
<span><span class="co"># mincriterion = 0.95</span></span>
<span><span class="co"># minsplit = 20</span></span>
<span><span class="co"># minbucket = 7</span></span></code></pre></div>
<p>If <strong>all</strong> features are categorical, one may use the
categorical approach as follows:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># For the sake of illustration, convert ALL features to factors</span></span>
<span><span class="va">data</span><span class="op">[</span>, <span class="va">Solar.R_factor</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cut.html" class="external-link">cut</a></span><span class="op">(</span><span class="va">Solar.R</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">data</span><span class="op">[</span>, <span class="va">Wind_factor</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cut.html" class="external-link">cut</a></span><span class="op">(</span><span class="va">Wind</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">data</span><span class="op">[</span>, <span class="va">Temp_factor</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cut.html" class="external-link">cut</a></span><span class="op">(</span><span class="va">Temp</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">data</span><span class="op">[</span>, <span class="va">Month_factor</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">Month</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="va">data_train_all_cat</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="op">]</span></span>
<span><span class="va">data_explain_all_cat</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">ind_x_explain</span>, <span class="op">]</span></span>
<span></span>
<span></span>
<span><span class="va">x_var_all_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Solar.R_factor"</span>, <span class="st">"Wind_factor"</span>, <span class="st">"Temp_factor"</span>, <span class="st">"Month_factor"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x_train_all_cat</span> <span class="op">&lt;-</span> <span class="va">data_train_all_cat</span><span class="op">[</span>, <span class="va">..x_var_all_cat</span><span class="op">]</span></span>
<span><span class="va">x_explain_all_cat</span> <span class="op">&lt;-</span> <span class="va">data_explain_all_cat</span><span class="op">[</span>, <span class="va">..x_var_all_cat</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Fit an lm model here</span></span>
<span><span class="va">lm_formula_all_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">as.formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">y_var</span>, <span class="st">" ~ "</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">x_var_all_cat</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model_lm_all_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">lm_formula_all_cat</span>, <span class="va">data_train_all_cat</span><span class="op">)</span></span>
<span></span>
<span><span class="va">explanation_cat_method</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_lm_all_cat</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain_all_cat</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train_all_cat</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"categorical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:33 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;lm&gt;</span></span>
<span><span class="co">#&gt; • Approach: categorical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9361d2d6a5.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span></code></pre></div>
<p>Shapley values can be used to explain any predictive model. For
predictive models taking time series as input,
<code>approach='timeseries'</code> can be used. In such models, joint
behavior of consecutive time points is often more important for the
outcome than the single time points. Therefore, it makes sense to derive
Shapley value segments of the time series instead of for each single
time point. In <code>shapr</code> this can be achieved through the
<code>group</code> attribute. Other optional parameters of
<code>approach='timeseries'</code> are
<code>timeseries.fixed_sigma</code> and <code>timeseries.bounds</code>
(a vector indicating upper and lower bounds of the time series if
necessary).</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate time series data with AR(1)-structure</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">data_ts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, ncol <span class="op">=</span> <span class="fl">41</span>, nrow <span class="op">=</span> <span class="fl">4</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span>  <span class="va">e</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">42</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">m_1</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">m_1</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">+</span> <span class="fl">0.8</span> <span class="op">*</span> <span class="va">m_1</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">e</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">data_ts</span><span class="op">[</span><span class="va">n</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">m_1</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span><span class="va">data_ts</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">data_ts</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x_var_ts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">40</span><span class="op">)</span></span>
<span><span class="va">y_var_ts</span> <span class="op">&lt;-</span> <span class="st">"X41"</span></span>
<span></span>
<span><span class="va">ind_x_explain</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span></span>
<span><span class="va">data_ts_train</span> <span class="op">&lt;-</span> <span class="va">data_ts</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Creating a predictive model (for illustration just predicting the next point in the time series with a linear model)</span></span>
<span><span class="va">lm_ts_formula</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">as.formula</a></span><span class="op">(</span><span class="va">X41</span> <span class="op">~</span> <span class="va">.</span><span class="op">)</span></span>
<span><span class="va">model_lm_ts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">lm_ts_formula</span>, <span class="va">data_ts_train</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x_explain_ts</span> <span class="op">&lt;-</span> <span class="va">data_ts</span><span class="op">[</span><span class="va">ind_x_explain</span>, <span class="va">..x_var_ts</span><span class="op">]</span></span>
<span><span class="va">x_train_ts</span> <span class="op">&lt;-</span> <span class="va">data_ts</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="va">..x_var_ts</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Spitting the time series into 4 segments</span></span>
<span><span class="va">group_ts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  S1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>  S2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">11</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span>,</span>
<span>  S3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">21</span><span class="op">:</span><span class="fl">30</span><span class="op">)</span>,</span>
<span>  S4 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">31</span><span class="op">:</span><span class="fl">40</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">p0_ts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/unlist.html" class="external-link">unlist</a></span><span class="op">(</span><span class="va">data_ts_train</span><span class="op">[</span>, <span class="va">..y_var_ts</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">explanation_timeseries</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_lm_ts</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain_ts</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train_ts</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"timeseries"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0_ts</span>,</span>
<span>  group <span class="op">=</span> <span class="va">group_ts</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_groups = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_groups = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:33 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;lm&gt;</span></span>
<span><span class="co">#&gt; • Approach: timeseries</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of group-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c933989300e.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span></code></pre></div>
<div class="section level3">
<h3 id="msev-evaluation-criterion">MSEv evaluation criterion<a class="anchor" aria-label="anchor" href="#msev-evaluation-criterion"></a>
</h3>
<p>We can use the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion proposed by <span class="citation">Frye et al. (2021)</span>,
and later used by, e.g., <span class="citation">Olsen et al.
(2022)</span> and <span class="citation">Olsen et al. (2024)</span>, to
evaluate and rank the approaches/methods. The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
is given by</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mo>MSE</mo><mi>v</mi></msub><mo>=</mo><msub><mo>MSE</mo><mi>v</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mtext mathvariant="normal">method </mtext><mspace width="0.333em"></mspace></mrow><mtext mathvariant="monospace">𝚚</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><msub><mi>N</mi><mi>𝒮</mi></msub></mfrac><munder><mo>∑</mo><mrow><mi>𝒮</mi><mo>∈</mo><msup><mi>𝒫</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ℳ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><mfrac><mn>1</mn><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐱</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo stretchy="true" form="postfix">]</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mover><mi>v</mi><mo accent="true">̂</mo></mover><mtext mathvariant="monospace">𝚚</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo>,</mo><msup><mi>𝐱</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo stretchy="true" form="postfix">]</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mspace width="-0.167em"></mspace><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
    \label{eq:MSE_v}
    \operatorname{MSE}_{v} = \operatorname{MSE}_{v}(\text{method } \texttt{q})
    =
     \frac{1}{N_\mathcal{S}} \sum_{\mathcal{S} \in \mathcal{P}^*(\mathcal{M})} \frac{1}{N_\text{explain}}
     \sum_{i=1}^{N_\text{explain}} \left( f(\boldsymbol{x}^{[i]}) - {\hat{v}}_{\texttt{q}}(\mathcal{S}, \boldsymbol{x}^{[i]})\right)^2\!,
\end{align}</annotation></semantics></math><p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>v</mi><mo accent="true">̂</mo></mover><mtext mathvariant="monospace">𝚚</mtext></msub><annotation encoding="application/x-tex">{\hat{v}}_{\texttt{q}}</annotation></semantics></math>
is the estimated contribution function using method
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtext mathvariant="monospace">𝚚</mtext><annotation encoding="application/x-tex">\texttt{q}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>𝒮</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>𝒫</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ℳ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mo>=</mo><msup><mn>2</mn><mi>M</mi></msup><mo>−</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N_\mathcal{S} = |\mathcal{P}^*(\mathcal{M})| = 2^M-2</annotation></semantics></math>,
i.e., we have removed the empty
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒮</mi><mo>=</mo><mi>∅</mi></mrow><annotation encoding="application/x-tex">\mathcal{S} = \emptyset</annotation></semantics></math>)
and the grand combinations
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒮</mi><mo>=</mo><mi>ℳ</mi></mrow><annotation encoding="application/x-tex">\mathcal{S} = \mathcal{M}</annotation></semantics></math>)
as they are method independent. Meaning that these two combinations do
not influence the ranking of the methods as the methods are not used to
compute the contribution function for them.</p>
<p>The motivation behind the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion is that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝔼</mi><mi>𝒮</mi></msub><msub><mi>𝔼</mi><mi>𝐱</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>v</mi><mtext mathvariant="monospace">𝚝𝚛𝚞𝚎</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mover><mi>v</mi><mo accent="true">̂</mo></mover><mtext mathvariant="monospace">𝚚</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mathbb{E}_\mathcal{S}\mathbb{E}_{\boldsymbol{x}} (v_{\texttt{true}}(\mathcal{S},\boldsymbol{x}) - \hat{v}_{\texttt{q}}(\mathcal{S}, \boldsymbol{x}))^2</annotation></semantics></math>
can be decomposed as</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>𝔼</mi><mi>𝒮</mi></msub><msub><mi>𝔼</mi><mi>𝐱</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>v</mi><mtext mathvariant="monospace">𝚝𝚛𝚞𝚎</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mover><mi>v</mi><mo accent="true">̂</mo></mover><mtext mathvariant="monospace">𝚚</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>𝔼</mi><mi>𝒮</mi></msub><msub><mi>𝔼</mi><mi>𝐱</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mover><mi>v</mi><mo accent="true">̂</mo></mover><mtext mathvariant="monospace">𝚚</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mphantom><mrow><mspace width="0.167em"></mspace><mspace width="0.167em"></mspace><mspace width="0.167em"></mspace><mspace width="0.167em"></mspace><mspace width="0.167em"></mspace><mspace width="0.167em"></mspace><mspace width="0.167em"></mspace></mrow></mphantom><mo>−</mo><msub><mi>𝔼</mi><mi>𝒮</mi></msub><msub><mi>𝔼</mi><mi>𝐱</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>v</mi><mtext mathvariant="monospace">𝚝𝚛𝚞𝚎</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒮</mi><mo>,</mo><mi>𝐱</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mtd></mtr></mtable></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
    \label{eq:expectation_decomposition}
    \begin{split}
    \mathbb{E}_\mathcal{S}\mathbb{E}_{\boldsymbol{x}} (v_{\texttt{true}}(\mathcal{S}, \boldsymbol{x})- \hat{v}_{\texttt{q}}(\mathcal{S}, \boldsymbol{x}))^2
    &amp;=
    \mathbb{E}_\mathcal{S}\mathbb{E}_{\boldsymbol{x}} (f(\boldsymbol{x}) - \hat{v}_{\texttt{q}}(\mathcal{S}, \boldsymbol{x}))^2 \\
    &amp;\phantom{\,\,\,\,\,\,\,}- \mathbb{E}_\mathcal{S}\mathbb{E}_{\boldsymbol{x}} (f(\boldsymbol{x})-v_{\texttt{true}}(\mathcal{S}, \boldsymbol{x}))^2,
    \end{split}
\end{align}</annotation></semantics></math><p>see Appendix A in <span class="citation">Covert, Lundberg, and Lee
(2020)</span>. The first term on the right-hand side of the equation
above can be estimated by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>,
while the second term is a fixed (unknown) constant not influenced by
the approach . Thus, a low value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
indicates that the estimated contribution function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>v</mi><mo accent="true">̂</mo></mover><mtext mathvariant="monospace">𝚚</mtext></msub><annotation encoding="application/x-tex">\hat{v}_{\texttt{q}}</annotation></semantics></math>
is closer to the true counterpart
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mtext mathvariant="monospace">𝚝𝚛𝚞𝚎</mtext></msub><annotation encoding="application/x-tex">v_{\texttt{true}}</annotation></semantics></math>
than a high value.</p>
<p>In <code>shapr</code>, we allow for weighting the combinations in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
evaluation criterion either uniformly or by using the corresponding
Shapley kernel weights (or the sampling frequencies when sampling of
combinations is used). This is determined by the logical parameter
<code>MSEv_uniform_comb_weights</code> in the <code><a href="../reference/explain.html">explain()</a></code>
function, and the default is to do uniform weighting, that is,
<code>MSEv_uniform_comb_weights = TRUE</code>.</p>
<div class="section level4">
<h4 id="advantage">Advantage:<a class="anchor" aria-label="anchor" href="#advantage"></a>
</h4>
<p>An advantage of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion is that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mtext mathvariant="monospace">𝚝𝚛𝚞𝚎</mtext></msub><annotation encoding="application/x-tex">v_\texttt{true}</annotation></semantics></math>
is not involved. Thus, we can apply it as an evaluation criterion to
real-world data sets where the true Shapley values are unknown.</p>
</div>
<div class="section level4">
<h4 id="disadvantages">Disadvantages:<a class="anchor" aria-label="anchor" href="#disadvantages"></a>
</h4>
<p>First, we can only use the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion to rank the methods and not assess their closeness to the
optimum since the minimum value of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion is unknown. Second, the criterion evaluates the contribution
functions and not the Shapley values.</p>
<p><span class="citation">Olsen et al. (2024)</span> observed a
relatively linear relationship between the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion and the mean absolute error
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mo>MAE</mo><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\operatorname{MAE})</annotation></semantics></math>
between the true and estimated Shapley values in extensive simulation
studies where the true Shapley values were known. That is, a method that
achieves a low
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
score also tends to obtain a low
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>MAE</mo><annotation encoding="application/x-tex">\operatorname{MAE}</annotation></semantics></math>
score, and vice versa.</p>
</div>
<div class="section level4">
<h4 id="confidence-intervals">Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"></a>
</h4>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion can be written as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>MSE</mo><mi>v</mi></msub><mo>=</mo><mfrac><mn>1</mn><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub></msubsup><msub><mo>MSE</mo><mrow><mi>v</mi><mo>,</mo><mrow><mtext mathvariant="normal">explain </mtext><mspace width="0.333em"></mspace></mrow><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\operatorname{MSE}_{v} = \frac{1}{N_\text{explain}}\sum_{i=1}^{N_\text{explain}} \operatorname{MSE}_{v,\text{explain }i}</annotation></semantics></math>.
We can therefore use the central limit theorem to compute an approximate
confidence interval for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion. We have that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>MSE</mo><mi>v</mi></msub><mo>±</mo><msub><mi>t</mi><mrow><mi>α</mi><mi>/</mi><mn>2</mn></mrow></msub><mfrac><mrow><mo>SD</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>MSE</mo><mi>v</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msqrt><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\operatorname{MSE}_{v} \pm t_{\alpha/2}\frac{\operatorname{SD}(\operatorname{MSE}_{v})}{\sqrt{N_\text{explain}}}</annotation></semantics></math>
is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mi>/</mi><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>%</mi></mrow><annotation encoding="application/x-tex">(1-\alpha/2)\%</annotation></semantics></math>
approximate confidence interval for the evaluation criterion, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mrow><mi>α</mi><mi>/</mi><mn>2</mn></mrow></msub><annotation encoding="application/x-tex">t_{\alpha/2}</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\alpha/2</annotation></semantics></math>
percentile of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>T</mi><mrow><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">T_{N_\text{explain}-1}</annotation></semantics></math>
distribution. Note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub><annotation encoding="application/x-tex">N_\text{explain}</annotation></semantics></math>
should be large (rule of thumb is at least
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>30</mn><annotation encoding="application/x-tex">30</annotation></semantics></math>)
for the central limit theorem to be valid. The quantities
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mo>SD</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>MSE</mo><mi>v</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msqrt><msub><mi>N</mi><mtext mathvariant="normal">explain</mtext></msub></msqrt></mfrac><annotation encoding="application/x-tex">\frac{\operatorname{SD}(\operatorname{MSE}_{v})}{\sqrt{N_\text{explain}}}</annotation></semantics></math>
are returned by the <code><a href="../reference/explain.html">explain()</a></code> function in the
<code>MSEv</code> list of data tables. We can also compute similar
approximate confidence interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
criterion for each combination/coalition when only averaging over the
observations. However, it does not make sense in the other direction,
i.e., when only averaging over the combinations for each observation, as
each combination is a different prediction tasks.</p>
</div>
<div class="section level4">
<h4 id="msev-examples">MSEv examples<a class="anchor" aria-label="anchor" href="#msev-examples"></a>
</h4>
<p>Start by explaining the predictions by using different methods and
combining them into lists.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># We use more explicands here for more stable confidence intervals</span></span>
<span><span class="va">ind_x_explain_many</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">25</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain_many</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span><span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain_many</span>, <span class="fu"><a href="https://rdrr.io/r/base/get.html" class="external-link">get</a></span><span class="op">(</span><span class="va">y_var</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">x_explain</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">ind_x_explain_many</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Fitting a basic xgboost model to the training data</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">xgboost</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html" class="external-link">xgboost</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">x_train</span><span class="op">)</span>,</span>
<span>  label <span class="op">=</span> <span class="va">y_train</span>,</span>
<span>  nround <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Specifying the phi_0, i.e. the expected prediction without any features</span></span>
<span><span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Independence approach</span></span>
<span><span class="va">explanation_independence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"independence"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  n_MC_samples <span class="op">=</span> <span class="fl">1e2</span>,</span>
<span>  MSEv_uniform_comb_weights <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:35 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: independence</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 25</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9321f2ff20.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Empirical approach</span></span>
<span><span class="va">explanation_empirical</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  n_MC_samples <span class="op">=</span> <span class="fl">1e2</span>,</span>
<span>  MSEv_uniform_comb_weights <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:36 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 25</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9372002e18.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Gaussian 1e1 approach</span></span>
<span><span class="va">explanation_gaussian_1e1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"gaussian"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  n_MC_samples <span class="op">=</span> <span class="fl">1e1</span>,</span>
<span>  MSEv_uniform_comb_weights <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:43 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: gaussian</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 25</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9371245336.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Gaussian 1e2 approach</span></span>
<span><span class="va">explanation_gaussian_1e2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"gaussian"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  n_MC_samples <span class="op">=</span> <span class="fl">1e2</span>,</span>
<span>  MSEv_uniform_comb_weights <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:43 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: gaussian</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 25</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93155a4322.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Combined approach</span></span>
<span><span class="va">explanation_combined</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"gaussian"</span>, <span class="st">"empirical"</span>, <span class="st">"independence"</span><span class="op">)</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  n_MC_samples <span class="op">=</span> <span class="fl">1e2</span>,</span>
<span>  MSEv_uniform_comb_weights <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:43 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: gaussian, empirical, and independence</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 25</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c937ba6bac.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Create a list of explanations with names</span></span>
<span><span class="va">explanation_list_named</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="st">"Ind."</span> <span class="op">=</span> <span class="va">explanation_independence</span>,</span>
<span>  <span class="st">"Emp."</span> <span class="op">=</span> <span class="va">explanation_empirical</span>,</span>
<span>  <span class="st">"Gaus. 1e1"</span> <span class="op">=</span> <span class="va">explanation_gaussian_1e1</span>,</span>
<span>  <span class="st">"Gaus. 1e2"</span> <span class="op">=</span> <span class="va">explanation_gaussian_1e2</span>,</span>
<span>  <span class="st">"Combined"</span> <span class="op">=</span> <span class="va">explanation_combined</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>We can then compare the different approaches by creating plots of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
evaluation criterion.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create the MSEv plots with approximate 95% confidence intervals</span></span>
<span><span class="va">MSEv_plots</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/plot_MSEv_eval_crit.html">plot_MSEv_eval_crit</a></span><span class="op">(</span><span class="va">explanation_list_named</span>,</span>
<span>  plot_type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"overall"</span>, <span class="st">"comb"</span>, <span class="st">"explicand"</span><span class="op">)</span>,</span>
<span>  CI_level <span class="op">=</span> <span class="fl">0.95</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 5 plots are made</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">MSEv_plots</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "MSEv_explicand_bar"        "MSEv_explicand_line_point" "MSEv_coalition_bar"       </span></span>
<span><span class="co">#&gt; [4] "MSEv_coalition_line_point" "MSEv_bar"</span></span></code></pre></div>
<p>The main plot if interest is the <code>MSEv_bar</code>, which
displays the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>MSE</mo><mi>v</mi></msub><annotation encoding="application/x-tex">\operatorname{MSE}_{v}</annotation></semantics></math>
evaluation criterion for each method averaged over both the
combinations/coalitions and test observations/explicands. However, we
can also look at the other plots where we have only averaged over the
observations or the combinations (both as bar and line plots).</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The main plot of the overall MSEv averaged over both the combinations and observations</span></span>
<span><span class="va">MSEv_plots</span><span class="op">$</span><span class="va">MSEv_bar</span></span></code></pre></div>
<p><img src="figure_general_usage%2FMSEv-plot-2-1.png"></p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># The MSEv averaged over only the explicands for each combinations</span></span>
<span><span class="va">MSEv_plots</span><span class="op">$</span><span class="va">MSEv_combination_bar</span></span>
<span><span class="co">#&gt; NULL</span></span>
<span></span>
<span><span class="co"># The MSEv averaged over only the combinations for each observation/explicand</span></span>
<span><span class="va">MSEv_plots</span><span class="op">$</span><span class="va">MSEv_explicand_bar</span></span></code></pre></div>
<p><img src="figure_general_usage%2FMSEv-plot-2-2.png"></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># To see which coalition S each of the `id_combination` corresponds to,</span></span>
<span><span class="co"># i.e., which features that are conditions on.</span></span>
<span><span class="va">explanation_list_named</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">MSEv</span><span class="op">$</span><span class="va">MSEv_combination</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"id_combination"</span>, <span class="st">"features"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co">#&gt; NULL</span></span></code></pre></div>
<p>We can specify the <code>index_x_explain</code> and
<code>id_combination</code> parameters in
<code><a href="../reference/plot_MSEv_eval_crit.html">plot_MSEv_eval_crit()</a></code> to only plot certain test
observations and combinations, respectively.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># We can specify which test observations or combinations to plot</span></span>
<span><span class="fu"><a href="../reference/plot_MSEv_eval_crit.html">plot_MSEv_eval_crit</a></span><span class="op">(</span><span class="va">explanation_list_named</span>,</span>
<span>  plot_type <span class="op">=</span> <span class="st">"explicand"</span>,</span>
<span>  index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  CI_level <span class="op">=</span> <span class="fl">0.95</span></span>
<span><span class="op">)</span><span class="op">$</span><span class="va">MSEv_explicand_bar</span></span></code></pre></div>
<p><img src="figure_general_usage%2FMSEv-plot-3-1.png"></p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plot_MSEv_eval_crit.html">plot_MSEv_eval_crit</a></span><span class="op">(</span><span class="va">explanation_list_named</span>,</span>
<span>  plot_type <span class="op">=</span> <span class="st">"comb"</span>,</span>
<span>  id_coalition <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">9</span>, <span class="fl">13</span><span class="op">:</span><span class="fl">15</span><span class="op">)</span>,</span>
<span>  CI_level <span class="op">=</span> <span class="fl">0.95</span></span>
<span><span class="op">)</span><span class="op">$</span><span class="va">MSEv_combination_bar</span></span>
<span><span class="co">#&gt; NULL</span></span></code></pre></div>
<p>We can also alter the plots design-wise as we do in the code
below.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bar_text_n_decimals</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="fu"><a href="../reference/plot_MSEv_eval_crit.html">plot_MSEv_eval_crit</a></span><span class="op">(</span><span class="va">explanation_list_named</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_discrete.html" class="external-link">scale_x_discrete</a></span><span class="op">(</span>limits <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rev.html" class="external-link">rev</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/levels.html" class="external-link">levels</a></span><span class="op">(</span><span class="va">MSEv_plots</span><span class="op">$</span><span class="va">MSEv_bar</span><span class="op">$</span><span class="va">data</span><span class="op">$</span><span class="va">Method</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_flip.html" class="external-link">coord_flip</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_brewer.html" class="external-link">scale_fill_brewer</a></span><span class="op">(</span>palette <span class="op">=</span> <span class="st">"Paired"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="co"># This must be set before other theme calls</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html" class="external-link">theme</a></span><span class="op">(</span></span>
<span>    plot.title <span class="op">=</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html" class="external-link">element_text</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>    legend.position <span class="op">=</span> <span class="st">"bottom"</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html" class="external-link">geom_text</a></span><span class="op">(</span></span>
<span>    <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="st">"%."</span>, <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"%d"</span>, <span class="va">bar_text_n_decimals</span><span class="op">)</span>, <span class="st">"f"</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">MSEv</span>, <span class="va">bar_text_n_decimals</span><span class="op">)</span></span>
<span>    <span class="op">)</span><span class="op">)</span>,</span>
<span>    vjust <span class="op">=</span> <span class="op">-</span><span class="fl">0.35</span>, <span class="co"># This number might need altering for different plots sizes</span></span>
<span>    hjust <span class="op">=</span> <span class="fl">1.1</span>, <span class="co"># This number might need altering for different plots sizes</span></span>
<span>    color <span class="op">=</span> <span class="st">"black"</span>,</span>
<span>    position <span class="op">=</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/position_dodge.html" class="external-link">position_dodge</a></span><span class="op">(</span><span class="fl">0.9</span><span class="op">)</span>,</span>
<span>    size <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2FMSEv-plot-4-1.png"></p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="iterative">Iterative estimation<a class="anchor" aria-label="anchor" href="#iterative"></a>
</h2>
<p>Iterative estimation is the default when computing Shapley values
with six or more features (or feature groups), and can always be
manually overridden by setting <code>iterative = FALSE</code> in the
<code><a href="../reference/explain.html">explain()</a></code> function. The idea behind iterative estimation is
to estimate sufficiently accurate Shapley value estimates faster. First,
an initial number of coalitions is sampled, then, bootsrapping is used
to estimate the variance of the Shapley values. A convergence criterion
is used to determine if the variances of the Shapley values are
sufficiently small. If the variances are too high, we estimate the
number of required samples to reach convergence, and thereby add more
coalitions. The process is repeated until the variances are below the
threshold. Specifics related to the iterative process and convergence
criterion are set through <code>iterative_args</code> argument.</p>
<p>The convergence criterion we use is adopted from <span class="citation">Covert and Lee (2021)</span>, and slightly modified to
work for multiple observations</p>
<p><span class="math display">$$ \median_i\left(\frac{max_j
\hat{\text{sd}}(\hat{\phi}_{ij}){\max_j \hat{\phi}_{ij} - \min_j
\hat{\phi}_{ij}}\right), &lt; t  $$</span></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>ϕ</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\phi}_{ij}</annotation></semantics></math>
is the Shapley value of feature
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
for observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">sd</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ϕ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{sd}(\phi_{ij})</annotation></semantics></math>
is the its (bootstrap) estimated standard deviation. The default value
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>
is 0.02. Below we provide some examples of how to use the iterative
estimation procedure.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/dmlc/xgboost" class="external-link">xgboost</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com" class="external-link">data.table</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"airquality"</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html" class="external-link">complete.cases</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">x_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Solar.R"</span>, <span class="st">"Wind"</span>, <span class="st">"Temp"</span>, <span class="st">"Month"</span>, <span class="st">"Day"</span><span class="op">)</span></span>
<span><span class="va">y_var</span> <span class="op">&lt;-</span> <span class="st">"Ozone"</span></span>
<span></span>
<span><span class="va">ind_x_explain</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span><span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="fu"><a href="https://rdrr.io/r/base/get.html" class="external-link">get</a></span><span class="op">(</span><span class="va">y_var</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">x_explain</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fitting a basic xgboost model to the training data</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">xgboost</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html" class="external-link">xgboost</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">x_train</span><span class="op">)</span>,</span>
<span>  label <span class="op">=</span> <span class="va">y_train</span>,</span>
<span>  nround <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Specifying the phi_0, i.e. the expected prediction without any features</span></span>
<span><span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initial explanation computation</span></span>
<span><span class="va">ex</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"gaussian"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  iterative <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  iterative_args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>convergence_tol <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 32, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 32.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:48 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: gaussian</span></span>
<span><span class="co">#&gt; • Iterative estimation: TRUE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 5</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c933097162d.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── iterative computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 1 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 5 of 32 coalitions, 5 new.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 2 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 10 of 32 coalitions, 4 new.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 3 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 12 of 32 coalitions, 2 new.</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="para">Parallelization<a class="anchor" aria-label="anchor" href="#para"></a>
</h2>
<p>The <code>shapr</code> package supports parallelization of the
Shapley value estimation process through the <code>future</code>
package. The parallelization is conducted over batches of
<code>v(S)</code>-values. We therefore start by describing this batch
computing.</p>
<div class="section level3">
<h3 id="batch-computation">Batch computation<a class="anchor" aria-label="anchor" href="#batch-computation"></a>
</h3>
<p>The computational complexity of Shapley value based explanations
grows fast in the number of features, as the number of conditional
expectations one needs to estimate in the Shapley formula grows
exponentially. As outlined <a href="#KSHAP">above</a>, the estimating of
each of these conditional expectations is also computationally
expensive, typically requiring estimation of a conditional probability
distribution, followed by Monte Carlo integration. These computations
are not only heavy for the CPU, they also require a lot of memory (RAM),
which typically is a limited resource. By doing the most resource hungry
computations (the computation of v(S)) in sequential batches with
different feature subsets
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>,
the memory usage can be significantly reduces. The user can control the
number of batches by setting the two arguments
<code>extra_computation_args$max_batch_size</code> (defaults to 10) and
<code>extra_computation_args$min_n_batches</code> (defaults to 10).</p>
</div>
<div class="section level3">
<h3 id="parallelized-computation">Parallelized computation<a class="anchor" aria-label="anchor" href="#parallelized-computation"></a>
</h3>
<p>In addition to reducing the memory consumption, the batch computing
allows the computations within each batch to be performed in parallel.
The parallelization in <code><a href="../reference/explain.html">shapr::explain()</a></code> is handled by the
<code>future_apply</code> package which builds on the
<code>future</code> environment. These packages work on all OS, allows
the user to decide the parallelization backend (multiple R processes or
forking), works directly with hpc clusters, and also supports progress
updates for the parallelized task via the associated
<code>progressr</code> package (see <a href="#verbose">Verbosity and
progress updates</a>).</p>
<p>Note that, since it takes some time to duplicate data into different
processes/machines when running in parallel, it is not always preferable
to run <code><a href="../reference/explain.html">shapr::explain()</a></code> in parallel, at least not with many
parallel sessions/workers. Parallelization also increases the memory
consumption proportionally, so you may want to limit the number of
workers for that reason too. Below is a basic example of a
parallelization with two workers.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://future.futureverse.org" class="external-link">future</a></span><span class="op">)</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html" class="external-link">plan</a></span><span class="op">(</span><span class="va">multisession</span>, workers <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">explanation_par</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 32, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 32.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:51 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 5</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9348628d8d.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 32 of 32 coalitions.</span></span>
<span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html" class="external-link">plan</a></span><span class="op">(</span><span class="va">sequential</span><span class="op">)</span> <span class="co"># To return to non-parallel computation</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="verbose">Verbosity and progress updates<a class="anchor" aria-label="anchor" href="#verbose"></a>
</h2>
<p>The <code>verbose</code> argument controls the verbosity of the
output while running <code><a href="../reference/explain.html">explain()</a></code>, and allows one or more of
the strings <code>"basic"</code>, <code>"progress"</code>,
<code>"convergence"</code>, <code>"shapley"</code> and
<code>"vS_details"</code>. <code>"basic"</code> (default) displays basic
information about the computation which is being performed,
<code>"progress</code> displays information about where in the
calculation process the function currently is,
<code>"convergence"</code> displays information on how close to
convergence the Shapley value estimates are (for iterative estimation),
<code>"shapley"</code> displays (intermediate) Shapley value estimates
and standard deviations + the final estimates, while
<code>"vS_details"</code> displays information about the
<code>v(S)</code> estimates for some of the approaches. If the user
wants no printout, the argument can be set to <code>NULL</code>.</p>
<p>In addition, progress updates for the computation of the
<code>v(S)</code> values are (optionally) provided through the R-package
<code>progressr</code>. This gives the user full control over the visual
appearance of these progress updates. The main reason for providing this
separate progress update feature is that it integrates seamlessly with
the parallelization framework <code>future</code> used by
<code>shapr</code> (see <a href="#para">Parallelization</a>), and
apparently is the only framework allowing progress updates also for
parallelized tasks. These progress updates can be used in combination
with, or independently of, the <code>verbose</code> argument.</p>
<p>These progress updates via <code>progressr</code> can be enabled for
the current R-session by running the command
<code>progressr::handlers(local=TRUE)</code>, before calling
<code><a href="../reference/explain.html">explain()</a></code>. To use progress updates for only a single call
to <code><a href="../reference/explain.html">explain()</a></code>, one can wrap the call using
<code><a href="https://progressr.futureverse.org/reference/with_progress.html" class="external-link">progressr::with_progress</a></code> as follows:
<code>progressr::with_progress({ shapr::explain() })</code> The default
appearance of the progress updates is a basic ASCII-based horizontal
progress bar. Other variants can be chosen by passing different strings
to <code><a href="https://progressr.futureverse.org/reference/handlers.html" class="external-link">progressr::handlers()</a></code>, some of which require additional
packages. If you are using Rstudio, the progress can be displayed
directly in the gui with <code>progressr::handlers('rstudio')</code>
(requires the <code>rstudioapi</code> package). If you are running
Windows, you may use the pop-up gui progress bar
<code>progressr::handlers('handler_winprogressbar')</code>. A wrapper
for progressbar of the flexible <code>cli</code> package, is also
available <code>progressr::handlers('cli')</code>.</p>
<p>For a full list of all progression handlers and the customization
options available with <code>progressr</code>, see the
<code>progressr</code> <a href="https://progressr.futureverse.org/articles/progressr-intro.html" class="external-link">vignette</a>.
A full code example of using <code>progressr</code> with
<code>shapr</code> is shown below:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://progressr.futureverse.org" class="external-link">progressr</a></span><span class="op">)</span></span>
<span><span class="fu">progressr</span><span class="fu">::</span><span class="fu"><a href="https://progressr.futureverse.org/reference/handlers.html" class="external-link">handlers</a></span><span class="op">(</span>global <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://progressr.futureverse.org/reference/handlers.html" class="external-link">handlers</a></span><span class="op">(</span><span class="st">"cli"</span><span class="op">)</span></span>
<span><span class="co"># If no progression handler is specified, the txtprogressbar is used</span></span>
<span><span class="co"># Other progression handlers:</span></span>
<span><span class="co"># progressr::handlers('progress') # requires the 'progress' package</span></span>
<span><span class="co"># progressr::handlers('rstudio') # requires the 'rstudioapi' package</span></span>
<span><span class="co"># progressr::handlers('handler_winprogressbar') # Window only</span></span>
<span><span class="va">ex_progress</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ■■■■■■■■■■■                       32% | Estimating v(S) ETA:  2s</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="advanced">Advanced usage<a class="anchor" aria-label="anchor" href="#advanced"></a>
</h2>
<div class="section level3">
<h3 id="combined">Combined approach<a class="anchor" aria-label="anchor" href="#combined"></a>
</h3>
<p>In addition to letting the user select one of the five aforementioned
approaches for estimating the conditional distribution of the data (i.e.
<code>approach</code> equals either <a href="#gaussian"><code>"gaussian"</code></a>, <a href="#copula"><code>"copula"</code></a>, <a href="#empirical"><code>"empirical"</code></a>, <a href="#ctree"><code>"ctree"</code></a>, <a href="#vaeac"><code>"vaeac"</code></a>, <a href="#categorical"><code>"categorical"</code></a>) or
<code>"timeseries"</code>, the package allows the user to combine the
given approaches. The <code>'regression_surrogate'</code> and
<code>'regression_separate</code> approaches are not supported for the
combined approach. To simplify the usage, the flexibility is restricted
such that the same approach is used when conditioning on the same number
of features. This is also in line <span class="citation">Aas, Jullum,
and Løland (2021, sec. 3.4)</span>.</p>
<p>This can be done by setting <code>approach</code> equal to a
character vector, where the length of the vector is one less than the
number of features in the model. Consider a situation where you have
trained a model that consists of 10 features, and you would like to use
the <code>"empirical"</code> approach when you condition on 1-3
features, the <code>"copula"</code> approach when you condition on 4-5
features, and the <code>"gaussian"</code> approach when conditioning on
6 or more features. This can be applied by simply passing
<code>approach = c(rep("empirical", 3), rep("copula", 2), rep("gaussian", 4))</code>,
i.e. <code>approach[i]</code> determines which method to use when
conditioning on <code>i</code> features. Conditioning on all features
needs no approach as that is given by the complete prediction itself,
and should thus not be part of the vector.</p>
<p>The code below exemplifies this approach for a case where there are
four features, using <code>"empirical", "copula"</code> and
<code>"gaussian"</code> when conditioning on respectively 1, 2 and 3
features.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/dmlc/xgboost" class="external-link">xgboost</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com" class="external-link">data.table</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"airquality"</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html" class="external-link">complete.cases</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">x_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Solar.R"</span>, <span class="st">"Wind"</span>, <span class="st">"Temp"</span>, <span class="st">"Month"</span><span class="op">)</span></span>
<span><span class="va">y_var</span> <span class="op">&lt;-</span> <span class="st">"Ozone"</span></span>
<span></span>
<span><span class="va">ind_x_explain</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span><span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="fu"><a href="https://rdrr.io/r/base/get.html" class="external-link">get</a></span><span class="op">(</span><span class="va">y_var</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">x_explain</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fitting a basic xgboost model to the training data</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">xgboost</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html" class="external-link">xgboost</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">x_train</span><span class="op">)</span>,</span>
<span>  label <span class="op">=</span> <span class="va">y_train</span>,</span>
<span>  nround <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Specifying the phi_0, i.e. the expected prediction without any features</span></span>
<span><span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Use the combined approach</span></span>
<span><span class="va">explanation_combined</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"empirical"</span>, <span class="st">"copula"</span>, <span class="st">"gaussian"</span><span class="op">)</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:54 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical, copula, and gaussian</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c936a965bd5.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span><span class="co"># Plot the resulting explanations for observations 1 and 6, excluding</span></span>
<span><span class="co"># the no-covariate effect</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation_combined</span>, bar_plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fcombined-1-1.png"></p>
<p>As a second example using <code>"ctree"</code> to condition on 1 and
2 features, and <code>"empirical"</code> when conditioning on 3
features:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Use the combined approach</span></span>
<span><span class="va">explanation_combined</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ctree"</span>, <span class="st">"ctree"</span>, <span class="st">"empirical"</span><span class="op">)</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:57 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: ctree, ctree, and empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93207aeb18.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="explain-groups-of-features">Explain groups of features<a class="anchor" aria-label="anchor" href="#explain-groups-of-features"></a>
</h3>
<p>In some cases, especially when the number of features is very large,
it may be more appropriate to explain predictions in terms of groups of
features instead of single features, see (<span class="citation">Jullum,
Redelmeier, and Aas (2021)</span>) for intuition and real world
examples. Explaining prediction in terms of groups of features is very
easy using <code>shapr</code>:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define the feature groups</span></span>
<span><span class="va">group_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  A <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Temp"</span>, <span class="st">"Month"</span><span class="op">)</span>,</span>
<span>  B <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Wind"</span>, <span class="st">"Solar.R"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Use the empirical approach</span></span>
<span><span class="va">explanation_group</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  group <span class="op">=</span> <span class="va">group_list</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_groups = 4, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_groups = 4.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:44:59 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of group-wise Shapley values: 2</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93241851b9.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 4 of 4 coalitions.</span></span>
<span><span class="co"># Prints the group-wise explanations</span></span>
<span><span class="va">explanation_group</span></span>
<span><span class="co">#&gt;    explain_id  none      A        B</span></span>
<span><span class="co">#&gt;         &lt;int&gt; &lt;num&gt;  &lt;num&gt;    &lt;num&gt;</span></span>
<span><span class="co">#&gt; 1:          1 43.09 -29.25  16.0731</span></span>
<span><span class="co">#&gt; 2:          2 43.09 -15.17  -7.8373</span></span>
<span><span class="co">#&gt; 3:          3 43.09 -13.07 -10.8778</span></span>
<span><span class="co">#&gt; 4:          4 43.09 -17.47   0.6653</span></span>
<span><span class="co">#&gt; 5:          5 43.09 -28.27   3.5289</span></span>
<span><span class="co">#&gt; 6:          6 43.09 -20.59  -3.3793</span></span>
<span><span class="co"># Plots the group-wise explanations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation_group</span>, bar_plot_phi0 <span class="op">=</span> <span class="cn">TRUE</span>, index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fgroup-1.png"></p>
</div>
<div class="section level3">
<h3 id="explain-custom-models">Explain custom models<a class="anchor" aria-label="anchor" href="#explain-custom-models"></a>
</h3>
<p><code>shapr</code> currently natively supports explanation of
predictions from models fitted with the following functions:</p>
<ul>
<li><code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">stats::lm</a></code></li>
<li><code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">stats::glm</a></code></li>
<li><code><a href="http://imbs-hl.github.io/ranger/reference/ranger.html" class="external-link">ranger::ranger</a></code></li>
<li><code><a href="https://rdrr.io/pkg/mgcv/man/gam.html" class="external-link">mgcv::gam</a></code></li>
<li>
<code><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html" class="external-link">xgboost::xgboost</a></code>/<code><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html" class="external-link">xgboost::xgb.train</a></code>
</li>
<li><code><a href="https://workflows.tidymodels.org/reference/workflow.html" class="external-link">workflows::workflow</a></code></li>
</ul>
<p>Any continuous response regression model or binary classification
model of these model classes, can be explained with the package directly
as exemplified above, while we give an example for the
<code><a href="https://workflows.tidymodels.org/reference/workflow.html" class="external-link">workflows::workflow</a></code> in the <a href="#workflow_example"><code>tidymodels</code>/<code>workflows</code></a>
section. Moreover, essentially any feature dependent prediction model
can be explained by the package by specifying two (or one) simple
additional functions for your model.</p>
<p>The first function is <code>predict_model</code>, taking the model
and data (as a <code>matrix</code> or
<code>data.frame/data.table</code>) as input and outputting the
corresponding prediction as a numeric vector. The second (optional, but
highly recommended) function is <code>get_model_specs</code>, taking the
model as input and outputting a list with the following elements:
<em>labels</em> (vector with the feature names to compute Shapley values
for), <em>classes</em> (a named vector with the labels as names and the
class type as elements), <em>factor_levels</em> (a named list with the
labels as names and vectors with the factor levels as elements (NULL if
the feature is not a factor)). The <code>get_model_specs</code> function
is used to check that the format of the data passed to
<code>explain</code> have the correct format in terms of the necessary
feature columns being available and having the correct class/attributes.
It is highly recommended to do such checks in order to ensure correct
usage of <code>explain</code>. If, for some reason, such checking is not
desirable, one does not have to provide the <code>get_model_specs</code>
function. This will, however, throw a warning that all feature
consistency checking against the model is disabled.</p>
<p>Once the above functions are created, you can explain predictions
from this model as before by passing the functions through the input
arguments <code>predict_model</code> and <code>get_model_specs</code> of
<code><a href="../reference/explain.html">explain()</a></code>.</p>
<p>These functions <strong>can</strong> be made general enough to handle
all supported model types of that class, or they can be made minimal,
possibly only allowing explanation of the specific version of the model
class at hand. Below we give examples of both full support versions of
these functions and a minimal version which skips the
<code>get_model_specs</code> function. We do this for the
<code>gbm</code> model class from the <code>gbm</code> package, fitted
to the same airquality data set as used above.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/gbm-developers/gbm" class="external-link">gbm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">formula_gbm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">as.formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">y_var</span>, <span class="st">"~"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">x_var</span>, collapse <span class="op">=</span> <span class="st">"+"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Fitting a gbm model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">825</span><span class="op">)</span></span>
<span><span class="va">model_gbm</span> <span class="op">&lt;-</span> <span class="fu">gbm</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/gbm/man/gbm.html" class="external-link">gbm</a></span><span class="op">(</span></span>
<span>  <span class="va">formula_gbm</span>,</span>
<span>  data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">x_train</span>, Ozone <span class="op">=</span> <span class="va">y_train</span><span class="op">)</span>,</span>
<span>  distribution <span class="op">=</span> <span class="st">"gaussian"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co">#### Full feature versions of the three required model functions ####</span></span>
<span><span class="va">MY_predict_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">newdata</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html" class="external-link">requireNamespace</a></span><span class="op">(</span><span class="st">"gbm"</span>, quietly <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop</a></span><span class="op">(</span><span class="st">"The gbm package is required for predicting train models"</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">model_type</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span></span>
<span>    <span class="va">x</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="va">name</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"bernoulli"</span>, <span class="st">"adaboost"</span><span class="op">)</span>,</span>
<span>    <span class="st">"classification"</span>,</span>
<span>    <span class="st">"regression"</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">model_type</span> <span class="op">==</span> <span class="st">"classification"</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">newdata</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"response"</span>, n.trees <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">n.trees</span><span class="op">)</span></span>
<span>  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">newdata</span><span class="op">)</span>, n.trees <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">n.trees</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span><span class="va">MY_get_model_specs</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">feature_specs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="va">feature_specs</span><span class="op">$</span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/labels.html" class="external-link">labels</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">Terms</span><span class="op">)</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">feature_specs</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span>  <span class="va">feature_specs</span><span class="op">$</span><span class="va">classes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">Terms</span>, <span class="st">"dataClasses"</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">feature_specs</span><span class="op">$</span><span class="va">factor_levels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/setNames.html" class="external-link">setNames</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">vector</a></span><span class="op">(</span><span class="st">"list"</span>, <span class="va">m</span><span class="op">)</span>, <span class="va">feature_specs</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span>  <span class="va">feature_specs</span><span class="op">$</span><span class="va">factor_levels</span><span class="op">[</span><span class="va">feature_specs</span><span class="op">$</span><span class="va">classes</span> <span class="op">==</span> <span class="st">"factor"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span> <span class="co"># model object doesn't contain factor levels info</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">feature_specs</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Compute the Shapley values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span></span>
<span><span class="va">explanation_custom</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_gbm</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  predict_model <span class="op">=</span> <span class="va">MY_predict_model</span>,</span>
<span>  get_model_specs <span class="op">=</span> <span class="va">MY_get_model_specs</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:45:01 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;gbm&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c933748f819.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Plot results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation_custom</span>, index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fcustom-1.png"></p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span></span>
<span><span class="co">#### Minimal version of the custom model setup ####</span></span>
<span><span class="co"># Note: Working only for this exact version of the model class</span></span>
<span><span class="co"># Avoiding to define get_model_specs skips all feature</span></span>
<span><span class="co"># consistency checking between your data and model</span></span>
<span><span class="va">MY_MINIMAL_predict_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">newdata</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">newdata</span><span class="op">)</span>, n.trees <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">n.trees</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Compute the Shapley values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">explanation_custom_minimal</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_gbm</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  predict_model <span class="op">=</span> <span class="va">MY_MINIMAL_predict_model</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: You passed a model to explain() which is not natively supported, and did not supply a 'get_model_specs' function to explain().</span></span>
<span><span class="co">#&gt; Consistency checks between model and data is therefore disabled.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:45:07 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;gbm&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9344b0ff8e.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># Plot results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">explanation_custom_minimal</span>, index_x_explain <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fcustom-2.png"></p>
</div>
<div class="section level3">
<h3 id="workflow_example">Tidymodels and workflows<a class="anchor" aria-label="anchor" href="#workflow_example"></a>
</h3>
<p>In this section, we demonstrate how to use <code>shapr</code> to
explain <code>tidymodels</code> models fitted using
<code>workflows</code>. In the example <a href="#examples">above</a>, we
directly used the <code>xgboost</code> package to fit the
<code>xgboost</code> model. However, we can also fit the
<code>xgboost</code> model using the <code>tidymodels</code> package.
These fits will be identical as <code>tidymodels</code> calls
<code>xgboost</code> internally. which we demonstrate in the example
below. Note that we can replace <code>xgboost</code> (i.e.,
<code><a href="https://parsnip.tidymodels.org/reference/boost_tree.html" class="external-link">parsnip::boost_tree</a></code>) with any other fitted
<code>tidymodels</code> in the <code>workflows</code> procedure outlined
below.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fitting a basic xgboost model to the training data using tidymodels</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> <span class="co"># Set the same seed as above</span></span>
<span><span class="va">all_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">y_var</span>, <span class="va">x_var</span><span class="op">)</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="va">..all_var</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Fitting the `tidymodels` model using `workflows`</span></span>
<span><span class="va">model_tidymodels</span> <span class="op">&lt;-</span> <span class="fu">parsnip</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>  <span class="fu">workflows</span><span class="fu">::</span><span class="fu"><a href="https://workflows.tidymodels.org/reference/add_recipe.html" class="external-link">add_recipe</a></span><span class="op">(</span></span>
<span>    <span class="fu">workflows</span><span class="fu">::</span><span class="fu"><a href="https://workflows.tidymodels.org/reference/add_model.html" class="external-link">add_model</a></span><span class="op">(</span></span>
<span>      <span class="fu">workflows</span><span class="fu">::</span><span class="fu"><a href="https://workflows.tidymodels.org/reference/workflow.html" class="external-link">workflow</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>      <span class="fu">parsnip</span><span class="fu">::</span><span class="fu"><a href="https://parsnip.tidymodels.org/reference/boost_tree.html" class="external-link">boost_tree</a></span><span class="op">(</span>trees <span class="op">=</span> <span class="fl">20</span>, engine <span class="op">=</span> <span class="st">"xgboost"</span>, mode <span class="op">=</span> <span class="st">"regression"</span><span class="op">)</span></span>
<span>    <span class="op">)</span>,</span>
<span>    <span class="fu">recipes</span><span class="fu">::</span><span class="fu"><a href="https://recipes.tidymodels.org/reference/recipe.html" class="external-link">recipe</a></span><span class="op">(</span><span class="va">Ozone</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">train</span><span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">train</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># # We can also specify the same model using pipes `%&gt;%` by (if pipes are installed/loaded)</span></span>
<span><span class="co"># model_tidymodels &lt;-</span></span>
<span><span class="co">#   workflows::workflow() %&gt;%</span></span>
<span><span class="co">#   workflows::add_model(parsnip::boost_tree(trees = 20, engine = "xgboost", mode = "regression")) %&gt;%</span></span>
<span><span class="co">#   workflows::add_recipe(recipes::recipe(Ozone ~ ., data = train)) %&gt;%</span></span>
<span><span class="co">#   parsnip::fit(data = train)</span></span>
<span></span>
<span><span class="co"># See that the output of the two models are identical</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/all.equal.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model_tidymodels</span>, <span class="va">x_train</span><span class="op">)</span><span class="op">$</span><span class="va">.pred</span>, <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model</span>, <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">x_train</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span></span>
<span><span class="co"># Create the Shapley values for the tidymodels version</span></span>
<span><span class="va">explanation_tidymodels</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_tidymodels</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:45:13 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;workflow&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93476395f2.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span>
<span></span>
<span><span class="co"># See that the Shapley value explanations are identical too</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/all.equal.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="va">explanation</span><span class="op">$</span><span class="va">shapley_values_est</span>, <span class="va">explanation_tidymodels</span><span class="op">$</span><span class="va">shapley_values_est</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="the-parameters-of-the-vaeac-approach">The parameters of the <code>vaeac</code> approach<a class="anchor" aria-label="anchor" href="#the-parameters-of-the-vaeac-approach"></a>
</h3>
<p>The <code>vaeac</code> approach is a very flexible method that
supports mixed data. The main parameters are the the number of layers in
the networks (<code>vaeac.depth</code>), the width of the layers
(<code>vaeac.width</code>), the number of dimensions in the latent space
(<code>vaeac.latent_dim</code>), the activation function between the
layers in the networks (<code>vaeac.activation_function</code>), the
learning rate in the ADAM optimizer (<code>vaeac.lr</code>), the number
of <code>vaeac</code> models to initiate to remedy poorly initiated
model parameter values (<code>vaeac.n_vaeacs_initialize</code>), and the
number of learning epochs (<code>vaeac.epochs</code>). Call
<code><a href="../reference/setup_approach.html">?shapr::setup_approach.vaeac</a></code> for a more detailed
description of the parameters.</p>
<p>There are additional extra parameters which can be set by including a
named list in the call to the <code><a href="../reference/explain.html">explain()</a></code> function. For
example, we can the change the batch size to 32 by including
<code>vaeac.extra_parameters = list(vaeac.batch_size = 32)</code> as a
parameter in the call the <code><a href="../reference/explain.html">explain()</a></code> function. See
<code><a href="../reference/vaeac_get_extra_para_default.html">?shapr::vaeac_get_extra_para_default</a></code> for a description of
the possible extra parameters to the <code>vaeac</code> approach. The
main parameters are directly entered to the <code><a href="../reference/explain.html">explain()</a></code>
function, while the extra parameters are included in a named list called
<code>vaeac.extra_parameters</code>.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">explanation_vaeac</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"vaeac"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  n_MC_samples <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  vaeac.width <span class="op">=</span> <span class="fl">16</span>,</span>
<span>  vaeac.depth <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  vaeac.epochs <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  vaeac.n_vaeacs_initialize <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:45:19 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: vaeac</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c932349a0b1.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span></code></pre></div>
<p>Can look at the training and validation error for the trained
<code>vaeac</code> model and see that <code>vaeac.epochs = 3</code> is
likely to few epochs as it still seems like the <code>vaeac</code> model
is learning.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Look at the training and validation errors.</span></span>
<span><span class="fu"><a href="../reference/plot_vaeac_eval_crit.html">plot_vaeac_eval_crit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"Vaeac 3 epochs"</span> <span class="op">=</span> <span class="va">explanation_vaeac</span><span class="op">)</span>, plot_type <span class="op">=</span> <span class="st">"method"</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fvaeac-plot-1-1.png"></p>
<div class="section level4">
<h4 id="early-stopping">Early stopping<a class="anchor" aria-label="anchor" href="#early-stopping"></a>
</h4>
<p>If we are uncertain about the choice of <code>vaeac.epochs</code>, we
can rather use <code>vaeac</code> with early stopping. We will then set
<code>vaeac.epochs</code> to a large number which will act as a maximum
number of allowed epochs and in the <code>vaeac.extra_parameters</code>
list, we set <code>vaeac.epochs_early_stopping</code> the number of
epochs we allow the <code>vaeac</code> model to not improve its
validation score. That is, if
<code>vaeac.epochs_early_stopping = 2</code>, then <code>vaeac</code>
will stop the training procedure if there has been no improvement in the
validation score for <code>2</code> consecutive epochs, of if
<code>vaeac.epochs</code> is reached. Note that if using early stopping
and progress updates simultaneously, then the estimated timer remaining
will obviously be incorrect if early stopping is applied. Furthermore, a
value of <code>2</code> is too low for real world applications, but we
set it so low here to make the vignette faster to build.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">explanation_vaeac_early_stop</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"vaeac"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  n_MC_samples <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  vaeac.width <span class="op">=</span> <span class="fl">16</span>,</span>
<span>  vaeac.depth <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  vaeac.epochs <span class="op">=</span> <span class="fl">1000</span>, <span class="co"># Set it to a large number</span></span>
<span>  vaeac.n_vaeacs_initialize <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  vaeac.extra_parameters <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>vaeac.epochs_early_stopping <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 16, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 16.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:45:34 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: vaeac</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 4</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9361ebec61.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 16 of 16 coalitions.</span></span></code></pre></div>
<p>Can compare with the previous version and see that the results are
more stable now.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Look at the training and validation errors.</span></span>
<span><span class="fu"><a href="../reference/plot_vaeac_eval_crit.html">plot_vaeac_eval_crit</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"Vaeac 3 epochs"</span> <span class="op">=</span> <span class="va">explanation_vaeac</span>, <span class="st">"Vaeac early stopping"</span> <span class="op">=</span> <span class="va">explanation_vaeac_early_stop</span><span class="op">)</span>,</span>
<span>  plot_type <span class="op">=</span> <span class="st">"method"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fvaeac-plot-2-1.png"></p>
<p>Can also compare the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>S</mi><msub><mi>E</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">MSE_{v}</annotation></semantics></math>
evaluation scores.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plot_MSEv_eval_crit.html">plot_MSEv_eval_crit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"Vaeac 3 epochs"</span> <span class="op">=</span> <span class="va">explanation_vaeac</span>, <span class="st">"Vaeac early stopping"</span> <span class="op">=</span> <span class="va">explanation_vaeac_early_stop</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="figure_general_usage%2Fvaeac-plot-3-1.png"></p>
</div>
</div>
<div class="section level3">
<h3 id="cont_computation">Continued computation<a class="anchor" aria-label="anchor" href="#cont_computation"></a>
</h3>
<p>In this section, we demonstrate how to continue to improve estimation
accuracy with additional coalition samples, from a previous Shapley
value computation based on <code><a href="../reference/explain.html">shapr::explain()</a></code> with the
iterative estimation procedure. This can be done either by passing an
existing object of class <code>shapr</code>, or by passing a string with
the path to the intermediately saved results. The latter is found at
<code>SHAPR_OBJ$saving_path</code>, defaults to a temporary folder, and
is updated after each iteration. This can be particularly handy for
long-running computations.</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># First we run the computation with the iterative estimation procedure for a limited number of coalition samples</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/dmlc/xgboost" class="external-link">xgboost</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com" class="external-link">data.table</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"airquality"</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html" class="external-link">complete.cases</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">x_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Solar.R"</span>, <span class="st">"Wind"</span>, <span class="st">"Temp"</span>, <span class="st">"Month"</span>, <span class="st">"Day"</span><span class="op">)</span></span>
<span><span class="va">y_var</span> <span class="op">&lt;-</span> <span class="st">"Ozone"</span></span>
<span></span>
<span><span class="va">ind_x_explain</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span><span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">-</span><span class="va">ind_x_explain</span>, <span class="fu"><a href="https://rdrr.io/r/base/get.html" class="external-link">get</a></span><span class="op">(</span><span class="va">y_var</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">x_explain</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">ind_x_explain</span>, <span class="va">..x_var</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fitting a basic xgboost model to the training data</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">xgboost</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html" class="external-link">xgboost</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">x_train</span><span class="op">)</span>,</span>
<span>  label <span class="op">=</span> <span class="va">y_train</span>,</span>
<span>  nround <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Specifying the phi_0, i.e. the expected prediction without any features</span></span>
<span><span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initial explanation computation</span></span>
<span><span class="va">ex_init</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"gaussian"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  max_n_coalitions <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  iterative <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:45:57 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: gaussian</span></span>
<span><span class="co">#&gt; • Iterative estimation: TRUE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 5</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93667c79.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── iterative computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 1 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 5 of 32 coalitions, 5 new.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 2 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 10 of 32 coalitions, 4 new.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 3 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 12 of 32 coalitions, 2 new.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 4 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 16 of 32 coalitions, 4 new.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Iteration 5 ───────────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; ℹ Using 18 of 32 coalitions, 2 new.</span></span>
<span></span>
<span><span class="co"># Using the ex_init object to continue the computation with 5 more coalition samples</span></span>
<span><span class="va">ex_further</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"gaussian"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  max_n_coalitions <span class="op">=</span> <span class="fl">25</span>,</span>
<span>  iterative_args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>convergence_tol <span class="op">=</span> <span class="fl">0.005</span><span class="op">)</span>, <span class="co"># Decrease the convergence threshold</span></span>
<span>  prev_shapr_object <span class="op">=</span> <span class="va">ex_init</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:46:01 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: gaussian</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 5</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c937cd2ac71.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 24 of 32 coalitions.</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">ex_further</span><span class="op">$</span><span class="va">saving_path</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "/tmp/RtmpCcS4cy/shapr_obj_1b8c937cd2ac71.rds"</span></span>
<span></span>
<span><span class="co"># Using the ex_init object to continue the computation for the remaining coalition samples</span></span>
<span><span class="co"># but this time using the path to the saved intermediate estimation object</span></span>
<span><span class="va">ex_even_further</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model</span>,</span>
<span>  x_explain <span class="op">=</span> <span class="va">x_explain</span>,</span>
<span>  x_train <span class="op">=</span> <span class="va">x_train</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"gaussian"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0</span>,</span>
<span>  max_n_coalitions <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  prev_shapr_object <span class="op">=</span> <span class="va">ex_further</span><span class="op">$</span><span class="va">saving_path</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature classes extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Assuming feature classes from the data are correct.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 32, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 32.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:46:02 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;xgb.Booster&gt;</span></span>
<span><span class="co">#&gt; • Approach: gaussian</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 5</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 6</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c936aa37ec8.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 32 of 32 coalitions.</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="forecasting">Explaining a forecasting model using
<code>explain_forecast</code><a class="anchor" aria-label="anchor" href="#forecasting"></a>
</h2>
<p><code>shapr</code> provides a specific function,
<code>explain_forecast</code>, to explain forecasts from time series
models, at one or more steps into the future. The main difference
compared to <code>explain</code> is that the data is supplied as (set
of) time series, in addition to index arguments (<code>train_idx</code>
and <code>explain_idx</code>) specifying which time points that
represents the train and explain parts of the data. See
<code><a href="../reference/explain_forecast.html">?explain_forecast</a></code> for more information.</p>
<p>To demonstrate how to use the function, 500 observations are
generated which follow an AR(1) structure, i.e.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mn>0.5</mn><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ε</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t = 0.5 y_{t-1} + \varepsilon_t</annotation></semantics></math>.
To this data an arima model of order (2, 0, 0) is fitted, and we
therefore would like to explain the forecasts in terms of the two
previous lags of the time series. This is is specified through the
argument <code>explain_y_lags = 2</code>. Note that some models may also
put restrictions on the amount of data required to make a forecast. The
AR(2) model we used there, for instance, requires two previous time
point to make a forecast.</p>
<p>In the example, two separate forecasts, each three steps ahead, are
explained. To set the starting points of the two forecasts,
<code>explain_idx</code> is set to <code>499:500</code>. This means that
one forecast of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>500</mn><mo>,</mo><mn>501</mn><mo>,</mo><mn>502</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">t = (500, 501, 502)</annotation></semantics></math>
and another of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>501</mn><mo>,</mo><mn>502</mn><mo>,</mo><mn>503</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">t = (501, 502, 503)</annotation></semantics></math>,
will be explained. In other words, <code>explain_idx</code> tells
<code>shapr</code> at which points in time data was available up until,
when making the forecast to explain.</p>
<p>In the same way, <code>train_idx</code> denotes the points in time
used to estimate the conditional expectations used to explain the
different forecasts. Note that since we want to explain the forecasts in
terms of the two previous lags (<code>explain_y_lags = 2</code>), the
smallest value of <code>train_idx</code> must also be 2, because at time
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t = 1</annotation></semantics></math>
there was only a single observation available.</p>
<p>Since the data is stationary, the mean of the data is used as value
of <code>phi0</code>
(i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ϕ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\phi_0</annotation></semantics></math>).
This can however be chosen differently depending on the data and
application.</p>
<p>For a multivariate model such as a VAR (Vector AutoRegressive model),
it may be more interesting to explain the impact of each variable,
rather than each lag of each variable. This can be done by setting
<code>group_lags = TRUE</code>.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate time series data with AR(1)-structure.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">data_ts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>Y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.sim.html" class="external-link">arima.sim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>order <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, ar <span class="op">=</span> <span class="fl">.5</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">500</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">data_ts</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">data_ts</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit an ARIMA(2, 0, 0) model.</span></span>
<span><span class="va">arima_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html" class="external-link">arima</a></span><span class="op">(</span><span class="va">data_ts</span>, order <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set prediction zero as the mean of the data for each forecast point.</span></span>
<span><span class="va">p0_ar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data_ts</span><span class="op">$</span><span class="va">Y</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Explain forecasts from points t = 499 and t = 500.</span></span>
<span><span class="va">explain_idx</span> <span class="op">&lt;-</span> <span class="fl">499</span><span class="op">:</span><span class="fl">500</span></span>
<span></span>
<span><span class="va">explanation_forecast</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain_forecast.html">explain_forecast</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">arima_model</span>,</span>
<span>  y <span class="op">=</span> <span class="va">data_ts</span>,</span>
<span>  train_idx <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">498</span>,</span>
<span>  explain_idx <span class="op">=</span> <span class="fl">499</span><span class="op">:</span><span class="fl">500</span>,</span>
<span>  explain_y_lags <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  horizon <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="va">p0_ar</span>,</span>
<span>  group_lags <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature names extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Consistency checks between model and data is therefore disabled.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 4, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 4.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:46:03 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;Arima&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 2</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 2</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c931eee7980.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 4 of 4 coalitions.</span></span>
<span><span class="va">explanation_forecast</span></span>
<span><span class="co">#&gt;    explain_idx horizon    none     Y.1      Y.2</span></span>
<span><span class="co">#&gt;          &lt;int&gt;   &lt;int&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;</span></span>
<span><span class="co">#&gt; 1:         499       1 0.04018  0.5053 -0.07659</span></span>
<span><span class="co">#&gt; 2:         500       1 0.04018 -0.3622  0.02497</span></span>
<span><span class="co">#&gt; 3:         499       2 0.04018  0.5053 -0.07659</span></span>
<span><span class="co">#&gt; 4:         500       2 0.04018 -0.3622  0.02497</span></span>
<span><span class="co">#&gt; 5:         499       3 0.04018  0.5053 -0.07659</span></span>
<span><span class="co">#&gt; 6:         500       3 0.04018 -0.3622  0.02497</span></span></code></pre></div>
<p>Note that for a multivariate model such as a VAR (Vector
AutoRegressive model), or for models also including several exogenous
variables, it may be of more informative to explain the impact of each
variable, rather than each lag of each variable. This can be done by
setting <code>group_lags = TRUE</code>. This does not make sense for
this model, however, as that would result in decomposing the forecast
into a single group.</p>
<p>We now give a more hands on example of how to use the
<code>explain_forecast</code> function. Say that we have an AR(2) model
which describes the change over time of the variable <code>Temp</code>
in the dataset <code>airquality</code>. It seems reasonable to assume
that the temperature today should affect the temperature tomorrow. To a
lesser extent, we may also suggest that the temperature today should
also have an impact on that of the day after tomorrow.</p>
<p>We start by building our AR(2) model, naming it
<code>model_ar_temp</code>. This model is then used to make a forecast
of the temperature of the day that comes after the last day in the data,
this forecast starts from index 153.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data_ts2</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model_ar_temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ar.html" class="external-link">ar</a></span><span class="op">(</span><span class="va">data_ts2</span><span class="op">$</span><span class="va">Temp</span>, order <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model_ar_temp</span>, n.ahead <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">$</span><span class="va">pred</span></span>
<span><span class="co">#&gt; Time Series:</span></span>
<span><span class="co">#&gt; Start = 154 </span></span>
<span><span class="co">#&gt; End = 155 </span></span>
<span><span class="co">#&gt; Frequency = 1 </span></span>
<span><span class="co">#&gt; [1] 71.081 71.524</span></span></code></pre></div>
<p>First, we pass the model and the data as <code>model</code> and
<code>y</code>. Since we have an AR(2) model, we want to explain the
forecasts in terms of the two previous lags, which we specify with
<code>explain_y_lags = 2</code>. Then, we let <code>shapr</code> know
which time indices to use as training data through the argument
<code>train_idx</code>. We use <code>2:152</code>, meaning that we skip
the first index, as we want to explain the two previous lags. Letting
the training indices go up until 152 means that every point in time
except the first and last will be used as training data.</p>
<p>The last index, 153 is passed as the argument
<code>explain_idx</code>, which means that we want to explain a forecast
made from time point 153 in the data. The argument <code>horizon</code>
is set to 2 in order to explain a forecast of length 2.</p>
<p>The argument <code>phi0</code> is set to the mean of the time series,
and is repeated two times. Each value of <code>phi0</code> is the
baseline for each forecast horizon. In our example, we assume that given
no effect from the two lags, the temperature would just be the average
during the observed period. Finally, we opt to not group the lags by
setting <code>group_lags</code> to <code>FALSE</code>. This means that
lag 1 and 2 will be explained separately. Grouping lags may be more
interesting to do in a model with multiple variables, as it is then
possible to explain each variable separately.</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">explanation_forecast</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain_forecast.html">explain_forecast</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_ar_temp</span>,</span>
<span>  y <span class="op">=</span> <span class="va">data_ts2</span><span class="op">[</span>, <span class="st">"Temp"</span><span class="op">]</span>,</span>
<span>  train_idx <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">152</span>,</span>
<span>  explain_idx <span class="op">=</span> <span class="fl">153</span>,</span>
<span>  explain_y_lags <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  horizon <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">Temp</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>  group_lags <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature names extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Consistency checks between model and data is therefore disabled.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 4, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 4.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:46:05 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;ar&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 2</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 1</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c9360892168.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 4 of 4 coalitions.</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">explanation_forecast</span><span class="op">)</span></span>
<span><span class="co">#&gt;    explain_idx horizon  none Temp.1 Temp.2</span></span>
<span><span class="co">#&gt;          &lt;num&gt;   &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;</span></span>
<span><span class="co">#&gt; 1:         153       1 77.79 -6.578 -0.134</span></span>
<span><span class="co">#&gt; 2:         153       2 77.79 -5.980 -0.288</span></span></code></pre></div>
<p>The results are presented per value of <code>explain_idx</code> and
forecast horizon. We can see that the mean temperature was around 77.9
degrees. At horizon 1, the first lag in the model caused it to be 6.6
degrees lower, and the second lag had just a minor effect. At horizon 2,
the first lag has a slightly smaller negative impact, and the second lag
has a slightly larger impact.</p>
<p>It is also possible to explain a forecasting model which uses
exogenous regressors. The previous example is expanded to use an
ARIMA(2,0,0) model with <code>Wind</code> as an exogenous regressor.
Since the exogenous regressor must be available for the predicted time
points, the model is just fit on the 151 first observations, leaving two
observations of <code>Wind</code> to be used as exogenous values during
the prediction phase.</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data_ts3</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_fit</span> <span class="op">&lt;-</span> <span class="va">data_ts3</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="fl">151</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">model_arimax_temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html" class="external-link">arima</a></span><span class="op">(</span><span class="va">data_fit</span><span class="op">$</span><span class="va">Temp</span>, order <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, xreg <span class="op">=</span> <span class="va">data_fit</span><span class="op">$</span><span class="va">Wind</span><span class="op">)</span></span>
<span></span>
<span><span class="va">newxreg</span> <span class="op">&lt;-</span> <span class="va">data_ts3</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="fl">151</span><span class="op">)</span>, <span class="st">"Wind"</span>, drop <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">model_arimax_temp</span>, n.ahead <span class="op">=</span> <span class="fl">2</span>, newxreg <span class="op">=</span> <span class="va">newxreg</span><span class="op">)</span><span class="op">$</span><span class="va">pred</span></span>
<span><span class="co">#&gt; Time Series:</span></span>
<span><span class="co">#&gt; Start = 152 </span></span>
<span><span class="co">#&gt; End = 153 </span></span>
<span><span class="co">#&gt; Frequency = 1 </span></span>
<span><span class="co">#&gt; [1] 77.500 76.381</span></span></code></pre></div>
<p>The <code>shapr</code> package can then explain not only the two
autoregressive lags, but also the single lag of the exogenous regressor.
In order to do so, the <code>Wind</code> variable is passed as the
argument <code>xreg</code>, and <code>explain_xreg_lags</code> is set to
1. Notice how only the first 151 observations are used for
<code>y</code> and all 153 are used for <code>xreg</code>. This makes it
possible for <code>shapr</code> to not only explain the effect of the
first lag of the exogenous variable, but also the contemporary effect
during the forecasting period.</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">explanation_forecast</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain_forecast.html">explain_forecast</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="va">model_ar_temp</span>,</span>
<span>  y <span class="op">=</span> <span class="va">data_fit</span><span class="op">[</span>, <span class="st">"Temp"</span><span class="op">]</span>,</span>
<span>  xreg <span class="op">=</span> <span class="va">data_ts3</span><span class="op">[</span>, <span class="st">"Wind"</span><span class="op">]</span>,</span>
<span>  train_idx <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">150</span>,</span>
<span>  explain_idx <span class="op">=</span> <span class="fl">151</span>,</span>
<span>  explain_y_lags <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  explain_xreg_lags <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  horizon <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  approach <span class="op">=</span> <span class="st">"empirical"</span>,</span>
<span>  phi0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data_fit</span><span class="op">$</span><span class="va">Temp</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>  group_lags <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: Feature names extracted from the model contains NA.</span></span>
<span><span class="co">#&gt; Consistency checks between model and data is therefore disabled.</span></span>
<span><span class="co">#&gt; Success with message:</span></span>
<span><span class="co">#&gt; max_n_coalitions is NULL or larger than or 2^n_features = 32, </span></span>
<span><span class="co">#&gt; and is therefore set to 2^n_features = 32.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Starting `shapr::explain()` at 2024-11-21 20:46:07 ────────────────────────────────────</span></span>
<span><span class="co">#&gt; • Model class: &lt;ar&gt;</span></span>
<span><span class="co">#&gt; • Approach: empirical</span></span>
<span><span class="co">#&gt; • Iterative estimation: FALSE</span></span>
<span><span class="co">#&gt; • Number of feature-wise Shapley values: 5</span></span>
<span><span class="co">#&gt; • Number of observations to explain: 1</span></span>
<span><span class="co">#&gt; • Computations (temporary) saved at: '/tmp/RtmpCcS4cy/shapr_obj_1b8c93398eb42e.rds'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Main computation started ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ Using 32 of 32 coalitions.</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">explanation_forecast</span><span class="op">$</span><span class="va">shapley_values_est</span><span class="op">)</span></span>
<span><span class="co">#&gt;    explain_idx horizon  none   Temp.1   Temp.2  Wind.1  Wind.F1  Wind.F2</span></span>
<span><span class="co">#&gt;          &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt;</span></span>
<span><span class="co">#&gt; 1:         151       1 77.96 -0.67793 -0.67340 -1.2688 0.493408       NA</span></span>
<span><span class="co">#&gt; 2:         151       2 77.96  0.39968 -0.50059 -1.4655 0.065913 -0.47422</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-aas2019explaining" class="csl-entry">
Aas, Kjersti, Martin Jullum, and Anders Løland. 2021. <span>“Explaining
Individual Predictions When Features Are Dependent: More Accurate
Approximations to Shapley Values.”</span> <em><span>Artificial
Intelligence</span></em> 298.
</div>
<div id="ref-covert2021improving" class="csl-entry">
Covert, Ian, and Su-In Lee. 2021. <span>“Improving Kernelshap: Practical
Shapley Value Estimation Using Linear Regression.”</span> In
<em>International Conference on Artificial Intelligence and
Statistics</em>, 3457–65. PMLR.
</div>
<div id="ref-covert2020understanding" class="csl-entry">
Covert, Ian, Scott M Lundberg, and Su-In Lee. 2020. <span>“Understanding
Global Feature Contributions with Additive Importance Measures.”</span>
<em>Advances in Neural Information Processing Systems</em> 33: 17212–23.
</div>
<div id="ref-torch" class="csl-entry">
Falbel, Daniel, and Javier Luraschi. 2023. <em>Torch: Tensors and Neural
Networks with ’GPU’ Acceleration</em>. <a href="https://CRAN.R-project.org/package=torch" class="external-link">https://CRAN.R-project.org/package=torch</a>.
</div>
<div id="ref-frye2020shapley" class="csl-entry">
Frye, Christopher, Damien de Mijolla, Tom Begley, Laurence Cowton, Megan
Stanley, and Ilya Feige. 2021. <span>“Shapley Explainability on the Data
Manifold.”</span> In <em>International Conference on Learning
Representations</em>.
</div>
<div id="ref-hothorn2006unbiased" class="csl-entry">
Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. <span>“Unbiased
Recursive Partitioning: A Conditional Inference Framework.”</span>
<em>Journal of Computational and Graphical Statistics</em> 15 (3):
651–74.
</div>
<div id="ref-partykit_package" class="csl-entry">
Hothorn, Torsten, and Achim Zeileis. 2015. <span>“<span class="nocase">partykit</span>: A Modular Toolkit for Recursive
Partytioning in <span>R</span>.”</span> <em>Journal of Machine Learning
Research</em> 16: 3905–9.
</div>
<div id="ref-jullum2021efficient" class="csl-entry">
Jullum, Martin, Annabelle Redelmeier, and Kjersti Aas. 2021.
<span>“Efficient and Simple Prediction Explanations with groupShapley: A
Practical Perspective.”</span> In <em>Proceedings of the 2nd Italian
Workshop on Explainable Artificial Intelligence</em>, 28–43. CEUR
Workshop Proceedings.
</div>
<div id="ref-kingma2014autoencoding" class="csl-entry">
Kingma, Diederik P., and Max Welling. 2014. <span>“<span>Auto-Encoding
Variational Bayes</span>.”</span> In <em>2nd International Conference on
Learning Representations, <span>ICLR</span> 2014, Banff, AB, Canada,
April 14-16, 2014, Conference Track Proceedings</em>.
</div>
<div id="ref-lundberg2017unified" class="csl-entry">
Lundberg, Scott M, and Su-In Lee. 2017. <span>“A Unified Approach to
Interpreting Model Predictions.”</span> In <em>Advances in Neural
Information Processing Systems</em>, 4765–74.
</div>
<div id="ref-olsen2022using" class="csl-entry">
Olsen, Lars Henry Berge, Ingrid Kristine Glad, Martin Jullum, and
Kjersti Aas. 2022. <span>“Using Shapley Values and Variational
Autoencoders to Explain Predictive Models with Dependent Mixed
Features.”</span> <em>Journal of Machine Learning Research</em> 23
(213): 1–51.
</div>
<div id="ref-olsen2024comparative" class="csl-entry">
———. 2024. <span>“A Comparative Study of Methods for Estimating
Model-Agnostic Shapley Value Explanations.”</span> <em>Data Mining and
Knowledge Discovery</em>, 1–48.
</div>
<div id="ref-redelmeier2020explaining" class="csl-entry">
Redelmeier, Annabelle, Martin Jullum, and Kjersti Aas. 2020.
<span>“Explaining Predictive Models with Mixed Features Using Shapley
Values and Conditional Inference Trees.”</span> In <em>International
Cross-Domain Conference for Machine Learning and Knowledge
Extraction</em>, 117–37. Springer.
</div>
<div id="ref-rosenblatt1956" class="csl-entry">
Rosenblatt, Murray. 1956. <span>“<span class="nocase">Remarks on Some
Nonparametric Estimates of a Density Function</span>.”</span>
<em><span>The Annals of Mathematical Statistics</span></em> 27: 832–37.
</div>
<div id="ref-Shapley53" class="csl-entry">
Shapley, Lloyd S. 1953. <span>“<span class="nocase">A Value for N-Person
Games</span>.”</span> <em><span>Contributions to the Theory of
Games</span></em> 2: 307–17.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Martin Jullum, Lars Henry Berge Olsen, Annabelle Redelmeier, Jon Lachmann, Nikolai Sellereite, Norsk Regnesentral.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
