% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/explain.R
\name{explain}
\alias{explain}
\title{Explain the output of machine learning models with more accurately estimated Shapley values}
\usage{
explain(
  model,
  x_explain,
  x_train,
  approach,
  prediction_zero,
  n_combinations = NULL,
  group = NULL,
  n_samples = 1000,
  n_batches = 1,
  seed = 1,
  keep_samp_for_vS = FALSE,
  predict_model = NULL,
  get_model_specs = NULL,
  ...
)
}
\arguments{
\item{model}{The model whose predictions we want to explain.
Run \code{\link[=get_supported_models]{get_supported_models()}}
for a table of which models \code{explain} supports natively. Unsupported models
can still be explained by passing \code{predict_model} and (optionally) \code{get_model_specs},
see details for more information.}

\item{x_explain}{A matrix or data.frame/data.table.
Contains the the features, whose predictions ought to be explained.}

\item{x_train}{Matrix or data.frame/data.table.
Contains the data used to estimate the (conditional) distributions for the features
needed to properly estimate the conditional expectations in the Shapley formula.}

\item{approach}{Character vector of length \code{1} or \code{n_features}.
\code{n_features} equals the total number of features in the model. All elements should,
either be \code{"gaussian"}, \code{"copula"}, \code{"empirical"}, \code{"ctree"}, or \code{"independence"}.
See details for more information.}

\item{prediction_zero}{Numeric.
The prediction value for unseen data, i.e. an estimate of the expected prediction without conditioning on any
features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable.}

\item{n_combinations}{Integer.
The number of feature combinations to sample. If \code{NULL},
the exact method is used and all combinations are considered. The maximum number of
combinations equals \code{2^ncol(x)}.}

\item{group}{List.
If \code{NULL} regular feature wise Shapley values are computed.
If provided, group wise Shapley values are computed. \code{group} then has length equal to
the number of groups. The list element contains character vectors with the features included
in each of the different groups.}

\item{n_samples}{Positive integer.
Indicating the maximum number of samples to use in the
Monte Carlo integration for every conditional expectation. See also details.}

\item{n_batches}{Positive integer.
Specifies how many batches the total number of feature combinations should be split into when calculating the
contribution function for each test observation.
The default value is 1.
Increasing the number of batches may significantly reduce the RAM allocation for models with many features.
This typically comes with a small increase in computation time.}

\item{seed}{Positive integer.
Specifies the seed before any randomness based code is being run.
If \code{NULL} the seed will be inherited from the calling environment.}

\item{keep_samp_for_vS}{Logical.
Indicates whether the samples used in the Monte Carlo estimation of v_S should be returned
(in \code{internal$output})}

\item{predict_model}{Function.
The prediction function used when \code{model} is not natively supported.
(Run \code{\link[=get_supported_models]{get_supported_models()}} for a list of natively supported
models.)
The function must have two arguments, \code{model} and \code{newdata} which specify, respectively, the model
and a data.frame/data.table to compute predictions for. The function must give the prediction as a numeric vector.
\code{NULL} (the default) uses functions specified internally.
Can also be used to override the default function for natively supported model classes.}

\item{get_model_specs}{Function.
An optional function for checking model/data consistency when \code{model} is not natively supported.
(Run \code{\link[=get_supported_models]{get_supported_models()}} for a list of natively supported
models.)
The function takes \code{model} as argument and provides a list with 3 elements:
\describe{
\item{labels}{Character vector with the names of each feature.}
\item{classes}{Character vector with the classes of each features.}
\item{factor_levels}{Character vector with the levels for any categorical features.}
}
If \code{NULL} (the default) internal functions are used for natively supported model classes, and the checking is
disabled for unsupported model classes.
Can also be used to override the default function for natively supported model classes.}

\item{...}{Additional arguments passed to \code{\link[=setup_approach]{setup_approach()}} for specific approaches.}
}
\value{
Object of class \code{c("shapr", "list")}. Contains the following items:
\describe{
\item{shapley_values}{data.table with the estimated Shapley values}
\item{internal}{List with the different parameters, data and functions used internally}
\item{pred_explain}{Numeric vector with the predictions for the explained observations.}
}

\code{shapley_values} is a data.table where the number of rows equals
the number of observations you'd like to explain, and the number of columns equals \code{m +1},
where \code{m} equals the total number of features in your model.

If \code{shapley_values[i, j + 1] > 0} it indicates that the j-th feature increased the prediction for
the i-th observation. Likewise, if \code{shapley_values[i, j + 1] < 0} it indicates that the j-th feature
decreased the prediction for the i-th observation.
The magnitude of the value is also important to notice. E.g. if \code{shapley_values[i, k + 1]} and
\code{shapley_values[i, j + 1]} are greater than \code{0}, where \code{j != k}, and
\code{shapley_values[i, k + 1]} > \code{shapley_values[i, j + 1]} this indicates that feature
\code{j} and \code{k} both increased the value of the prediction, but that the effect of the k-th
feature was larger than the j-th feature.

The first column in \code{dt}, called \code{none}, is the prediction value not assigned to any of the features
(\ifelse{html}{\eqn{\phi}\out{<sub>0</sub>}}{\eqn{\phi_0}}).
It's equal for all observations and set by the user through the argument \code{prediction_zero}.
The difference between the prediction and \code{none} is distributed among the other features.
In theory this value should be the expected prediction without conditioning on any features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable. \code{\link[=explain]{explain()}} \code{\link[=explain]{explain()}}
}
\description{
Computes dependence-aware Shapley values for observations in \code{x_explain} from the specified
\code{model} by using the method specified in \code{approach} to estimate the conditional expectation.
}
\details{
The most important thing to notice is that \code{shapr} has implemented five different
approaches for estimating the conditional distributions of the data, namely \code{"empirical"},
\code{"gaussian"}, \code{"copula"}, \code{"ctree"} and \code{"independence"}.
In addition, the user also has the option of combining the four approaches.
E.g., if you're in a situation where you have trained a model that consists of 10 features,
and you'd like to use the \code{"gaussian"} approach when you condition on a single feature,
the \code{"empirical"} approach if you condition on 2-5 features, and \code{"copula"} version
if you condition on more than 5 features this can be done by simply passing
\code{approach = c("gaussian", rep("empirical", 4), rep("copula", 5))}. If
\code{"approach[i]" = "gaussian"} means that you'd like to use the \code{"gaussian"} approach
when conditioning on \code{i} features.

For \code{approach="ctree"}, \code{n_samples} corresponds to the number of samples
from the leaf node (see an exception related to the \code{sample} argument).
For \code{approach="empirical"}, \code{n_samples} is  the \eqn{K} parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
\code{w_threshold} argument.
}
\examples{

# Load example data
data("airquality")
airquality <- airquality[complete.cases(airquality), ]
x_var <- c("Solar.R", "Wind", "Temp", "Month")
y_var <- "Ozone"

# Split data into test- and training data
data_train <- head(airquality, -3)
data_test <- tail(airquality, 3)

x_train <- data_train[, x_var]
x_test <- data_test[,x_var]

# Fit a linear model
lm_formula <- as.formula(paste0(y_var, " ~ ", paste0(x_var, collapse = " + ")))
model <- lm(lm_formula, data = data_train)

# Explain predictions
p <- mean(data_train[,y_var])

# Empirical approach
explain1 <- explain(
  x_train,
  x_test,
  model = model,
  approach = "empirical",
  prediction_zero = p,
  n_samples = 1e2
)

# Gaussian approach
explain2 <- explain(
  x_train,
  x_test,
  model = model,
  approach = "gaussian",
  prediction_zero = p,
  n_samples = 1e2
)

# Gaussian copula approach
explain3 <- explain(
  x_train,
  x_test,
  model = model,
  approach = "copula",
  prediction_zero = p,
  n_samples = 1e2
)

# ctree approach
explain4 <- explain(
  x_train,
  x_test,
  model = model,
  approach = "ctree",
  prediction_zero = p,
  n_samples = 1e2
)

# Combined approach
approach <- c("gaussian", "gaussian", "empirical", "empirical")
explain5 <- explain(
  x_train,
  x_test,
  model = model,
  approach = approach,
  prediction_zero = p,
  n_samples = 1e2
)

# Print the Shapley values
print(explain1$shapley_values)

# Plot the results
if (requireNamespace("ggplot2", quietly = TRUE)) {
  plot(explain1)
  plot(explain1, plot_type = "waterfall")
}

# Group-wise explanations
group_list <- list(A = c("Temp", "Month"), B = c("Wind", "Solar.R"))

explain_groups <- explain(
  x_train,
  x_test,
  model = model,
  group = group_list,
  approach = "empirical",
  prediction_zero = p,
  n_samples = 1e2
)
print(explain_groups$shapley_values)

}
\references{
Aas, K., Jullum, M., & LÃ¸land, A. (2021). Explaining individual predictions when features are dependent:
More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502.
}
\author{
Martin Jullum
}
