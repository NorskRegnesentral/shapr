% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/explanation.R
\name{explain}
\alias{explain}
\alias{explain.independence}
\alias{explain.empirical}
\alias{explain.gaussian}
\alias{explain.copula}
\alias{explain.ctree}
\alias{explain.combined}
\alias{explain.ctree_comb_mincrit}
\title{Explain the output of machine learning models with more accurately estimated Shapley values}
\usage{
explain(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples = 1000,
  n_batches = 1,
  ...
)

\method{explain}{independence}(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples = 1000,
  n_batches = 1,
  seed = 1,
  ...
)

\method{explain}{empirical}(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples = 1000,
  n_batches = 1,
  seed = 1,
  w_threshold = 0.95,
  type = "fixed_sigma",
  fixed_sigma_vec = 0.1,
  n_samples_aicc = 1000,
  eval_max_aicc = 20,
  start_aicc = 0.1,
  cov_mat = NULL,
  ...
)

\method{explain}{gaussian}(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples = 1000,
  n_batches = 1,
  seed = 1,
  mu = NULL,
  cov_mat = NULL,
  ...
)

\method{explain}{copula}(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples = 1000,
  n_batches = 1,
  seed = 1,
  ...
)

\method{explain}{ctree}(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples = 1000,
  n_batches = 1,
  seed = 1,
  mincriterion = 0.95,
  minsplit = 20,
  minbucket = 7,
  sample = TRUE,
  ...
)

\method{explain}{combined}(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples = 1000,
  n_batches = 1,
  seed = 1,
  mu = NULL,
  cov_mat = NULL,
  ...
)

\method{explain}{ctree_comb_mincrit}(
  x,
  explainer,
  approach,
  prediction_zero,
  n_samples,
  n_batches = 1,
  seed = 1,
  mincriterion,
  ...
)
}
\arguments{
\item{x}{A matrix or data.frame. Contains the the features, whose
predictions ought to be explained (test data).}

\item{explainer}{An \code{explainer} object to use for explaining the observations.
See \code{\link{shapr}}.}

\item{approach}{Character vector of length \code{1} or \code{n_features}.
\code{n_features} equals the total number of features in the model. All elements should,
either be \code{"gaussian"}, \code{"copula"}, \code{"empirical"}, \code{"ctree"}, or \code{"independence"}.
See details for more information.}

\item{prediction_zero}{Numeric. The prediction value for unseen data, typically equal to the mean of
the response.}

\item{n_samples}{Positive integer. Indicating the maximum number of samples to use in the
Monte Carlo integration for every conditional expectation. See also details.}

\item{n_batches}{Positive integer.
Specifies how many batches the total number of feature combinations should be split into when calculating the
contribution function for each test observation.
The default value is 1.
Increasing the number of batches may significantly reduce the RAM allocation for models with many features.
This typically comes with a small increase in computation time.}

\item{...}{Additional arguments passed to \code{\link{prepare_and_predict}}}

\item{seed}{Positive integer. If \code{NULL} the seed will be inherited from the calling environment.}

\item{w_threshold}{Numeric vector of length 1, with \code{0 < w_threshold <= 1} representing the minimum proportion
of the total empirical weight that data samples should use. If e.g. \code{w_threshold = .8} we will choose the
\code{K} samples with the largest weight so that the sum of the weights accounts for 80\% of the total weight.
\code{w_threshold} is the \eqn{\eta} parameter in equation (15) of Aas et al (2021).}

\item{type}{Character. Should be equal to either \code{"independence"},
\code{"fixed_sigma"}, \code{"AICc_each_k"} or \code{"AICc_full"}.}

\item{fixed_sigma_vec}{Numeric. Represents the kernel bandwidth. Note that this argument is only
applicable when \code{approach = "empirical"}, and \code{type = "fixed_sigma"}}

\item{n_samples_aicc}{Positive integer. Number of samples to consider in AICc optimization.
Note that this argument is only applicable when \code{approach = "empirical"}, and \code{type}
is either equal to \code{"AICc_each_k"} or \code{"AICc_full"}}

\item{eval_max_aicc}{Positive integer. Maximum number of iterations when
optimizing the AICc. Note that this argument is only applicable when
\code{approach = "empirical"}, and \code{type} is either equal to
\code{"AICc_each_k"} or \code{"AICc_full"}}

\item{start_aicc}{Numeric. Start value of \code{sigma} when optimizing the AICc. Note that this argument
is only applicable when \code{approach = "empirical"}, and \code{type} is either equal to
\code{"AICc_each_k"} or \code{"AICc_full"}}

\item{cov_mat}{Numeric matrix. (Optional) Containing the covariance matrix of the data
generating distribution. \code{NULL} means it is estimated from the data if needed
(in the Gaussian approach).}

\item{mu}{Numeric vector. (Optional) Containing the mean of the data generating distribution.
If \code{NULL} the expected values are estimated from the data. Note that this is only used
when \code{approach = "gaussian"}.}

\item{mincriterion}{Numeric value or vector where length of vector is the number of features in model.
Value is equal to 1 - alpha where alpha is the nominal level of the conditional
independence tests.
If it is a vector, this indicates which mincriterion to use
when conditioning on various numbers of features.}

\item{minsplit}{Numeric value. Equal to the value that the sum of the left and right daughter nodes need to exceed.}

\item{minbucket}{Numeric value. Equal to the minimum sum of weights in a terminal node.}

\item{sample}{Boolean. If TRUE, then the method always samples \code{n_samples} from the leaf (with replacement).
If FALSE and the number of obs in the leaf is less than \code{n_samples}, the method will take all observations
in the leaf. If FALSE and the number of obs in the leaf is more than \code{n_samples}, the method will sample
\code{n_samples} (with replacement). This means that there will always be sampling in the leaf unless
\code{sample} = FALSE AND the number of obs in the node is less than \code{n_samples}.}
}
\value{
Object of class \code{c("shapr", "list")}. Contains the following items:
\describe{
  \item{dt}{data.table}
  \item{model}{Model object}
  \item{p}{Numeric vector}
  \item{x_test}{data.table}
}

Note that the returned items \code{model}, \code{p} and \code{x_test} are mostly added due
to the implementation of \code{plot.shapr}. If you only want to look at the numerical results
it is sufficient to focus on \code{dt}. \code{dt} is a data.table where the number of rows equals
the number of observations you'd like to explain, and the number of columns equals \code{m +1},
where \code{m} equals the total number of features in your model.

If \code{dt[i, j + 1] > 0} it indicates that the j-th feature increased the prediction for
the i-th observation. Likewise, if \code{dt[i, j + 1] < 0} it indicates that the j-th feature
decreased the prediction for the i-th observation. The magnitude of the value is also important
to notice. E.g. if \code{dt[i, k + 1]} and \code{dt[i, j + 1]} are greater than \code{0},
where \code{j != k}, and \code{dt[i, k + 1]} > \code{dt[i, j + 1]} this indicates that feature
\code{j} and \code{k} both increased the value of the prediction, but that the effect of the k-th
feature was larger than the j-th feature.

The first column in \code{dt}, called `none`, is the prediction value not assigned to any of the features
(\ifelse{html}{\eqn{\phi}\out{<sub>0</sub>}}{\eqn{\phi_0}}).
It's equal for all observations and set by the user through the argument \code{prediction_zero}.
In theory this value should be the expected prediction without conditioning on any features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable.
}
\description{
Explain the output of machine learning models with more accurately estimated Shapley values
}
\details{
The most important thing to notice is that \code{shapr} has implemented five different
approaches for estimating the conditional distributions of the data, namely \code{"empirical"},
\code{"gaussian"}, \code{"copula"}, \code{"ctree"} and \code{"independence"}.
In addition, the user also has the option of combining the four approaches.
E.g., if you're in a situation where you have trained a model that consists of 10 features,
and you'd like to use the \code{"gaussian"} approach when you condition on a single feature,
the \code{"empirical"} approach if you condition on 2-5 features, and \code{"copula"} version
if you condition on more than 5 features this can be done by simply passing
\code{approach = c("gaussian", rep("empirical", 4), rep("copula", 5))}. If
\code{"approach[i]" = "gaussian"} means that you'd like to use the \code{"gaussian"} approach
when conditioning on \code{i} features.

For \code{approach="ctree"}, \code{n_samples} corresponds to the number of samples
from the leaf node (see an exception related to the \code{sample} argument).
For \code{approach="empirical"}, \code{n_samples} is  the \eqn{K} parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
\code{w_threshold} argument.
}
\examples{
if (requireNamespace("MASS", quietly = TRUE)) {
  # Load example data
  data("Boston", package = "MASS")

  # Split data into test- and training data
  x_train <- head(Boston, -3)
  x_test <- tail(Boston, 3)

  # Fit a linear model
  model <- lm(medv ~ lstat + rm + dis + indus, data = x_train)

  # Create an explainer object
  explainer <- shapr(x_train, model)

  # Explain predictions
  p <- mean(x_train$medv)

  # Empirical approach
  explain1 <- explain(x_test, explainer,
    approach = "empirical",
    prediction_zero = p, n_samples = 1e2
  )

  # Gaussian approach
  explain2 <- explain(x_test, explainer,
    approach = "gaussian",
    prediction_zero = p, n_samples = 1e2
  )

  # Gaussian copula approach
  explain3 <- explain(x_test, explainer,
    approach = "copula",
    prediction_zero = p, n_samples = 1e2
  )

  # ctree approach
  explain4 <- explain(x_test, explainer,
    approach = "ctree",
    prediction_zero = p
  )

  # Combined approach
  approach <- c("gaussian", "gaussian", "empirical", "empirical")
  explain5 <- explain(x_test, explainer,
    approach = approach,
    prediction_zero = p, n_samples = 1e2
  )

  # Print the Shapley values
  print(explain1$dt)

  # Plot the results
  if (requireNamespace("ggplot2", quietly = TRUE)) {
    plot(explain1)
  }

  # Group-wise explanations
  group <- list(A = c("lstat", "rm"), B = c("dis", "indus"))
  explainer_group <- shapr(x_train, model, group = group)
  explain_groups <- explain(
    x_test,
    explainer_group,
    approach = "empirical",
    prediction_zero = p,
    n_samples = 1e2
  )
  print(explain_groups$dt)
}
}
\references{
Aas, K., Jullum, M., & Løland, A. (2021). Explaining individual predictions when features are dependent:
  More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502.
}
\author{
Camilla Lingjaerde, Nikolai Sellereite, Martin Jullum, Annabelle Redelmeier
}
