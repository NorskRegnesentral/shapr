% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/asymmetric_and_casual_Shapley.R
\name{create_marginal_data_training}
\alias{create_marginal_data_training}
\title{Function that samples data from the empirical marginal training distribution}
\usage{
create_marginal_data_training(
  x_train,
  n_explain,
  Sbar_features,
  n_MC_samples = 1000,
  stable_version = TRUE
)
}
\arguments{
\item{x_train}{Matrix or data.frame/data.table.
Contains the data used to estimate the (conditional) distributions for the features
needed to properly estimate the conditional expectations in the Shapley formula.}

\item{n_explain}{Integer. The number of explicands/observations to explain.}

\item{Sbar_features}{Vector of integers containing the features indices to generate marginal observations for.
That is, if \code{Sbar_features} is \code{c(1,4)}, then we sample \code{n_MC_samples} observations from \eqn{P(X_1, X_4)} using the
empirical training observations (with replacements). That is, we sample the first and fourth feature values from
the same training observation, so we do not break the dependence between them.}

\item{n_MC_samples}{Positive integer.
Indicating the maximum number of samples to use in the  Monte Carlo integration for every conditional expectation.
For \code{approach="ctree"}, \code{n_MC_samples} corresponds to the number of samples
from the leaf node (see an exception related to the \code{ctree.sample} argument \code{\link[=setup_approach.ctree]{setup_approach.ctree()}}).
For \code{approach="empirical"}, \code{n_MC_samples} is  the \eqn{K} parameter in equations (14-15) of
Aas et al. (2021), i.e. the maximum number of observations (with largest weights) that is used, see also the
\code{empirical.eta} argument \code{\link[=setup_approach.empirical]{setup_approach.empirical()}}.}

\item{stable_version}{Logical. If \code{TRUE} and \code{n_MC_samples} > \code{n_train}, then we include each training observation
\code{n_MC_samples \%/\% n_train} times and then sample the remaining \verb{n_MC_samples \%\% n_train samples}. Only the latter is
done when \code{n_MC_samples < n_train}. This is done separately for each explicand. If \code{FALSE}, we randomly sample the
from the observations.}
}
\value{
Data table of dimension \eqn{`n_MC_samples` \times `length(Sbar_features)`} with the sampled observations.
}
\description{
Sample observations from the empirical distribution P(X) using the training dataset.
}
\examples{
\dontrun{
data("airquality")
data <- data.table::as.data.table(airquality)
data <- data[complete.cases(data), ]

x_var <- c("Solar.R", "Wind", "Temp", "Month")
y_var <- "Ozone"

ind_x_explain <- 1:6
x_train <- data[-ind_x_explain, ..x_var]
x_train
create_marginal_data__training(x_train = x_train, Sbar_features = c(1, 4), n_MC_samples = 10)
}

}
\author{
Lars Henry Berge Olsen
}
\keyword{internal}
