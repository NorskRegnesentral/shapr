% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/explain_forecast.R
\name{explain_forecast}
\alias{explain_forecast}
\title{Explain a forecast from a time series model using Shapley values.}
\usage{
explain_forecast(
  model,
  y,
  xreg = NULL,
  train_idx = NULL,
  explain_idx,
  explain_y_lags,
  explain_xreg_lags = explain_y_lags,
  horizon,
  approach,
  prediction_zero,
  n_combinations = NULL,
  group_lags = TRUE,
  group = NULL,
  n_samples = 1000,
  n_batches = NULL,
  seed = 1,
  keep_samp_for_vS = FALSE,
  predict_model = NULL,
  get_model_specs = NULL,
  timing = TRUE,
  ...
)
}
\arguments{
\item{model}{The model whose predictions we want to explain.
Run \code{\link[=get_supported_models]{get_supported_models()}}
for a table of which models \code{explain} supports natively. Unsupported models
can still be explained by passing \code{predict_model} and (optionally) \code{get_model_specs},
see details for more information.}

\item{y}{Matrix, data.frame/data.table or a numeric vector.
Contains the endogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.}

\item{xreg}{Matrix, data.frame/data.table or a numeric vector.
Contains the exogenous variables used to estimate the (conditional) distributions
needed to properly estimate the conditional expectations in the Shapley formula
including the observations to be explained.
As exogenous variables are used contemporaneusly when producing a forecast,
this item should contain nrow(y) + horizon rows.}

\item{train_idx}{Numeric vector
The row indices in data and reg denoting points in time to use when estimating the conditional expectations in
the Shapley value formula.
If \code{train_idx = NULL} (default) all indices not selected to be explained will be used.}

\item{explain_idx}{Numeric vector
The row indices in data and reg denoting points in time to explain.}

\item{explain_y_lags}{Numeric vector.
Denotes the number of lags that should be used for each variable in \code{y} when making a forecast.}

\item{explain_xreg_lags}{Numeric vector.
If \code{xreg != NULL}, denotes the number of lags that should be used for each variable in \code{xreg} when making a forecast.}

\item{horizon}{Numeric.
The forecast horizon to explain. Passed to the \code{predict_model} function.}

\item{approach}{Character vector of length \code{1} or \code{n_features}.
\code{n_features} equals the total number of features in the model. All elements should,
either be \code{"gaussian"}, \code{"copula"}, \code{"empirical"}, \code{"ctree"}, \code{"categorical"}, \code{"timeseries"}, or \code{"independence"}.
See details for more information.}

\item{prediction_zero}{Numeric.
The prediction value for unseen data, i.e. an estimate of the expected prediction without conditioning on any
features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable.}

\item{n_combinations}{Integer.
If \code{group = NULL}, \code{n_combinations} represents the number of unique feature combinations to sample.
If \code{group != NULL}, \code{n_combinations} represents the number of unique group combinations to sample.
If \code{n_combinations = NULL}, the exact method is used and all combinations are considered.
The maximum number of combinations equals \code{2^m}, where \code{m} is the number of features.}

\item{group_lags}{Logical.
If \code{TRUE} all lags of each variable are grouped together and explained as a group.
If \code{FALSE} all lags of each variable are explained individually.}

\item{group}{List.
If \code{NULL} regular feature wise Shapley values are computed.
If provided, group wise Shapley values are computed. \code{group} then has length equal to
the number of groups. The list element contains character vectors with the features included
in each of the different groups.}

\item{n_samples}{Positive integer.
Indicating the maximum number of samples to use in the
Monte Carlo integration for every conditional expectation. See also details.}

\item{n_batches}{Positive integer (or NULL).
Specifies how many batches the total number of feature combinations should be split into when calculating the
contribution function for each test observation.
The default value is NULL which uses a reasonable trade-off between RAM allocation and computation speed,
which depends on \code{approach} and \code{n_combinations}.
For models with many features, increasing the number of batches reduces the RAM allocation significantly.
This typically comes with a small increase in computation time.}

\item{seed}{Positive integer.
Specifies the seed before any randomness based code is being run.
If \code{NULL} the seed will be inherited from the calling environment.}

\item{keep_samp_for_vS}{Logical.
Indicates whether the samples used in the Monte Carlo estimation of v_S should be returned
(in \code{internal$output})}

\item{predict_model}{Function.
The prediction function used when \code{model} is not natively supported.
(Run \code{\link[=get_supported_models]{get_supported_models()}} for a list of natively supported
models.)
The function must have two arguments, \code{model} and \code{newdata} which specify, respectively, the model
and a data.frame/data.table to compute predictions for. The function must give the prediction as a numeric vector.
\code{NULL} (the default) uses functions specified internally.
Can also be used to override the default function for natively supported model classes.}

\item{get_model_specs}{Function.
An optional function for checking model/data consistency when \code{model} is not natively supported.
(Run \code{\link[=get_supported_models]{get_supported_models()}} for a list of natively supported
models.)
The function takes \code{model} as argument and provides a list with 3 elements:
\describe{
\item{labels}{Character vector with the names of each feature.}
\item{classes}{Character vector with the classes of each features.}
\item{factor_levels}{Character vector with the levels for any categorical features.}
}
If \code{NULL} (the default) internal functions are used for natively supported model classes, and the checking is
disabled for unsupported model classes.
Can also be used to override the default function for natively supported model classes.}

\item{timing}{Logical.
Whether the timing of the different parts of the \code{explain()} should saved in the model object.}

\item{...}{
  Arguments passed on to \code{\link[=setup_approach.empirical]{setup_approach.empirical}}, \code{\link[=setup_approach.independence]{setup_approach.independence}}, \code{\link[=setup_approach.gaussian]{setup_approach.gaussian}}, \code{\link[=setup_approach.copula]{setup_approach.copula}}, \code{\link[=setup_approach.ctree]{setup_approach.ctree}}, \code{\link[=setup_approach.categorical]{setup_approach.categorical}}, \code{\link[=setup_approach.timeseries]{setup_approach.timeseries}}
  \describe{
    \item{\code{empirical.type}}{Character. (default = \code{"fixed_sigma"})
Should be equal to either \code{"independence"},\code{"fixed_sigma"}, \code{"AICc_each_k"} \code{"AICc_full"}.
TODO: Describe better what the methods do here.}
    \item{\code{empirical.eta}}{Numeric. (default = 0.95)
Needs to be \verb{0 < eta <= 1}.
Represents the minimum proportion of the total empirical weight that data samples should use.
If e.g. \code{eta = .8} we will choose the \code{K} samples with the largest weight so that the sum of the weights
accounts for 80\\% of the total weight.
\code{eta} is the \eqn{\eta} parameter in equation (15) of Aas et al (2021).}
    \item{\code{empirical.fixed_sigma}}{Positive numeric scalar. (default = 0.1)
Represents the kernel bandwidth in the distance computation used when conditioning on all different combinations.
Only used when \code{empirical.type = "fixed_sigma"}}
    \item{\code{empirical.n_samples_aicc}}{Positive integer. (default = 1000)
Number of samples to consider in AICc optimization.
Only used for \code{empirical.type} is either \code{"AICc_each_k"} or \code{"AICc_full"}.}
    \item{\code{empirical.eval_max_aicc}}{Positive integer. (default = 20)
Maximum number of iterations when optimizing the AICc.
Only used for \code{empirical.type} is either \code{"AICc_each_k"} or \code{"AICc_full"}.}
    \item{\code{empirical.start_aicc}}{Numeric. (default = 0.1)
Start value of the \code{sigma} parameter when optimizing the AICc.
Only used for \code{empirical.type} is either \code{"AICc_each_k"} or \code{"AICc_full"}.}
    \item{\code{empirical.cov_mat}}{Numeric matrix. (Optional, default = NULL)
Containing the covariance matrix of the data generating distribution used to define the Mahalanobis distance.
\code{NULL} means it is estimated from \code{x_train}.}
    \item{\code{internal}}{Not used.}
    \item{\code{gaussian.mu}}{Numeric vector. (Optional)
Containing the mean of the data generating distribution.
\code{NULL} means it is estimated from the \code{x_train}.}
    \item{\code{gaussian.cov_mat}}{Numeric matrix. (Optional)
Containing the covariance matrix of the data generating distribution.
\code{NULL} means it is estimated from the \code{x_train}.}
    \item{\code{ctree.mincriterion}}{Numeric scalar or vector. (default = 0.95)
Either a scalar or vector of length equal to the number of features in the model.
Value is equal to 1 - \eqn{\alpha} where \eqn{\alpha} is the nominal level of the conditional independence tests.
If it is a vector, this indicates which value to use when conditioning on various numbers of features.}
    \item{\code{ctree.minsplit}}{Numeric scalar. (default = 20)
Determines minimum value that the sum of the left and right daughter nodes required for a split.}
    \item{\code{ctree.minbucket}}{Numeric scalar. (default = 7)
Determines the minimum sum of weights in a terminal node required for a split}
    \item{\code{ctree.sample}}{Boolean. (default = TRUE)
If TRUE, then the method always samples \code{n_samples} observations from the leaf nodes (with replacement).
If FALSE and the number of observations in the leaf node is less than \code{n_samples},
the method will take all observations in the leaf.
If FALSE and the number of observations in the leaf node is more than \code{n_samples},
the method will sample \code{n_samples} observations (with replacement).
This means that there will always be sampling in the leaf unless
\code{sample} = FALSE AND the number of obs in the node is less than \code{n_samples}.}
    \item{\code{categorical.joint_prob_dt}}{Data.table. (Optional)
Containing the joint probability distribution for each combination of feature
values.
\code{NULL} means it is estimated from the \code{x_train} and \code{x_explain}.}
    \item{\code{categorical.epsilon}}{Numeric value. (Optional)
If \code{joint_probability_dt} is not supplied, probabilities/frequencies are
estimated using \code{x_train}. If certain observations occur in \code{x_train} and NOT in \code{x_explain},
then epsilon is used as the proportion of times that these observations occurs in the training data.
In theory, this proportion should be zero, but this causes an error later in the Shapley computation.}
    \item{\code{timeseries.fixed_sigma_vec}}{Numeric. (Default = 2)
Represents the kernel bandwidth in the distance computation. TODO: What length should it have? 1?}
    \item{\code{timeseries.bounds}}{Numeric vector of length two. (Default = c(NULL, NULL))
If one or both of these bounds are not NULL, we restrict the sampled time series to be
between these bounds.
This is useful if the underlying time series are scaled between 0 and 1, for example.}
  }}
}
\value{
Object of class \code{c("shapr", "list")}. Contains the following items:
\describe{
\item{shapley_values}{data.table with the estimated Shapley values}
\item{internal}{List with the different parameters, data and functions used internally}
\item{pred_explain}{Numeric vector with the predictions for the explained observations.}
}

\code{shapley_values} is a data.table where the number of rows equals
the number of observations you'd like to explain, and the number of columns equals \code{m +1},
where \code{m} equals the total number of features in your model.

If \code{shapley_values[i, j + 1] > 0} it indicates that the j-th feature increased the prediction for
the i-th observation. Likewise, if \code{shapley_values[i, j + 1] < 0} it indicates that the j-th feature
decreased the prediction for the i-th observation.
The magnitude of the value is also important to notice. E.g. if \code{shapley_values[i, k + 1]} and
\code{shapley_values[i, j + 1]} are greater than \code{0}, where \code{j != k}, and
\code{shapley_values[i, k + 1]} > \code{shapley_values[i, j + 1]} this indicates that feature
\code{j} and \code{k} both increased the value of the prediction, but that the effect of the k-th
feature was larger than the j-th feature.

The first column in \code{dt}, called \code{none}, is the prediction value not assigned to any of the features
(\ifelse{html}{\eqn{\phi}\out{<sub>0</sub>}}{\eqn{\phi_0}}).
It's equal for all observations and set by the user through the argument \code{prediction_zero}.
The difference between the prediction and \code{none} is distributed among the other features.
In theory this value should be the expected prediction without conditioning on any features.
Typically we set this value equal to the mean of the response variable in our training data, but other choices
such as the mean of the predictions in the training data are also reasonable. \code{\link[=explain]{explain()}} \code{\link[=explain]{explain()}}
}
\description{
Computes dependence-aware Shapley values for observations in \code{explain_idx} from the specified
\code{model} by using the method specified in \code{approach} to estimate the conditional expectation.
}
\details{
This function explains a forecast of length \code{horizon}. The argument \code{train_idx}
is analogous to x_train in \code{explain()}, however, it just contains the time indices of where
in the data the forecast should start for each training sample. In the same way \code{explain_idx}
defines the time index (indices) which will precede a forecast to be explained.

As any autoregressive forecast model will require a set of lags to make a forecast at an
arbitrary point in time, \code{explain_y_lags} and \code{explain_xreg_lags} define how many lags
are required to "refit" the model at any given time index. This allows the different
approaches to work in the same way they do for time-invariant models.
}
\examples{

# Load example data
data("airquality")
data <- data.table::as.data.table(airquality)

# Fit an AR(2) model.
model_ar_temp <- ar(data$Temp, order = 2)

# Calculate the zero prediction values for a three step forecast.
p0_ar <- rep(mean(data$Temp), 3)

# Empirical approach, explaining forecasts starting at T = 152 and T = 153.
explain_forecast(
  model = model_ar_temp,
  y = data[, "Temp"],
  train_idx = 2:151,
  explain_idx = 152:153,
  explain_y_lags = 2,
  horizon = 3,
  approach = "empirical",
  prediction_zero = p0_ar,
  group_lags = FALSE
)

}
\references{
Aas, K., Jullum, M., & L<U+00F8>land, A. (2021). Explaining individual predictions when features are dependent:
More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502.
}
\author{
Martin Jullum
}
