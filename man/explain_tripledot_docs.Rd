% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/documentation.R
\name{explain_tripledot_docs}
\alias{explain_tripledot_docs}
\title{Documentation of the approach-specific parameters in \code{\link[=explain]{explain()}}}
\usage{
explain_tripledot_docs(...)
}
\arguments{
\item{...}{
  Arguments passed on to \code{\link[=setup_approach.independence]{setup_approach.independence}}, \code{\link[=setup_approach.empirical]{setup_approach.empirical}}, \code{\link[=setup_approach.categorical]{setup_approach.categorical}}, \code{\link[=setup_approach.copula]{setup_approach.copula}}, \code{\link[=setup_approach.ctree]{setup_approach.ctree}}, \code{\link[=setup_approach.gaussian]{setup_approach.gaussian}}, \code{\link[=setup_approach.regression_separate]{setup_approach.regression_separate}}, \code{\link[=setup_approach.regression_surrogate]{setup_approach.regression_surrogate}}, \code{\link[=setup_approach.timeseries]{setup_approach.timeseries}}, \code{\link[=setup_approach.vaeac]{setup_approach.vaeac}}
  \describe{
    \item{\code{empirical.type}}{Character. (default = \code{"fixed_sigma"})
Should be equal to either \code{"independence"},\code{"fixed_sigma"}, \code{"AICc_each_k"} \code{"AICc_full"}.
TODO: Describe better what the methods do here.}
    \item{\code{empirical.eta}}{Numeric. (default = 0.95)
Needs to be \verb{0 < eta <= 1}.
Represents the minimum proportion of the total empirical weight that data samples should use.
If e.g. \code{eta = .8} we will choose the \code{K} samples with the largest weight so that the sum of the weights
accounts for 80\\% of the total weight.
\code{eta} is the \eqn{\eta} parameter in equation (15) of Aas et al (2021).}
    \item{\code{empirical.fixed_sigma}}{Positive numeric scalar. (default = 0.1)
Represents the kernel bandwidth in the distance computation used when conditioning on all different coalitions.
Only used when \code{empirical.type = "fixed_sigma"}}
    \item{\code{empirical.n_samples_aicc}}{Positive integer. (default = 1000)
Number of samples to consider in AICc optimization.
Only used for \code{empirical.type} is either \code{"AICc_each_k"} or \code{"AICc_full"}.}
    \item{\code{empirical.eval_max_aicc}}{Positive integer. (default = 20)
Maximum number of iterations when optimizing the AICc.
Only used for \code{empirical.type} is either \code{"AICc_each_k"} or \code{"AICc_full"}.}
    \item{\code{empirical.start_aicc}}{Numeric. (default = 0.1)
Start value of the \code{sigma} parameter when optimizing the AICc.
Only used for \code{empirical.type} is either \code{"AICc_each_k"} or \code{"AICc_full"}.}
    \item{\code{empirical.cov_mat}}{Numeric matrix. (Optional, default = NULL)
Containing the covariance matrix of the data generating distribution used to define the Mahalanobis distance.
\code{NULL} means it is estimated from \code{x_train}.}
    \item{\code{categorical.joint_prob_dt}}{Data.table. (Optional)
Containing the joint probability distribution for each combination of feature
values.
\code{NULL} means it is estimated from the \code{x_train} and \code{x_explain}.}
    \item{\code{categorical.epsilon}}{Numeric value. (Optional)
If \code{joint_probability_dt} is not supplied, probabilities/frequencies are
estimated using \code{x_train}. If certain observations occur in \code{x_explain} and NOT in \code{x_train},
then epsilon is used as the proportion of times that these observations occurs in the training data.
In theory, this proportion should be zero, but this causes an error later in the Shapley computation.}
    \item{\code{ctree.mincriterion}}{Numeric scalar or vector. (default = 0.95)
Either a scalar or vector of length equal to the number of features in the model.
Value is equal to 1 - \eqn{\alpha} where \eqn{\alpha} is the nominal level of the conditional independence tests.
If it is a vector, this indicates which value to use when conditioning on various numbers of features.}
    \item{\code{ctree.minsplit}}{Numeric scalar. (default = 20)
Determines minimum value that the sum of the left and right daughter nodes required for a split.}
    \item{\code{ctree.minbucket}}{Numeric scalar. (default = 7)
Determines the minimum sum of weights in a terminal node required for a split}
    \item{\code{ctree.sample}}{Boolean. (default = TRUE)
If TRUE, then the method always samples \code{n_MC_samples} observations from the leaf nodes (with replacement).
If FALSE and the number of observations in the leaf node is less than \code{n_MC_samples},
the method will take all observations in the leaf.
If FALSE and the number of observations in the leaf node is more than \code{n_MC_samples},
the method will sample \code{n_MC_samples} observations (with replacement).
This means that there will always be sampling in the leaf unless
\code{sample} = FALSE AND the number of obs in the node is less than \code{n_MC_samples}.}
    \item{\code{gaussian.mu}}{Numeric vector. (Optional)
Containing the mean of the data generating distribution.
\code{NULL} means it is estimated from the \code{x_train}.}
    \item{\code{gaussian.cov_mat}}{Numeric matrix. (Optional)
Containing the covariance matrix of the data generating distribution.
\code{NULL} means it is estimated from the \code{x_train}.}
    \item{\code{regression.model}}{A \code{tidymodels} object of class \code{model_specs}. Default is a linear regression model, i.e.,
\code{\link[parsnip:linear_reg]{parsnip::linear_reg()}}. See \href{https://www.tidymodels.org/find/parsnip/}{tidymodels} for all possible models,
and see the vignette for how to add new/own models. Note, to make it easier to call \code{explain()} from Python, the
\code{regression.model} parameter can also be a string specifying the model which will be parsed and evaluated. For
example, \verb{"parsnip::rand_forest(mtry = hardhat::tune(), trees = 100, engine = "ranger", mode = "regression")"}
is also a valid input. It is essential to include the package prefix if the package is not loaded.}
    \item{\code{regression.tune_values}}{Either \code{NULL} (default), a data.frame/data.table/tibble, or a function.
The data.frame must contain the possible hyperparameter value combinations to try.
The column names must match the names of the tuneable parameters specified in \code{regression.model}.
If \code{regression.tune_values} is a function, then it should take one argument \code{x} which is the training data
for the current coalition and returns a data.frame/data.table/tibble with the properties described above.
Using a function allows the hyperparameter values to change based on the size of the coalition See the regression
vignette for several examples.
Note, to make it easier to call \code{explain()} from Python, the \code{regression.tune_values} can also be a string
containing an R function. For example,
\code{"function(x) return(dials::grid_regular(dials::mtry(c(1, ncol(x)))), levels = 3))"} is also a valid input.
It is essential to include the package prefix if the package is not loaded.}
    \item{\code{regression.vfold_cv_para}}{Either \code{NULL} (default) or a named list containing
the parameters to be sent to \code{\link[rsample:vfold_cv]{rsample::vfold_cv()}}. See the regression vignette for
several examples.}
    \item{\code{regression.recipe_func}}{Either \code{NULL} (default) or a function that that takes in a \code{\link[recipes:recipe]{recipes::recipe()}}
object and returns a modified \code{\link[recipes:recipe]{recipes::recipe()}} with potentially additional recipe steps. See the regression
vignette for several examples.
Note, to make it easier to call \code{explain()} from Python, the \code{regression.recipe_func} can also be a string
containing an R function. For example,
\code{"function(recipe) return(recipes::step_ns(recipe, recipes::all_numeric_predictors(), deg_free = 2))"} is also
a valid input. It is essential to include the package prefix if the package is not loaded.}
    \item{\code{regression.surrogate_n_comb}}{Integer.
(default is \code{internal$iter_list[[length(internal$iter_list)]]$n_coalitions}) specifying the
number of unique coalitions to apply to each training observation. Maximum allowed value is
"\code{internal$iter_list[[length(internal$iter_list)]]$n_coalitions} - 2".
By default, we use all coalitions, but this can take a lot of memory in larger dimensions.
Note that by "all", we mean all coalitions chosen by \code{shapr} to be used.
This will be all \eqn{2^{n_{\text{features}}}} coalitions (minus empty and grand coalition) if \code{shapr} is in
the exact mode.
If the user sets a lower value than \code{internal$iter_list[[length(internal$iter_list)]]$n_coalitions},
then we sample this amount of unique coalitions separately for each training observations.
That is, on average, all coalitions should be equally trained.}
    \item{\code{timeseries.fixed_sigma_vec}}{Numeric. (Default = 2)
Represents the kernel bandwidth in the distance computation. TODO: What length should it have? 1?}
    \item{\code{timeseries.bounds}}{Numeric vector of length two. (Default = c(NULL, NULL))
If one or both of these bounds are not NULL, we restrict the sampled time series to be
between these bounds.
This is useful if the underlying time series are scaled between 0 and 1, for example.}
    \item{\code{vaeac.depth}}{Positive integer (default is \code{3}). The number of hidden layers
in the neural networks of the masked encoder, full encoder, and decoder.}
    \item{\code{vaeac.width}}{Positive integer (default is \code{32}). The number of neurons in each
hidden layer in the neural networks of the masked encoder, full encoder, and decoder.}
    \item{\code{vaeac.latent_dim}}{Positive integer (default is \code{8}). The number of dimensions in the latent space.}
    \item{\code{vaeac.lr}}{Positive numeric (default is \code{0.001}). The learning rate used in the \code{\link[torch:optim_adam]{torch::optim_adam()}} optimizer.}
    \item{\code{vaeac.activation_function}}{An \code{\link[torch:nn_module]{torch::nn_module()}} representing an activation function such as, e.g.,
\code{\link[torch:nn_relu]{torch::nn_relu()}} (default), \code{\link[torch:nn_leaky_relu]{torch::nn_leaky_relu()}}, \code{\link[torch:nn_selu]{torch::nn_selu()}}, or \code{\link[torch:nn_sigmoid]{torch::nn_sigmoid()}}.}
    \item{\code{vaeac.n_vaeacs_initialize}}{Positive integer (default is \code{4}). The number of different vaeac models to initiate
in the start. Pick the best performing one after \code{vaeac.extra_parameters$epochs_initiation_phase}
epochs (default is \code{2}) and continue training that one.}
    \item{\code{vaeac.epochs}}{Positive integer (default is \code{100}). The number of epochs to train the final vaeac model.
This includes \code{vaeac.extra_parameters$epochs_initiation_phase}, where the default is \code{2}.}
    \item{\code{vaeac.extra_parameters}}{Named list with extra parameters to the \code{vaeac} approach. See
\code{\link[=vaeac_get_extra_para_default]{vaeac_get_extra_para_default()}} for description of possible additional parameters and their default values.}
  }}
}
\description{
This helper function displays the specific arguments applicable to the different
approaches. Note that when calling \code{\link[=explain]{explain()}} from Python, the parameters
are renamed from the \code{approach.parameter_name} to \code{approach_parameter_name}.
That is, an underscore has replaced the dot as the dot is reserved in Python.
}
\author{
Lars Henry Berge Olsen and Martin Jullum
}
