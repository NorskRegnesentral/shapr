% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/observations.R
\name{observation_impute}
\alias{observation_impute}
\title{Generate permutations of training data using test observations}
\usage{
observation_impute(
  W_kernel,
  S,
  x_train,
  x_test,
  w_threshold = 0.7,
  n_samples = 1000
)
}
\arguments{
\item{W_kernel}{Numeric matrix. Contains all nonscaled weights between training and test
observations for all feature combinations. The dimension equals \code{n_train x m}.}

\item{S}{Integer matrix of dimension \code{n_combinations x m}, where \code{n_combinations}
and \code{m} equals the total number of sampled/non-sampled feature combinations and
the total number of unique features, respectively. Note that \code{m = ncol(x_train)}.}

\item{x_train}{Numeric matrix}

\item{x_test}{Numeric matrix}

\item{w_threshold}{Numeric vector of length 1, with \code{0 < w_threshold <= 1} representing the minimum proportion
of the total empirical weight that data samples should use. If e.g. \code{w_threshold = .8} we will choose the
\code{K} samples with the largest weight so that the sum of the weights accounts for 80\% of the total weight.
\code{w_threshold} is the \eqn{\eta} parameter in equation (15) of Aas et al (2021).}

\item{n_samples}{Positive integer. Indicating the maximum number of samples to use in the
Monte Carlo integration for every conditional expectation. See also details.}
}
\value{
data.table
}
\description{
Generate permutations of training data using test observations
}
\references{
Aas, K., Jullum, M., & LÃ¸land, A. (2021). Explaining individual predictions when features are dependent:
  More accurate approximations to Shapley values. Artificial Intelligence, 298, 103502.

Frye, C., Rowat, C., & Feige, I. (2020). Asymmetric Shapley values:
incorporating causal knowledge into model-agnostic explainability.
Advances in Neural Information Processing Systems, 33.

Heskes, T., Sijben, E., Bucur, I. G., & Claassen, T. (2020). Causal Shapley Values:
Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models.
Advances in Neural Information Processing Systems, 33.
}
\author{
Nikolai Sellereite
}
\keyword{internal}
