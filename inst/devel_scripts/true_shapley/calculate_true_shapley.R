#' Function to simulate Normal random variables with true mu and Sigma parameters.
#'
#' @description
#'
#' @param mu Numeric or vector indicating the true mean of the Normal or joint Normal random variables used to calculate
#' the true Shapley values.
#' @param Sigma Numeric (if mu is Numeric) or matrix with values used for covariance matrix of the random variables used to
#' calculate the true Shapley values.
#' @param beta Numeric or vector. These are the true coefficients of the response modelthat we are trying to explain with the
#' Shapley values.
#' @param N_shapley Numeric indicating how many Normal or joint Normal random variables to simulate. Default 10000.
#' @param explainer explainer object from shapr package.
#' @param cutoff vector of Numerics. This indicates where to cutoff the Normal random variables to make levels.
#' @param response_mod function. The true response model that indicates how the features relate to the response.
#' @details
#'
#' @return list
#'
#' @export

sim_true_Normal <- function(mu, Sigma, beta, N_shapley = 10000, explainer, cutoff, response_mod){

  nms <- colnames(explainer$x_train)

  if(!is.null(dim(cutoff))){
    cutoff[, 1] <- cutoff[, 1] - 10
    cutoff[, 4] <- cutoff[, 4] + 10
  }

  set.seed(1)
  sim <- mvrnorm(n = N_shapley, mu = mu, Sigma = Sigma)
  dt <- NULL
  if(is.matrix(cutoff)){
    for(i in 1:length(nms)){
      dt <- cbind(dt, cut(sim[, i], cutoff[i, ], labels = c(1:3), include.lowest = TRUE))
    }
  } else{
    for(i in 1:length(nms)){
      dt <- cbind(dt, cut(sim[, i], cutoff, labels = c(1:3)))
    }
  }
  dt <- data.table(dt)

  setnames(dt, c("V1", "V2", "V3"), nms)

  prop <- c(table(dt$feat1) / N_shapley, table(dt$feat3) / N_shapley, table(dt$feat3) / N_shapley  )

  dt[, feat1 := as.factor(feat1)]
  dt[, feat2 := as.factor(feat2)]
  dt[, feat3 := as.factor(feat3)]

  dt_response <- cbind(dt, data.table(model.matrix(~., data = dt)))

  ## 3. Calculate response
  dt_response[, response := response_mod(feat12, feat13, feat22, feat23, feat32, feat33, beta = beta)]

  mn <- mean(dt_response$response)

  # dt <- data.table(round(sim))

  joint_prob <- table(dt)  / N_shapley

  joint_prob_dt0 <- data.table(joint_prob)
  # joint_prob_dt0[, 'feat1' := as.numeric(feat1)][, 'feat2' := as.numeric(feat2)][, 'feat3' := as.numeric(feat3)]
  # setnames(joint_prob_dt0, c("V1", "V2", "V3"), nms)

  joint_prob_dt <- joint_prob_dt0[, ..nms][, lapply(.SD, as.factor)]
  joint_prob_dt <- cbind(joint_prob_dt, joint_prob_dt0[, .(N)])

  # joint_prob_dt[, p := mn]

  return(list(joint_prob_dt, mn, prop))
}

#' Function to calculate marginal probabilities of the cutoff jointly Normal random variables
#'
#' @description
#'
#' @param joint_prob_dt list. Calculated using the \code{sim_true_Normal} function.
#' @param explainer explainer object from shapr package.
#'
#' @return list
#'
#' @export

marg_prob <- function(joint_prob_dt, explainer){

  nms <- colnames(explainer$x_train)

  ## compute all conditional probabilities
  marg_list <- list()
  for(i in 1:nrow(explainer$S)){
    if(i == 1){
      marg_list[[i]] <- NA
    }
    if(i > 1){ ## this is the marginal distribution we're interested in i.e f(V1)
      feat <- nms[as.logical(explainer$S[i, ])]
      mat <- data.frame(matrix(NA, ncol = length(feat) + 1, nrow = nrow(unique(joint_prob_dt[, feat, with = FALSE]))))
      names(mat) <- c(feat, "prob")
      for(j in 1:nrow(unique(joint_prob_dt[, feat, with = FALSE]))){

        v <- unique(joint_prob_dt[, feat, with = FALSE])[j]

        if(ncol(v) == 1){
          v <- as.numeric(v[[1]])
          mat[j, 1] <- v
          mat[j, 2] <- joint_prob_dt[get(feat) == v, .(prob = sum(N))]

          if(j == nrow(unique(joint_prob_dt[, feat, with = FALSE]))){
            mat[, feat] <- as.factor(mat[, feat])
          }

        } else if (ncol(v) == 2){

          v1 <- as.numeric(v[[1]])
          v2 <- as.numeric(v[[2]])

          mat[j, 1] <- v1
          mat[j, 2] <- v2
          mat[j, 3] <-joint_prob_dt[get(feat[1]) == v1 & get(feat[2]) == v2, .(prob = sum(N))]

          if(j == nrow(unique(joint_prob_dt[, feat, with = FALSE]))){
            mat[, feat[1]] <- as.factor(mat[, feat[1]])
            mat[, feat[2]] <- as.factor(mat[, feat[2]])
          }

        } else if (ncol(v) == 3){

          v1 <- as.numeric(v[[1]])
          v2 <- as.numeric(v[[2]])
          v3 <- as.numeric(v[[3]])

          mat[j, 1] <- v1
          mat[j, 2] <- v2
          mat[j, 3] <- v3
          mat[j, 4] <- joint_prob_dt[get(feat[1]) == v1 & get(feat[2]) == v2 & get(feat[3]) == v3, .(prob = sum(N))]

          if(j == nrow(unique(joint_prob_dt[, feat, with = FALSE]))){
            mat[, feat[1]] <- as.factor(mat[, feat[1]])
            mat[, feat[2]] <- as.factor(mat[, feat[2]])
            mat[, feat[3]] <- as.factor(mat[, feat[3]])
          }

        }
      }
      # mat <- cbind(mat, data.frame(p = joint_prob_dt$p[1:nrow(mat)]))
      marg_list[[i]] <- mat
    }
  }
  return(marg_list)
}

#' Function to calculate conditional probabilities of the cutoff jointly Normal random variables
#'
#' @description
#'
#' @param marg_list list. Contains the marginal probabilities calculated using the \code{marg_prob} function.
#' @param joint_prob_dt list. Calculated using the \code{sim_true_Normal} function.
#' @param explainer explainer object from shapr package.
#'
#' @return list
#'
#' @export

cond_prob <- function(marg_list, joint_prob_dt, explainer){

  nms <- colnames(explainer$x_train)

  cond_list <- list()
  for(i in 1:nrow(explainer$S)){
    if(i == 1){
      cond_list[[i]] <- NA
    }
    if(i > 1){ ## this is the conditional distribution we're interested in i.e f(V2, V3 | V1) = f(V1, V2, V3) / f(V1)
      feat <- nms[as.logical(explainer$S[i, ])]
      mat <- NULL
      for(j in 1:nrow(unique(joint_prob_dt[, feat, with = FALSE]))){
        v <- unique(joint_prob_dt[, feat, with = FALSE])[j]

        if(ncol(v) == 1){
          v <- as.numeric(v[[1]])

          mat0 <- joint_prob_dt[get(feat) == v]
          mat0[,  marg_prob := marg_list[[i]][j, 'prob']]
          mat0[, cond_prob := N / marg_prob]
          setnames(mat0, 'N', 'joint_prob')
          mat0[, conditioned_on := feat]

        } else if (ncol(v) == 2){
          v1 <- as.numeric(v[[1]])
          v2 <- as.numeric(v[[2]])

          mat0 <-joint_prob_dt[get(feat[1]) == v1 & get(feat[2]) == v2]
          mat0[,  marg_prob := marg_list[[i]][j, 'prob']]
          mat0[, cond_prob := N / marg_prob]
          setnames(mat0, 'N', 'joint_prob')
          mat0[, conditioned_on := paste(feat, collapse = ", ")]

        } else if (ncol(v) == 3){
          v1 <- as.numeric(v[[1]])
          v2 <- as.numeric(v[[2]])
          v3 <- as.numeric(v[[3]])

          mat0 <- joint_prob_dt[get(feat[1]) == v1 & get(feat[2]) == v2 & get(feat[3]) == v3]
          mat0[,  marg_prob := marg_list[[i]][j, 'prob']]
          mat0[, cond_prob := N / marg_prob]
          setnames(mat0, 'N', 'joint_prob')
          mat0[, conditioned_on := paste(feat, collapse = ", ")]

        }
        mat <- rbind(mat, mat0)
      }
      cond_list[[i]] <- mat
    }
  }
  return(cond_list)
}

#' Function to calculate conditional expectations of the cutoff jointly Normal random variables
#'
#' @description
#'
#' @param x_test vector of test observations. Has the same dimension as the number of joint Normal random variables calculated in \code{sim_true_Normal} function.
#' @param cond_list list. Calculated using the \code{cond_prob} function.
#' @param explainer explainer object from shapr package.
#' @param prediction_zero Numeric. Number to assigned to phi_0 in Shapley framework.
#'
#' @return list
#'
#' @export

cond_expec <- function(x_test, cond_list, explainer, prediction_zero){ ## removed prediction_zero

  nms <- colnames(explainer$x_train)

  cond_expec <- NULL
  for(i in 1:nrow(explainer$S)){
    if(i == 1){
      cond_expec <- c(cond_expec, prediction_zero) #
    } else if(i > 1){ ## this is the conditional distribution we're interested in i.e f(V2, V3 | V1) = f(V1, V2, V3) / f(V1)
      feat <- nms[as.logical(explainer$S[i, ])]
      v <- x_test[as.logical(explainer$S[i, ])]
      if(length(v) == 1){

        v <- as.numeric(v[[1]])
        mat <- cond_list[[i]][get(feat) == v]
        mat[, predict := predict_model(explainer$model, newdata = .SD), .SDcols = nms]
        mat[, expected_value := predict * cond_prob]

        cond_expec <- c(cond_expec, sum(mat$expected_value))
      } else if(length(v) == 2){

        v1 <- as.numeric(v[[1]])
        v2 <- as.numeric(v[[2]])
        mat <- cond_list[[i]][get(feat[1]) == v1 & get(feat[2]) == v2]
        mat[, predict := predict_model(explainer$model, newdata = .SD), .SDcols = nms][, 'expected_value' := predict * cond_prob]
        cond_expec <- c(cond_expec, sum(mat$expected_value))
      } else if(length(v) == 3){

        v1 <- as.numeric(v[[1]])
        v2 <- as.numeric(v[[2]])
        v3 <- as.numeric(v[[3]])
        mat <- cond_list[[i]][get(feat[1]) == v1 & get(feat[2]) == v2 & get(feat[3]) == v3]
        mat[, predict := predict_model(explainer$model, newdata = .SD), .SDcols = nms][, 'expected_value' := predict * cond_prob]
        cond_expec <- c(cond_expec, sum(mat$expected_value))
      }

    }
  }
  return(cond_expec)
}

#' Function to calculate the true Shapley values based on the conditional expectations calculated using \code{cond_expec}
#'
#' @description
#'
#' @param explainer explainer object from shapr package.
#' @param cond_expec list. Calculated using \code{cond_expec} function.
#' @param x_test vector of test observations. Has the same dimension as the number of joint Normal random variables calculated in \code{sim_true_Normal} function.
#'
#' @return vector of Shapley values.
#'
#' @export

true_Kshap <- function(explainer, cond_expec, x_test){

  Kshap <- matrix(0, nrow = nrow(x_test), ncol = nrow(explainer$W))
  for (i in 1:nrow(x_test)) {
    Kshap[i, ] = explainer$W %*% cond_expec[i, ]
  }
  Kshap <- data.table(Kshap)
  names(Kshap) <- c("none", "feat1", "feat2", "feat3")

  return(Kshap)
}

#' Function to calculate the true Shapley values under the strict conditions that the features are independent and the response function is linear.
#'
#' @description
#'
#' @param x_test_onehot vector of Numerics. The testing observations, one-hot encoded
#' @param beta vector of Numerics. The coefficients of the linear model.
#' @param dt
#' @param prop
#'
#' @return vector of Shapley values.
#'
#' @export

linear_Kshap <- function(x_test_onehot, beta, dt, prop){

  # prop <- c(0, apply(dt[, .(feat12, feat13)], 2, sum) / nrow(dt), 0, apply(dt[, .(feat22, feat23)], 2, sum) / nrow(dt), 0, apply(dt[, .(feat32, feat33)], 2, sum) / nrow(dt))

  # for(i in c(1, 4, 7)){
  #   prop[i] <- 1 - prop[i + 1] - prop[i + 2]
  # }
  phi0 <- NULL
  phi0 <- c(phi0, beta[1] + sum(beta[2:10] * prop))

  x_test0 <- x_test_onehot

  # x_test1 <- cbind((1 - x_test0[1, 1]) * (1 - x_test0[1, 2]),  x_test0[1, 1:2], (1 - x_test0[1, 3]) * (1 - x_test0[1, 4]), x_test0[1, 3:4], (1 - x_test0[1, 5]) * (1 - x_test0[1, 6]), x_test0[1, 5:6])

  x_test1 <- c((1 - x_test0[1]) * (1 - x_test0[2]),  x_test0[1:2], (1 - x_test0[3]) * (1 - x_test0[4]), x_test0[3:4], (1 - x_test0[5]) * (1 - x_test0[6]), x_test0[5:6])
  for(i in 1:length(prop)){
    phi0 <- c(phi0, beta[i + 1] * (x_test1[[i]] - prop[i]) )
  }
  phi <- c(phi0[1], sum(phi0[2:4]), sum(phi0[5:7]), sum(phi0[8:10]))

  return(phi)
}

# shapley_method <- true_linear

#' Function to calculate the mean average error (MAE) between the true Shapley values and the estimated Shapley values
#'
#' @description
#'
#' @param true_shapley vector of Numerics. The vector of true Shapley values.
#' @param shapley_method vector of Numerics. The vector of estimated Shapley values
#'
#' @return vector of Shapley values.
#'
#' @export

MAE <- function(true_shapley, shapley_method){
  mean(apply(abs(true_shapley - shapley_method), 2, mean)[-1])
}

#' Function to simulate the data and calculate the estimated Shapley value as well as simulate the random variables to calculate the true Shapley values
#'
#' @description
#'
#' @param parameters_list list. List of all the parameters needed for simulating the data and calcualting the true and estimated Shapley values.
#'
#' @return vector of Shapley values.
#'
#' @export


simulate_data <- function(parameters_list){

  ## make sure Sigma is positive definite
  Sigma <- matrix(rep(parameters_list$corr, 9), 3, 3)
  Sigma[1, 1] <- Sigma[2, 2] <- Sigma[3, 3] <- parameters_list$Sigma_diag
  if(!lqmm::is.positive.definite(Sigma)) {
    print("Covariance matrix is not positive definite but will be converted.")
    Sigma <- make.positive.definite(Sigma)
    print("New Sigma matrix:")
    print(Sigma)
  }

  mu <- parameters_list$mu
  beta <- parameters_list$beta
  # N_data <- parameters_list$N_data
  N_shapley <- parameters_list$N_shapley
  noise <- parameters_list$noise
  response_mod <- parameters_list$response_mod
  fit_mod <- parameters_list$fit_mod
  methods <- parameters_list$methods
  cutoff <- parameters_list$cutoff
  N_testing <- parameters_list$N_testing
  N_training <- parameters_list$N_training

  ## 1. simulate training and testing data
  tm_current <- Sys.time()
  print("Simulating training and testing data", quote = FALSE, right = FALSE)
  x <- mvrnorm(n = N_testing + N_training, mu = mu, Sigma = Sigma)

  dt <- NULL
  if(is.null(cutoff)){ ## to get equal proportion in each level
    for(i in 1:ncol(x)){
      dt <- cbind(dt, cut(x[, i], quantile(x[, i], probs = c(0, 0.33, 0.66, 1)), labels = 1:3, include.lowest = TRUE)) # without include.lowest, you get NA at the boundaries
      cutoff <- c(cutoff, quantile(x[, i], probs = c(0, 0.33, 0.66, 1)))
    }
    cutoff <- t(matrix(cutoff, ncol = 3))
  } else{
    for(i in 1:ncol(x)){
      dt <- cbind(dt, cut(x[, i], cutoff, labels = c(1:3)))
    }
  }

  dt <- data.table(dt)
  ## Sanity check:
  # table(dt[, V1])

  setnames(dt, c("feat1", "feat2", "feat3"))

  dt[, feat1 := as.factor(feat1)]
  dt[, feat2 := as.factor(feat2)]
  dt[, feat3 := as.factor(feat3)]

  if(noise == TRUE){
    dt[, epsilon := rnorm(N_testing + N_training, 0, 0.1^2)] #
  } else{
    dt[, epsilon := 0]
  }

  ## 2. One-hot encoding of training data
  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("One-hot encoding training data", quote = FALSE, right = FALSE)
  dt <- cbind(dt, data.table(model.matrix(~., data = dt[, .(feat1, feat2, feat3)])))

  ## 3. Calculate response
  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Calculating response of training data", quote = FALSE, right = FALSE)
  dt[, response := response_mod(feat12, feat13, feat22, feat23, feat32, feat33, epsilon, beta)]

  ## 4. Fit model
  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Fitting model of training data", quote = FALSE, right = FALSE)
  if(fit_mod == 'regression'){
    model <- lm(response ~ feat1 + feat2 + feat3, data = dt[-(1:N_testing), .(feat1, feat2, feat3, response)])
  }

  ## 5. initalize shapr object with trained model -- this is used for calculating true shapley
  ## changed this Nov 21 --- removed as.matrix because features are categorical.
  # x_train <- as.matrix(dt[-(1:6), .(feat1, feat2, feat3)]) ## used in explainer()
  # x_test <- as.matrix(dt[(1:6), .(feat1, feat2, feat3)]) ## used in cond_expec_mat()
  # y_train <- as.matrix(dt[-(1:6), .(response)]) ## used in cond_expec_mat()

  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Initializing shapr object with trained model", quote = FALSE, right = FALSE)
  x_train <- dt[-(1:N_testing), .(feat1, feat2, feat3)] ## used in explainer()
  x_test <- dt[(1:N_testing), .(feat1, feat2, feat3)] ## used in cond_expec_mat()
  y_train <- dt[-(1:N_testing), .(response)] ## used in cond_expec_mat()
  explainer <- shapr(x_train, model)

  ## 6. calculate the true shapley values
  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Simulating Normal random variables to calculate true Shapley value", quote = FALSE, right = FALSE)
  joint_prob_dt <- sim_true_Normal(mu, Sigma, beta, N_shapley = N_shapley, explainer, cutoff, response_mod) ## 1 min for 10 mill

  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Calculating marginal probability distributions", quote = FALSE, right = FALSE)
  marg_list <- marg_prob(joint_prob_dt[[1]], explainer)

  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Calculating conditional probability distributions", quote = FALSE, right = FALSE)
  cond_list <- cond_prob(marg_list, joint_prob_dt[[1]], explainer)

  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Calculating conditional expectations", quote = FALSE, right = FALSE)
  cond_expec_mat <- t(apply(x_test, 1, FUN = cond_expec, cond_list, explainer, prediction_zero = joint_prob_dt[[2]]))

  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Calculating true Shapley values", quote = FALSE, right = FALSE)
  true_shapley <- true_Kshap(explainer, cond_expec_mat, x_test)

  ## 7. calculate true shapley under linear model and independence assumption (only if correlation is 0)
  tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
  print("Calculating Shapley value under linear model and indepdent variables assumption", quote = FALSE, right = FALSE)
  x_test_onehot <- dt[(1:N_testing), .(feat12, feat13, feat22, feat23, feat32, feat33)]
  if(explainer$model_type == 'regression'){
    if(parameters_list$corr == 0){
      true_linear <- t(apply(x_test_onehot, 1, FUN = linear_Kshap, beta, dt, prop = joint_prob_dt[[3]]))
    } else{
      true_linear <- NULL
    }
  } else{
    true_linear <- NULL
  }

  ## 8. calculate approximate shapley value with different methods

  p <- mean(y_train$response) # since y_train is no longer a matrix

  timeit <- list()

  explanation_list <- list()
  for(m in methods){
    tm_now <- Sys.time(); print(tm_now - tm_current); tm_current <- Sys.time()
    print(paste0("Estimating Shapley value with ", m, "method"), quote = FALSE, right = FALSE)
    if(m == 'empirical' | m == 'empirical_ind' |  m == 'gaussian' | m == 'ctree_onehot'){

      x_train_onehot <- as.matrix(dt[-(1:N_testing), .(feat12, feat13, feat22, feat23, feat32, feat33)])

      if(fit_mod == 'regression'){
        fmla <- as.formula(paste("response ~", paste(colnames(x_train_onehot), collapse = " + ")))
        model_onehot <- lm(fmla, data = dt[-(1:N_testing), !c("feat1", "feat2", "feat3", "epsilon")])
      }

      explainer_onehot <- shapr(x_train_onehot, model_onehot)

      if(m == 'ctree_onehot'){
        tm <- Sys.time()
        explanation_list[[m]] <- explain(
          x_test_onehot,
          approach = 'ctree',
          explainer = explainer_onehot,
          prediction_zero = p,
          sample = FALSE)
        tm2 <- Sys.time()
        timeit['ctree_onehot'] <- (tm2 - tm)
      } else if(m == 'empirical_ind'){
        tm <- Sys.time()
        explanation_list[[m]] <- explain(
          x_test_onehot,
          approach = "empirical",
          type = "independence",
          explainer = explainer_onehot,
          prediction_zero = p,
          sample = FALSE)
        tm2 <- Sys.time()
        timeit['empirical_ind'] <- (tm2 - tm)
      } else{
        tm <- Sys.time()
        explanation_list[[m]] <- explain(
          x_test_onehot,
          approach = m,
          explainer = explainer_onehot,
          prediction_zero = p,
          sample = FALSE)
        tm2 <- Sys.time()
        timeit[m] <- (tm2 - tm)
      }
      explanation_list[[m]]$dt_sum <- cbind(NULL, explanation_list[[m]]$dt[, 1])
      for(i in c(2, 4, 6)){
        explanation_list[[m]]$dt_sum <- cbind(explanation_list[[m]]$dt_sum, apply(explanation_list[[m]]$dt[, i:(i + 1)], 1, sum))
      }
      setnames(explanation_list[[m]]$dt_sum, c("none", "feat1", "feat2", "feat3"))

    } else { ## for ctree without one-hot encoding
      tm <- Sys.time()
      explanation_list[[m]] <- explain(
        x_test,
        approach = m,
        explainer = explainer,
        prediction_zero = p,
        sample = FALSE)
      tm2 <- Sys.time()
      timeit[m] <- (tm2 - tm)
    }
  }

  return_list <- list()
  return_list[['true_shapley']] <- true_shapley
  return_list[['true_linear']] <- true_linear
  return_list[['methods']] <- explanation_list
  return_list[['timing']] <- timeit
  print("--- End ---")
  return(return_list)

}


