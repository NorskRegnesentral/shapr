---
# Example from https://joss.readthedocs.io/en/latest/submitting.html
title: 'shapr: An R package for explaining machine learning models with dependence-aware Shapley values'
tags:
  - R
  - explainable AI
  - interpretable machine learning
  - shapley values
  - feature dependence
authors:
  - name: Nikolai Sellereite
    orcid: 0000-0002-4671-0337
    affiliation: 1 # (Multiple affiliations must be quoted)
  - name: Martin Jullum
    orcid: 0000-0003-3908-5155
    affiliation: 1
affiliations:
 - name: Norwegian Computing Center
   index: 1
citation_author: Sellereite and Jullum
date: 20 November 2019
year: 2019
formatted_doi: XX.XXXXX/joss.XXXXX
bibliography: paper.bib
output: rticles::joss_article
csl: apa.csl
journal: JOSS
---


<!-- What should my paper contain? -->
<!-- Important -->

<!-- Begin your paper with a summary of the high-level functionality of your software for a non-specialist reader. Avoid jargon in this section. -->

<!-- JOSS welcomes submissions from broadly diverse research areas. For this reason, we require that authors include in the paper some sentences that explain the software functionality and domain of use to a non-specialist reader. We also require that authors explain the research applications of the software. The paper should be between 250-1000 words. -->

<!-- Your paper should include: -->

<!-- A list of the authors of the software and their affiliations, using the correct format (see the example below). -->
<!-- A summary describing the high-level functionality and purpose of the software for a diverse, non-specialist audience. -->
<!-- A clear Statement of Need that illustrates the research purpose of the software. -->
<!-- A list of key references, including to other software addressing related needs. -->
<!-- Mention (if applicable) of any past or ongoing research projects using the software and recent scholarly publications enabled by it. -->
<!-- Acknowledgement of any financial support. -->
<!-- As this short list shows, JOSS papers are only expected to contain a limited set of metadata (see example below), a Statement of Need, Summary, Acknowledgements, and References sections. You can look at an example accepted paper. Given this format, a “full length” paper is not permitted, and software documentation such as API (Application Programming Interface) functionality should not be in the paper and instead should be outlined in the software documentation. 

USE: devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)

for word counting in the Rmarkdwon document (250-1000 words)

-->




# Summary

A common task within machine learning is to train a model which is able to predict an unknown outcome 
(response variable) based on a set of known input variables/features.
When using such models for real life applications, it is often crucial to understand why a certain set of features lead 
to exactly a specific prediction.
Most machine learning models are however so complicated and hard to understand that they are often viewed as 
"black-boxes" producing output when provided some input.

Shapley values (@Shapley53) is a concept from cooperative game theory used to fairly distribute a joint payoff among the
cooperating players. 
@kononenko2010efficient and later @lundberg2017unified proposed to use the Shapley value framework to explain 
predictions by distributing the prediction value on the input features. 
Established methods and implementations for explaining predictions with Shapley values like Shapley 
Sampling Values (@vstrumbelj2014explaining), SHAP/Kernel SHAP (@lundberg2017unified), and to some extent TreeSHAP 
(@lundberg2018consistent), assume that the features are independent when approximating the Shapley values for prediction
explanation. 
The `R`-package `shapr` implements the methodology proposed by @aas2019explaining, where predictions are explained while accounting for the 
dependence between the features, resulting in significantly more accurate approximations to the Shapley values. 



# Implementation

The package implements a variant of the Kernel SHAP (@lundberg2017unified) methodology for efficiently dealing with the 
combinatorial problem related to the Shapley value formula. 
The main methodological contribution of @aas2019explaining is three different methods to estimate certain conditional 
expectation quantities, referred to as the `empirical`, `gaussian` and `copula` approach. Additionaly, the user has
the option of combining the three approaches. 
The implementation supports explanation of the following models natively: `stats::lm`, `stats::glm`, `ranger::ranger`, 
`mgcv::gam` and `xgboost::xgboost`. 
Moreover, the package supports explanation of custom models by supplying two simple additional class functions.

The user interface in the package has largely been adopted from the `R`-package `lime` (@limeRpackage). 
The user first sets up the explainability framework for the model to explain with the `shapr` function. 
Then the output from `shapr` is provided to the `explain` function, along with the data to explain the prediction for
and which method should be used to estimate the aforementioned conditional expectations.

The majority of the code is plain `R`, while the most time consuming parts of the code has been coded in `C++` through 
the `Rcpp` package for computational speed up. 
Our implementation has methodological improvements to the Kernel SHAP methodology as implemented in the 
`Python` package `shap` (@shapPythonpackage). 
In addition to our package's ability to account for the feature dependence (which the `shap` package does not), 
basic tests suggest our implementation is about 3-4 times faster. 

For a detailed description of the underlying methodology of the package, we refer to the 
[paper](https://arxiv.org/abs/1903.10464) (@aas2019explaining).
For getting started with the package, we recommend the user to go through the package vignette and introductory examples available at the 
package's [pkgdown site](https://norskregnesentral.github.io/shapr/). 

# Acknowledgement

This work was supported by the Norwegian Research Council grant 237718 (Big Insight).


# References
