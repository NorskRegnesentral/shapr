---
output: github_document
bibliography: ./inst/REFERENCES.bib
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>",
    fig.path = "man/figures/README-",
    out.width = "100%"
)

```

# shapr

[![CircleCI](https://circleci.com/gh/NorskRegnesentral/shapr.svg?style=svg&circle-token=7c2a3a4edc870b4694982f0fe8ac66f92d639099)](https://circleci.com/gh/NorskRegnesentral/shapr)

Explaining complex or seemingly simple machine learning models is a practical and ethical question, 
as well as a legal issue. Can I trust the model? Is it biased? Can I explain it to others? We want 
to explain individual predictions from a complex machine learning model by learning simple, 
interpretable explanations. 

This package implements more accurate approximations to Shapley values as described in @aas2019explaining.

The following features are currently implemented:

- Something (@aas2019explaining).
- Something

Future releases will include:

- Something
- Something

All feedback and suggestions are very welcome.

## Installation

To install the current development version, use

```{r, eval=FALSE}
devtools::install_github("NorskRegnesentral/shapr")
```

## An example

## References


