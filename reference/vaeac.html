<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Initializing a vaeac model — vaeac • shapr</title><script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Initializing a vaeac model — vaeac"><meta name="description" content="Class that represents a vaeac model, i.e., the class creates the neural networks in the vaeac
model and necessary training utilities.
For more details, see Olsen et al. (2022)."><meta property="og:description" content="Class that represents a vaeac model, i.e., the class creates the neural networks in the vaeac
model and necessary training utilities.
For more details, see Olsen et al. (2022)."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">shapr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-vignettes" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Vignettes</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-vignettes"><li><a class="dropdown-item" href="../articles/general_usage.html">General usage of shapr</a></li>
    <li><a class="dropdown-item" href="../articles/vaeac.html">Advanced usage of the `vaeac` approach</a></li>
    <li><a class="dropdown-item" href="../articles/regression.html">The separate and surrogate regression approches</a></li>
    <li><a class="dropdown-item" href="../articles/asymmetric_causal.html">Asymmetric and Causal Shapley values</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Manual</a></li>
<li class="nav-item"><a class="nav-link" href="../shaprpy.html">Python</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/NorskRegnesentral/shapr/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch"><li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Initializing a vaeac model</h1>
      <small class="dont-index">Source: <a href="https://github.com/NorskRegnesentral/shapr/blob/master/R/approach_vaeac_torch_modules.R" class="external-link"><code>R/approach_vaeac_torch_modules.R</code></a></small>
      <div class="d-none name"><code>vaeac.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Class that represents a vaeac model, i.e., the class creates the neural networks in the vaeac
model and necessary training utilities.
For more details, see <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf" class="external-link">Olsen et al. (2022)</a>.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">vaeac</span><span class="op">(</span></span>
<span>  <span class="va">one_hot_max_sizes</span>,</span>
<span>  width <span class="op">=</span> <span class="fl">32</span>,</span>
<span>  depth <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  latent_dim <span class="op">=</span> <span class="fl">8</span>,</span>
<span>  activation_function <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="va"><a href="https://torch.mlverse.org/docs/reference/nn_relu.html" class="external-link">nn_relu</a></span>,</span>
<span>  skip_conn_layer <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  skip_conn_masked_enc_dec <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  batch_normalization <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  paired_sampling <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  mask_generator_name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mcar_mask_generator"</span>, <span class="st">"specified_prob_mask_generator"</span>,</span>
<span>    <span class="st">"specified_masks_mask_generator"</span><span class="op">)</span>,</span>
<span>  masking_ratio <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  mask_gen_coalitions <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  mask_gen_coalitions_prob <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  sigma_mu <span class="op">=</span> <span class="fl">10000</span>,</span>
<span>  sigma_sigma <span class="op">=</span> <span class="fl">1e-04</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-one-hot-max-sizes">one_hot_max_sizes<a class="anchor" aria-label="anchor" href="#arg-one-hot-max-sizes"></a></dt>
<dd><p>A torch tensor of dimension <code>n_features</code> containing the one hot sizes of the <code>n_features</code>
features. That is, if the <code>i</code>th feature is a categorical feature with 5 levels, then <code>one_hot_max_sizes[i] = 5</code>.
While the size for continuous features can either be <code>0</code> or <code>1</code>.</p></dd>


<dt id="arg-width">width<a class="anchor" aria-label="anchor" href="#arg-width"></a></dt>
<dd><p>Integer. The number of neurons in each hidden layer in the neural networks
of the masked encoder, full encoder, and decoder.</p></dd>


<dt id="arg-depth">depth<a class="anchor" aria-label="anchor" href="#arg-depth"></a></dt>
<dd><p>Integer. The number of hidden layers in the neural networks of the
masked encoder, full encoder, and decoder.</p></dd>


<dt id="arg-latent-dim">latent_dim<a class="anchor" aria-label="anchor" href="#arg-latent-dim"></a></dt>
<dd><p>Integer. The number of dimensions in the latent space.</p></dd>


<dt id="arg-activation-function">activation_function<a class="anchor" aria-label="anchor" href="#arg-activation-function"></a></dt>
<dd><p>A <code><a href="https://torch.mlverse.org/docs/reference/nn_module.html" class="external-link">torch::nn_module()</a></code> representing an activation function such as, e.g.,
<code><a href="https://torch.mlverse.org/docs/reference/nn_relu.html" class="external-link">torch::nn_relu()</a></code>, <code><a href="https://torch.mlverse.org/docs/reference/nn_leaky_relu.html" class="external-link">torch::nn_leaky_relu()</a></code>, <code><a href="https://torch.mlverse.org/docs/reference/nn_selu.html" class="external-link">torch::nn_selu()</a></code>,
<code><a href="https://torch.mlverse.org/docs/reference/nn_sigmoid.html" class="external-link">torch::nn_sigmoid()</a></code>.</p></dd>


<dt id="arg-skip-conn-layer">skip_conn_layer<a class="anchor" aria-label="anchor" href="#arg-skip-conn-layer"></a></dt>
<dd><p>Boolean. If we are to use skip connections in each layer, see <code><a href="skip_connection.html">skip_connection()</a></code>.
If <code>TRUE</code>, then we add the input to the outcome of each hidden layer, so the output becomes
\(X + \operatorname{activation}(WX + b)\). I.e., the identity skip connection.</p></dd>


<dt id="arg-skip-conn-masked-enc-dec">skip_conn_masked_enc_dec<a class="anchor" aria-label="anchor" href="#arg-skip-conn-masked-enc-dec"></a></dt>
<dd><p>Boolean. If we are to apply concatenating skip
connections between the layers in the masked encoder and decoder. The first layer of the masked encoder will be
linked to the last layer of the decoder. The second layer of the masked encoder will be
linked to the second to last layer of the decoder, and so on.</p></dd>


<dt id="arg-batch-normalization">batch_normalization<a class="anchor" aria-label="anchor" href="#arg-batch-normalization"></a></dt>
<dd><p>Boolean. If we are to use batch normalization after the activation function.
Note that if <code>skip_conn_layer</code> is TRUE, then the normalization is
done after the adding from the skip connection. I.e, we batch normalize the whole quantity X + activation(WX + b).</p></dd>


<dt id="arg-paired-sampling">paired_sampling<a class="anchor" aria-label="anchor" href="#arg-paired-sampling"></a></dt>
<dd><p>Boolean. If we are doing paired sampling. I.e., if we are to include both coalition S
and \(\bar{S}\) when we sample coalitions during training for each batch.</p></dd>


<dt id="arg-mask-generator-name">mask_generator_name<a class="anchor" aria-label="anchor" href="#arg-mask-generator-name"></a></dt>
<dd><p>String specifying the type of mask generator to use. Need to be one of
'mcar_mask_generator', 'specified_prob_mask_generator', and 'specified_masks_mask_generator'.</p></dd>


<dt id="arg-masking-ratio">masking_ratio<a class="anchor" aria-label="anchor" href="#arg-masking-ratio"></a></dt>
<dd><p>Scalar. The probability for an entry in the generated mask to be 1 (masked).
Not used if <code>mask_gen_coalitions</code> is given.</p></dd>


<dt id="arg-mask-gen-coalitions">mask_gen_coalitions<a class="anchor" aria-label="anchor" href="#arg-mask-gen-coalitions"></a></dt>
<dd><p>Matrix containing the different coalitions to learn.
Must be given if <code>mask_generator_name = 'specified_masks_mask_generator'</code>.</p></dd>


<dt id="arg-mask-gen-coalitions-prob">mask_gen_coalitions_prob<a class="anchor" aria-label="anchor" href="#arg-mask-gen-coalitions-prob"></a></dt>
<dd><p>Numerics containing the probabilities
for sampling each mask in <code>mask_gen_coalitions</code>.
Array containing the probabilities for sampling the coalitions in <code>mask_gen_coalitions</code>.</p></dd>


<dt id="arg-sigma-mu">sigma_mu<a class="anchor" aria-label="anchor" href="#arg-sigma-mu"></a></dt>
<dd><p>Numeric representing a hyperparameter in the normal-gamma prior used on the masked encoder,
see Section 3.3.1 in <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf" class="external-link">Olsen et al. (2022)</a>.</p></dd>


<dt id="arg-sigma-sigma">sigma_sigma<a class="anchor" aria-label="anchor" href="#arg-sigma-sigma"></a></dt>
<dd><p>Numeric representing a hyperparameter in the normal-gamma prior used on the masked encoder,
see Section 3.3.1 in <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf" class="external-link">Olsen et al. (2022)</a>.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>Returns a list with the neural networks of the masked encoder, full encoder, and decoder together
with reconstruction log probability function, optimizer constructor, sampler from the decoder output,
mask generator, batch size, and scale factor for the stability of the variational lower bound optimization.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>This function builds neural networks (masked encoder, full encoder, decoder) given
the list of one-hot max sizes of the features in the dataset we use to train the vaeac model,
and the provided parameters for the networks. It also creates, e.g., reconstruction log probability function,
methods for sampling from the decoder output, and then use these to create the vaeac model.</p>
    </div>
    <div class="section level2">
    <h2 id="make-observed">make_observed<a class="anchor" aria-label="anchor" href="#make-observed"></a></h2>


<p>Apply Mask to Batch to Create Observed Batch</p>
<p>Compute the parameters for the latent normal distributions inferred by the encoders.
If <code>only_masked_encoder = TRUE</code>, then we only compute the latent normal distributions inferred by the
masked encoder. This is used in the deployment phase when we do not have access to the full observation.</p>
    </div>
    <div class="section level2">
    <h2 id="make-latent-distributions">make_latent_distributions<a class="anchor" aria-label="anchor" href="#make-latent-distributions"></a></h2>


<p>Compute the Latent Distributions Inferred by the Encoders</p>
<p>Compute the parameters for the latent normal distributions inferred by the encoders.
If <code>only_masked_encoder = TRUE</code>, then we only compute the latent normal distributions inferred by the
masked encoder. This is used in the deployment phase when we do not have access to the full observation.</p>
    </div>
    <div class="section level2">
    <h2 id="masked-encoder-regularization">masked_encoder_regularization<a class="anchor" aria-label="anchor" href="#masked-encoder-regularization"></a></h2>


<p>Compute the Regularizes for the Latent Distribution Inferred by the Masked Encoder.</p>
<p>The masked encoder (prior) distribution regularization in the latent space.
This is used to compute the extended variational lower bound used to train vaeac, see
Section 3.3.1 in <a href="https://www.jmlr.org/papers/volume23/21-1413/21-1413.pdf" class="external-link">Olsen et al. (2022)</a>.
Though regularizing prevents the masked encoder distribution parameters from going to infinity,
the model usually doesn't diverge even without this regularization. It almost doesn't affect
learning process near zero with default regularization parameters which are recommended to be used.</p>
    </div>
    <div class="section level2">
    <h2 id="batch-vlb">batch_vlb<a class="anchor" aria-label="anchor" href="#batch-vlb"></a></h2>


<p>Compute the Variational Lower Bound for the Observations in the Batch</p>
<p>Compute differentiable lower bound for the given batch of objects and mask.
Used as the (negative) loss function for training the vaeac model.</p>
    </div>
    <div class="section level2">
    <h2 id="batch-iwae">batch_iwae<a class="anchor" aria-label="anchor" href="#batch-iwae"></a></h2>


<p>Compute IWAE log likelihood estimate with K samples per object.</p>
<p>Technically, it is differentiable, but it is recommended to use it for
evaluation purposes inside torch.no_grad in order to save memory. With <code><a href="https://torch.mlverse.org/docs/reference/with_no_grad.html" class="external-link">torch::with_no_grad()</a></code>
the method almost doesn't require extra memory for very large K. The method makes K independent
passes through decoder network, so the batch size is the same as for training with batch_vlb.
IWAE is an abbreviation for Importance Sampling Estimator:
$$
\log p_{\theta, \psi}(x|y) \approx
\log {\frac{1}{K} \sum_{i=1}^K [p_\theta(x|z_i, y) * p_\psi(z_i|y) / q_\phi(z_i|x,y)]} \newline
=
\log {\sum_{i=1}^K \exp(\log[p_\theta(x|z_i, y) * p_\psi(z_i|y) / q_\phi(z_i|x,y)])} - \log(K) \newline
=
\log {\sum_{i=1}^K \exp(\log[p_\theta(x|z_i, y)] + \log[p_\psi(z_i|y)] - \log[q_\phi(z_i|x,y)])} - \log(K) \newline
=
\operatorname{logsumexp}(\log[p_\theta(x|z_i, y)] + \log[p_\psi(z_i|y)] - \log[q_\phi(z_i|x,y)]) - \log(K) \newline
=
\operatorname{logsumexp}(\text{rec}\_\text{loss} + \text{prior}\_\text{log}\_\text{prob} -
 \text{proposal}\_\text{log}\_\text{prob}) - \log(K),$$
where \(z_i \sim q_\phi(z|x,y)\).</p>
    </div>
    <div class="section level2">
    <h2 id="generate-samples-params">generate_samples_params<a class="anchor" aria-label="anchor" href="#generate-samples-params"></a></h2>


<p>Generate the parameters of the generative distributions for samples from the batch.</p>
<p>The function makes K latent representation for each object from the batch, send these
latent representations through the decoder to obtain the parameters for the generative distributions.
I.e., means and variances for the normal distributions (continuous features) and probabilities
for the categorical distribution (categorical features).
The second axis is used to index samples for an object, i.e. if the batch shape is [n x D1 x D2], then
the result shape is [n x K x D1 x D2]. It is better to use it inside <code><a href="https://torch.mlverse.org/docs/reference/with_no_grad.html" class="external-link">torch::with_no_grad()</a></code> in order to save
memory. With <code><a href="https://torch.mlverse.org/docs/reference/with_no_grad.html" class="external-link">torch::with_no_grad()</a></code> the method doesn't require extra memory except the memory for the result.</p>
    </div>
    <div class="section level2">
    <h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a></h2>
    <p>Lars Henry Berge Olsen</p>
    </div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Martin Jullum, Lars Henry Berge Olsen, Annabelle Redelmeier, Jon Lachmann, Nikolai Sellereite, Norsk Regnesentral.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

