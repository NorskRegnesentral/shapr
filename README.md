
<!-- README.md is generated from README.Rmd. Please edit that file -->
shapr
=====

[![CircleCI](https://circleci.com/gh/NorskRegnesentral/shapr.svg?style=svg&circle-token=7c2a3a4edc870b4694982f0fe8ac66f92d639099)](https://circleci.com/gh/NorskRegnesentral/shapr)

Explaining complex or seemingly simple machine learning models is a practical and ethical question, as well as a legal issue. Can I trust the model? Is it biased? Can I explain it to others? We want to explain individual predictions from a complex machine learning model by learning simple, interpretable explanations.

This package implements more accurate approximations to Shapley values (Lundberg and Lee (2017)), as described in Aas, Jullum, and Løland (2019).

The following features are currently implemented:

-   Something (Aas, Jullum, and Løland (2019)).
-   Something

Future releases will include:

-   Something
-   Something

All feedback and suggestions are very welcome.

Installation
------------

To install the current development version, use

``` r
devtools::install_github("NorskRegnesentral/shapr")
```

An example
----------

References
----------

Aas, Kjersti, Martin Jullum, and Anders Løland. 2019. “Explaining Individual Predictions When Features Are Dependent: More Accurate Approximations to Shapley Values.” *arXiv Preprint arXiv:1903.10464*.

Lundberg, Scott M, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In *Advances in Neural Information Processing Systems 30*, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4765–74. Curran Associates, Inc. <http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>.
